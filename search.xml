<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>基础算法题总结11--树</title>
      <link href="/posts/t0011/"/>
      <url>/posts/t0011/</url>
      
        <content type="html"><![CDATA[<h2 id="树"><a href="#树" class="headerlink" title="树"></a>树</h2><h2 id="相关练习"><a href="#相关练习" class="headerlink" title="相关练习"></a>相关练习</h2><p>LeetCode中相关的题目：</p><ul><li><a href="https://leetcode-cn.com/problems/symmetric-tree/" target="_blank" rel="noopener">101. 对称二叉树</a>: 树、深度优先遍历</li><li><a href="https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/" target="_blank" rel="noopener">104. 二叉树的最大深度</a></li><li><a href="https://leetcode-cn.com/problems/construct-binary-tree-from-preorder-and-inorder-traversal/" target="_blank" rel="noopener">105. 从前序与中序遍历序列构造二叉树</a></li><li><a href="https://leetcode-cn.com/problems/unique-binary-search-trees/" target="_blank" rel="noopener">96. 不同的二叉搜索树</a></li></ul><p><strong>101. 对称二叉树</strong><br>递归 O(n) O(n)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSymmetric</span><span class="params">(self, root: TreeNode)</span> -&gt; bool:</span></span><br><span class="line">        <span class="keyword">return</span> self.isMirror(root,root)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isMirror</span><span class="params">(self, l: TreeNode, r: TreeNode)</span> -&gt; bool:</span></span><br><span class="line">        <span class="comment">## l,r是两个指针，每次检查两个指针的值是否相同，如相等再判断左右子树是否对称</span></span><br><span class="line">        <span class="comment">## 递归的终止天剑是两个节点都为空</span></span><br><span class="line">        <span class="comment">## 或者两个节点有一个为空</span></span><br><span class="line">        <span class="keyword">if</span> l == <span class="literal">None</span> <span class="keyword">and</span> r == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> l == <span class="literal">None</span> <span class="keyword">or</span> r == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="comment">## 两个根节点具有相同的值</span></span><br><span class="line">        <span class="comment">## 每个树的右子树与另一个树的左子树镜像对称</span></span><br><span class="line">        <span class="keyword">return</span> l.val == r.val <span class="keyword">and</span> self.isMirror(l.left,r.right) <span class="keyword">and</span> self.isMirror(l.right,r.left)</span><br></pre></td></tr></table></figure><p>迭代 O(n) O(n)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isSymmetric</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="comment">## 如果树为空，或者左子树右子树都为空，那么这也是对称的</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root <span class="keyword">or</span> <span class="keyword">not</span> (root.left <span class="keyword">or</span> root.right):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="comment"># 用队列保存节点，先将根节点的左子树根节点和右子树根节点放到队列中 </span></span><br><span class="line">        queue = [root.left,root.right]</span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            <span class="comment"># 从队列中取出两个节点，再比较这两个节点</span></span><br><span class="line">            left = queue.pop(<span class="number">0</span>)</span><br><span class="line">            right = queue.pop(<span class="number">0</span>)</span><br><span class="line">            <span class="comment"># 如果两个节点都为空就继续循环，两者有一个为空就返回false</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (left <span class="keyword">or</span> right):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> (left <span class="keyword">and</span> right):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">if</span> left.val!=right.val: <span class="comment">## 如果两个值不相同，则不对称</span></span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="comment"># 将左节点的左孩子， 右节点的右孩子放入队列</span></span><br><span class="line">            queue.append(left.left)</span><br><span class="line">            queue.append(right.right)</span><br><span class="line">            <span class="comment"># 将左节点的右孩子，右节点的左孩子放入队列</span></span><br><span class="line">            queue.append(left.right)</span><br><span class="line">            queue.append(right.left)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></table></figure><p><strong>104. 二叉树的最大深度</strong><br>二叉树的最大深度为max(l,r)+1<br>深度优先遍历：O(n) O(height)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        left = self.maxDepth(root.left)</span><br><span class="line">        right = self.maxDepth(root.right)</span><br><span class="line">        <span class="keyword">return</span> max(left,right) + <span class="number">1</span></span><br></pre></td></tr></table></figure><p>广度优先遍历：O(n) O(n)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        queue = collections.deque([root])  <span class="comment">## 队列</span></span><br><span class="line">        depth = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> queue: <span class="comment">## 队列为空，遍历完所有节点，循环结束</span></span><br><span class="line">            n = len(queue) <span class="comment">## 计算当前节点所在树的层的宽度</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">                node = queue.popleft() <span class="comment">## 把当前层的节点处队</span></span><br><span class="line">                <span class="keyword">if</span> node.left: <span class="comment">## 下一层的节点入队</span></span><br><span class="line">                    queue.append(node.left)</span><br><span class="line">                <span class="keyword">if</span> node.right:</span><br><span class="line">                    queue.append(node.right)</span><br><span class="line">            depth += <span class="number">1</span>  <span class="comment">## 当前层遍历完则深度+1</span></span><br><span class="line">        <span class="keyword">return</span> depth</span><br></pre></td></tr></table></figure><p><strong>105. 从前序与中序遍历序列构造二叉树</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Definition for a binary tree node.</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, val=0, left=None, right=None):</span></span><br><span class="line"><span class="comment">#         self.val = val</span></span><br><span class="line"><span class="comment">#         self.left = left</span></span><br><span class="line"><span class="comment">#         self.right = right</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">buildTree</span><span class="params">(self, preorder: List[int], inorder: List[int])</span> -&gt; TreeNode:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">buildTree</span><span class="params">(pre_left,pre_right,in_left,in_right)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> pre_left &gt; pre_right:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">## 前序遍历中第一个就是根节点</span></span><br><span class="line">            pre_root = pre_left</span><br><span class="line">            <span class="comment">## 查找根节点在中序遍历中的位置</span></span><br><span class="line">            in_root = index[preorder[pre_root]]</span><br><span class="line">            <span class="comment">## 建立根节点</span></span><br><span class="line">            root = TreeNode(preorder[pre_root])</span><br><span class="line">            <span class="comment">## 计算左子树的节点数目</span></span><br><span class="line">            n_left = in_root - in_left</span><br><span class="line">            <span class="comment">## 递归的建立左子树</span></span><br><span class="line">            root.left = buildTree(pre_left+<span class="number">1</span>,pre_left+n_left,in_left,in_root<span class="number">-1</span>)</span><br><span class="line">            <span class="comment">## 递归的建立右子树</span></span><br><span class="line">            root.right = buildTree(pre_left+n_left+<span class="number">1</span>,pre_right,in_root+<span class="number">1</span>,in_right)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> root</span><br><span class="line">        </span><br><span class="line">        n = len(preorder)</span><br><span class="line">        <span class="comment">## 构建哈希映射，快速定位根节点的位置</span></span><br><span class="line">        index = &#123;element: i <span class="keyword">for</span> i,element <span class="keyword">in</span> enumerate(inorder)&#125;</span><br><span class="line">        <span class="keyword">return</span> buildTree(<span class="number">0</span>,n<span class="number">-1</span>,<span class="number">0</span>,n<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p><strong>96. 不同的二叉搜索树</strong>: 满足数学公式</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numTrees</span><span class="params">(self, n: int)</span> -&gt; int:</span></span><br><span class="line">        c = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            c = c * <span class="number">2</span>*(<span class="number">2</span>*i+<span class="number">1</span>)/(i+<span class="number">2</span>)  <span class="comment">## 卡塔兰数公式</span></span><br><span class="line">        <span class="keyword">return</span> int(c)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> leetcode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础算法题总结9--堆</title>
      <link href="/posts/t0009/"/>
      <url>/posts/t0009/</url>
      
        <content type="html"><![CDATA[<h2 id="堆的基本知识"><a href="#堆的基本知识" class="headerlink" title="堆的基本知识"></a>堆的基本知识</h2><p>堆是一种完全二叉树<br>堆有两种类型：最大堆和最小堆。<br>区别：最大堆的父节点大于它的子节点，而最小堆中子节点大于父节点。</p><p>关于topK类的问题，可以考虑堆。<br>最大堆求topK小，最小堆求topK大。</p><h3 id="最大堆"><a href="#最大堆" class="headerlink" title="最大堆"></a>最大堆</h3><h3 id="最小堆"><a href="#最小堆" class="headerlink" title="最小堆"></a>最小堆</h3><h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> leetcode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础算法题总结10--数组</title>
      <link href="/posts/t0010/"/>
      <url>/posts/t0010/</url>
      
        <content type="html"><![CDATA[<h2 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h2><h2 id="相关练习"><a href="#相关练习" class="headerlink" title="相关练习"></a>相关练习</h2><p>LeetCode中相关的题目：</p><ul><li><a href="https://leetcode-cn.com/problems/two-sum/" target="_blank" rel="noopener">1. 两数之和</a>: 哈希表，一次遍历</li><li><a href="https://leetcode-cn.com/problems/3sum/" target="_blank" rel="noopener">15. 三数之和</a>： 排序+双指针</li></ul><ul><li><a href="https://leetcode-cn.com/problems/valid-parentheses/" target="_blank" rel="noopener">20. 有效的括号</a>:栈+字符串+字典</li><li><a href="https://leetcode-cn.com/problems/merge-two-sorted-lists/" target="_blank" rel="noopener">21. 合并两个有序链表</a>:递归、链表</li><li><a href="https://leetcode-cn.com/problems/add-two-numbers/" target="_blank" rel="noopener">2. 两数相加</a></li><li><a href="https://leetcode-cn.com/problems/zigzag-conversion/" target="_blank" rel="noopener">6. Z 字形变换</a></li><li><a href="https://leetcode-cn.com/problems/reverse-integer/" target="_blank" rel="noopener">7. 整数反转</a></li><li><a href="https://leetcode-cn.com/problems/integer-to-roman/" target="_blank" rel="noopener">12. 整数转罗马数字</a></li><li><a href="https://leetcode-cn.com/problems/roman-to-integer/" target="_blank" rel="noopener">13. 罗马数字转整数</a></li><li><a href="https://leetcode-cn.com/problems/longest-common-prefix/" target="_blank" rel="noopener">14. 最长公共前缀</a></li><li><a href="https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array/" target="_blank" rel="noopener">26. 删除有序数组中的重复项</a></li><li><a href="https://leetcode-cn.com/problems/remove-element/" target="_blank" rel="noopener">27. 移除元素</a></li><li><a href="https://leetcode-cn.com/problems/implement-strstr/" target="_blank" rel="noopener">28. 实现 strStr()</a></li><li><a href="https://leetcode-cn.com/problems/minimum-path-sum/" target="_blank" rel="noopener">64. 最小路径和</a></li><li><a href="https://leetcode-cn.com/problems/unique-paths/" target="_blank" rel="noopener">62. 不同路径</a></li></ul><h3 id="题目详细描述"><a href="#题目详细描述" class="headerlink" title="题目详细描述"></a>题目详细描述</h3><p><strong>两数之和</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span><span class="params">(self, nums: List[int], target: int)</span> -&gt; List[int]:</span></span><br><span class="line">        hashmap=&#123;&#125;  <span class="comment">## 使用哈希表，以空间换时间，只用遍历一遍数组</span></span><br><span class="line">        <span class="keyword">for</span> i,num <span class="keyword">in</span> enumerate(nums):</span><br><span class="line">            <span class="keyword">if</span> hashmap.get(target - num) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment">## 字典遍历只需O(1)</span></span><br><span class="line">                <span class="keyword">return</span> [hashmap.get(target - num),i]</span><br><span class="line">            hashmap[num] = i</span><br></pre></td></tr></table></figure><p><strong>三数之和</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">threeSum</span><span class="params">(self, nums: List[int])</span> -&gt; List[List[int]]:</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        <span class="keyword">if</span> n &lt; <span class="number">3</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        res = []</span><br><span class="line">        nums.sort()  <span class="comment">## 需要先排序</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">## 枚举a</span></span><br><span class="line">        <span class="keyword">for</span> first <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment">## 需要枚举的值和上次不一样</span></span><br><span class="line">            <span class="keyword">if</span> first &gt; <span class="number">0</span> <span class="keyword">and</span> nums[first] == nums[first<span class="number">-1</span>]: </span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            third = n<span class="number">-1</span>  <span class="comment">## 先让c指向数组的最右端</span></span><br><span class="line">            target = -nums[first]</span><br><span class="line">            <span class="comment">## 枚举b</span></span><br><span class="line">            <span class="keyword">for</span> second <span class="keyword">in</span> range(first+<span class="number">1</span>,n):</span><br><span class="line">                <span class="comment">## 需要枚举的值和上次不一样</span></span><br><span class="line">                <span class="keyword">if</span> second &gt; first+<span class="number">1</span> <span class="keyword">and</span> nums[second] == nums[second<span class="number">-1</span>]:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="comment">## b指针一直在c指针的左边，并且当两者的和一直大于target，那c指针一直后退</span></span><br><span class="line">                <span class="keyword">while</span> second &lt; third <span class="keyword">and</span> nums[second] + nums[third] &gt; target:</span><br><span class="line">                    third -= <span class="number">1</span></span><br><span class="line">                <span class="comment">## 当遍历结束没有找到，则就结束这次循环</span></span><br><span class="line">                <span class="keyword">if</span> second == third:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="comment">## 当找到之后，保存到结果的数组中</span></span><br><span class="line">                <span class="keyword">if</span> nums[second] + nums[third] == target:</span><br><span class="line">                    res.append([nums[first],nums[second],nums[third]])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p><strong>有效的括号</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isValid</span><span class="params">(self, s: str)</span> -&gt; bool:</span></span><br><span class="line">        <span class="keyword">if</span> len(s) % <span class="number">2</span> == <span class="number">1</span>: <span class="comment">## 当数组的长度不是偶数的时候，一定有括号是单个的</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        pairs = &#123;    <span class="comment">## 字典存放括号对</span></span><br><span class="line">            <span class="string">")"</span>:<span class="string">"("</span>,</span><br><span class="line">            <span class="string">"&#125;"</span>:<span class="string">"&#123;"</span>,</span><br><span class="line">            <span class="string">"]"</span>:<span class="string">"["</span></span><br><span class="line">        &#125;</span><br><span class="line">        stack = []</span><br><span class="line">        <span class="keyword">for</span> ch <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">in</span> pairs: <span class="comment">## 当前括号是后面的一部分</span></span><br><span class="line">                <span class="comment">## 如果栈为空或者栈顶和当前括号不匹配，那么不匹配</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> stack <span class="keyword">or</span> stack[<span class="number">-1</span>] != pairs[ch]:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">                stack.pop() <span class="comment">## 匹配则出栈</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                stack.append(ch) <span class="comment">## 前半括号入栈</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">not</span> stack  <span class="comment">## 如果栈为空，则全部匹配，否则就有多余的括号未匹配</span></span><br></pre></td></tr></table></figure><p><strong>合并两个有序链表</strong><br>第一个方法，递归 O(n+m) O(n+m)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mergeTwoLists</span><span class="params">(self, l1: ListNode, l2: ListNode)</span> -&gt; ListNode:</span></span><br><span class="line">        <span class="keyword">if</span> l1 <span class="keyword">is</span> <span class="literal">None</span>:  <span class="comment">## 链表为空的话，就不用合并，直接返回另一个链表</span></span><br><span class="line">            <span class="keyword">return</span> l2</span><br><span class="line">        <span class="keyword">if</span> l2 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> l1</span><br><span class="line">        <span class="comment">## 判断哪个链表的头结点的值更小，递归的决定下一个添加到结果里的节点</span></span><br><span class="line">        <span class="keyword">if</span> l1.val &lt; l2.val:  </span><br><span class="line">            l1.next = self.mergeTwoLists(l1.next,l2)</span><br><span class="line">            <span class="keyword">return</span> l1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l2.next = self.mergeTwoLists(l1,l2.next)</span><br><span class="line">            <span class="keyword">return</span> l2</span><br></pre></td></tr></table></figure><p>第二个方法， 迭代 O(n+m) O(1)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mergeTwoLists</span><span class="params">(self, l1, l2)</span>:</span></span><br><span class="line">        prehead = ListNode(<span class="number">-1</span>)  <span class="comment">## 哨兵节点</span></span><br><span class="line"></span><br><span class="line">        prev = prehead</span><br><span class="line">        <span class="keyword">while</span> l1 <span class="keyword">and</span> l2:</span><br><span class="line">            <span class="keyword">if</span> l1.val &lt;= l2.val: <span class="comment">## 将l1当前节点的节点接在pre节点的后面</span></span><br><span class="line">                prev.next = l1</span><br><span class="line">                l1 = l1.next <span class="comment">## 后移</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                prev.next = l2</span><br><span class="line">                l2 = l2.next            </span><br><span class="line">            prev = prev.next <span class="comment">## 后移一位，指向当前最后的一个元素</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 合并后 l1 和 l2 最多只有一个还未被合并完，我们直接将链表末尾指向未合并完的链表即可</span></span><br><span class="line">        prev.next = l1 <span class="keyword">if</span> l1 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> l2</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> prehead.next</span><br></pre></td></tr></table></figure><p><strong>2. 两数相加</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addTwoNumbers</span><span class="params">(self, l1: ListNode, l2: ListNode)</span> -&gt; ListNode:</span></span><br><span class="line">        head = curr = ListNode()</span><br><span class="line">        carry = val = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> carry <span class="keyword">or</span> l1 <span class="keyword">or</span> l2: <span class="comment">## 判断是否还有进位或者链表还未遍历结束</span></span><br><span class="line">            val = carry</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> l1: l1,val = l1.next, l1.val + val <span class="comment">## l1存在，则值相加，指针后移</span></span><br><span class="line">            <span class="keyword">if</span> l2: l2,val = l2.next, l2.val + val</span><br><span class="line"></span><br><span class="line">            <span class="comment">## 经过计算，两个链表节点相加是两个节点的值加上进位值的和为val</span></span><br><span class="line">            <span class="comment">## 那么新链表出的相应位置的数字是val%10</span></span><br><span class="line">            <span class="comment">## 而新的进位是val/10</span></span><br><span class="line">            <span class="comment">## divmod(a,b)函数返回的是(a//b,a%b)</span></span><br><span class="line">            carry, val = divmod(val, <span class="number">10</span>)</span><br><span class="line">            curr.next = ListNode(val) <span class="comment">## 添加新的节点并指向这个节点</span></span><br><span class="line">            curr = curr.next </span><br><span class="line">        <span class="comment"># if carry &gt; 0: ## 如果链表结束，则链表后面需要添加一个节点，这个在上面放在了while循环中</span></span><br><span class="line">        <span class="comment">#     curr.next = ListNode(carry)</span></span><br><span class="line">        <span class="keyword">return</span> head.next <span class="comment">## 头节点为空，所以从后一位开始作为头结点</span></span><br></pre></td></tr></table></figure><p><strong>6. Z 字形变换</strong>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(self, s: str, numRows: int)</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">if</span> numRows == <span class="number">1</span>: <span class="keyword">return</span> s  <span class="comment">## 当只有一行的时候只用输出就好了</span></span><br><span class="line"></span><br><span class="line">        res = [<span class="string">""</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(numRows)] <span class="comment">## numRow行的数组</span></span><br><span class="line">        curRow = <span class="number">0</span>  <span class="comment">## 初始指针为0</span></span><br><span class="line">        flag = <span class="number">-1</span>  <span class="comment">## 作为开始和边界的标志，来判断是否开始转</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> s:</span><br><span class="line">            res[curRow] += c</span><br><span class="line">            <span class="keyword">if</span> curRow == <span class="number">0</span> <span class="keyword">or</span> curRow == numRows - <span class="number">1</span>: <span class="comment">## 当刚开始或者已经完要转弯</span></span><br><span class="line">                flag = -flag</span><br><span class="line">            curRow += flag  <span class="comment">## 更改数组内小数组的坐标</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>.join(res)</span><br></pre></td></tr></table></figure><p><strong>7. 整数反转</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverse</span><span class="params">(self, x: int)</span> -&gt; int:</span></span><br><span class="line">        rev = <span class="number">0</span></span><br><span class="line">        y = abs(x)  <span class="comment">## 先取正</span></span><br><span class="line">        <span class="keyword">while</span> y != <span class="number">0</span>:</span><br><span class="line">            pop = y % <span class="number">10</span>  <span class="comment">## 获取最后一个数字</span></span><br><span class="line">            y = int(y/<span class="number">10</span>) <span class="comment">## 获取剩下的数字</span></span><br><span class="line">            <span class="comment">## 判断溢出，当rev&gt;max/10,一定会溢出；或者rev=max/10,那么pop&gt;7就会溢出</span></span><br><span class="line">            <span class="comment">## 7 = max%10  -8= min%10</span></span><br><span class="line">            <span class="keyword">if</span> rev &gt; (pow(<span class="number">2</span>,<span class="number">31</span>)<span class="number">-1</span>)/<span class="number">10</span> <span class="keyword">or</span> (rev &gt; (pow(<span class="number">2</span>,<span class="number">31</span>)<span class="number">-1</span>)/<span class="number">10</span> <span class="keyword">and</span> pop &gt; <span class="number">7</span>):</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            <span class="comment">## 判断溢出，当rev&lt;min/10,一定会溢出；或者rev=min/10,那么pop&lt;-8就会溢出</span></span><br><span class="line">            <span class="keyword">if</span> rev &lt; -pow(<span class="number">2</span>,<span class="number">31</span>)/<span class="number">10</span> <span class="keyword">or</span> (rev &lt; -pow(<span class="number">2</span>,<span class="number">31</span>)/<span class="number">10</span> <span class="keyword">and</span> pop &lt; <span class="number">-8</span>):</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">            <span class="comment">## </span></span><br><span class="line">            rev = rev * <span class="number">10</span> + pop <span class="comment">## 反转逐个求和</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> rev <span class="keyword">if</span> x&gt;<span class="number">0</span> <span class="keyword">else</span> -rev <span class="comment">## 加符号</span></span><br></pre></td></tr></table></figure><p><strong>12. 整数转罗马数字</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">intToRoman</span><span class="params">(self, num: int)</span> -&gt; str:</span></span><br><span class="line">        list1 = [<span class="number">1000</span>,<span class="number">900</span>,<span class="number">500</span>,<span class="number">400</span>,<span class="number">100</span>,<span class="number">90</span>,<span class="number">50</span>,<span class="number">40</span>,<span class="number">10</span>,<span class="number">9</span>,<span class="number">5</span>,<span class="number">4</span>,<span class="number">1</span>]</span><br><span class="line">        list2 = [<span class="string">'M'</span>,<span class="string">'CM'</span>,<span class="string">'D'</span>,<span class="string">'CD'</span>,<span class="string">'C'</span>,<span class="string">'XC'</span>,<span class="string">'L'</span>,<span class="string">'XL'</span>,<span class="string">'X'</span>,<span class="string">'IX'</span>,<span class="string">'V'</span>,<span class="string">'IV'</span>,<span class="string">'I'</span>]</span><br><span class="line">        result = <span class="string">""</span></span><br><span class="line">        length = len(list1)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">            <span class="keyword">while</span> num &gt;= list1[i]:  <span class="comment">## 贪心算法</span></span><br><span class="line">                result += list2[i]</span><br><span class="line">                num -= list1[i]</span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p><strong>13. 罗马数字转整数</strong>:把一个小值放在大值的左边，就是做减法，否则为加法。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">romanToInt</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        luoma = &#123;</span><br><span class="line">            <span class="string">'I'</span>:<span class="number">1</span>,</span><br><span class="line">            <span class="string">'V'</span>:<span class="number">5</span>,</span><br><span class="line">            <span class="string">'X'</span>:<span class="number">10</span>,</span><br><span class="line">            <span class="string">'L'</span>:<span class="number">50</span>,</span><br><span class="line">            <span class="string">'C'</span>:<span class="number">100</span>,</span><br><span class="line">            <span class="string">'D'</span>:<span class="number">500</span>,</span><br><span class="line">            <span class="string">'M'</span>:<span class="number">1000</span></span><br><span class="line">        &#125;</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        value = <span class="number">0</span></span><br><span class="line">        length = len(s)</span><br><span class="line">        <span class="keyword">while</span>(i &lt; length<span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span>  luoma[s[i]] &gt;= luoma[s[i+<span class="number">1</span>]]:  <span class="comment">## 正常顺序做加法</span></span><br><span class="line">                value += luoma[s[i]]</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment">## 否则把一个小值放在大值的左边，就是做减法</span></span><br><span class="line">                value -= luoma[s[i]]</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i == length<span class="number">-1</span>: <span class="comment">## 最后一个数的时候，需要加上</span></span><br><span class="line">            value += luoma[s[i]]</span><br><span class="line">        <span class="keyword">return</span> value</span><br></pre></td></tr></table></figure><p><strong>14. 最长公共前缀</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestCommonPrefix</span><span class="params">(self, strs: List[str])</span> -&gt; str:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> strs: <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">        length,count = len(strs[<span class="number">0</span>]),len(strs)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">            c = strs[<span class="number">0</span>][i]  <span class="comment">## 第一个字符的第i位，纵向扫描</span></span><br><span class="line">            <span class="comment">## any()函数用于判断给定的可迭代参数 iterable 是否全部为 False，则返回 False，如果有一个为 True，则返回 True。</span></span><br><span class="line">            <span class="keyword">if</span> any(i==len(strs[j]) <span class="keyword">or</span> strs[j][i] != c <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,count)):</span><br><span class="line">                <span class="keyword">return</span> strs[<span class="number">0</span>][:i]</span><br><span class="line">        <span class="keyword">return</span> strs[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p><strong>26. 删除有序数组中的重复项</strong> 双指针法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeDuplicates</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        lens = len(nums)</span><br><span class="line">        <span class="keyword">if</span> lens == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        i = <span class="number">0</span> <span class="comment">## 慢指针</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,lens): <span class="comment">## 快指针，当相等，则j不断增加</span></span><br><span class="line">            <span class="keyword">if</span> nums[i] != nums[j]: <span class="comment">## 遇到不重复项</span></span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">                nums[i] = nums[j] <span class="comment">## ，将其赋值到i+1这个位置上</span></span><br><span class="line">        <span class="keyword">return</span> i+<span class="number">1</span></span><br></pre></td></tr></table></figure><p><strong>27. 移除元素</strong> 双指针法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeElement</span><span class="params">(self, nums: List[int], val: int)</span> -&gt; int:</span> </span><br><span class="line">        i= <span class="number">0</span>  <span class="comment">## 慢指针</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n): <span class="comment">## 快指针</span></span><br><span class="line">            <span class="keyword">if</span> nums[j] != val: <span class="comment">## 当不是删除的元素，就转移到慢指针所在的位置</span></span><br><span class="line">                nums[i] = nums[j]</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> i</span><br></pre></td></tr></table></figure><p><strong>28. 实现 strStr()</strong> 暴力破解法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">strStr</span><span class="params">(self, haystack: str, needle: str)</span> -&gt; int:</span></span><br><span class="line">        n,L = len(haystack),len(needle)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> start <span class="keyword">in</span> range(n - L + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> haystack[start:start+L] == needle:</span><br><span class="line">                <span class="keyword">return</span> start</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><p><strong>64. 最小路径和</strong> 动态规划<br>状态dp[i][j]表示的是从开始到(i,j)的最小路径和</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minPathSum</span><span class="params">(self, grid: List[List[int]])</span> -&gt; int:</span></span><br><span class="line">        m,n = len(grid),len(grid[<span class="number">0</span>])  <span class="comment">## 计算行数列数</span></span><br><span class="line"></span><br><span class="line">        dp = [[<span class="number">0</span>]*n <span class="keyword">for</span> _ <span class="keyword">in</span> range(m)] <span class="comment">## 里面乘的是列，外面的是行</span></span><br><span class="line">        dp[<span class="number">0</span>][<span class="number">0</span>] = grid[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment">## 初始化开始</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,n):  <span class="comment">## 填充第1列，每次前面和当前的和</span></span><br><span class="line">            dp[<span class="number">0</span>][j] =  dp[<span class="number">0</span>][j<span class="number">-1</span>] + grid[<span class="number">0</span>][j]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,m):  <span class="comment">## 填充第1行，每次前面和当前的和</span></span><br><span class="line">            dp[i][<span class="number">0</span>] = dp[i<span class="number">-1</span>][<span class="number">0</span>] + grid[i][<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,n):</span><br><span class="line">                <span class="comment">## 最小的和当前的和</span></span><br><span class="line">                dp[i][j] = min(dp[i<span class="number">-1</span>][j],dp[i][j<span class="number">-1</span>]) + grid[i][j]  </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dp[m<span class="number">-1</span>][n<span class="number">-1</span>]  <span class="comment">## 返回最后一个值</span></span><br></pre></td></tr></table></figure><p><strong>62. 不同路径</strong><br>状态dp[i][j]表示的是从左上角走到(i,j)的路径数量</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">uniquePaths</span><span class="params">(self, m: int, n: int)</span> -&gt; int:</span></span><br><span class="line">        dp = [[<span class="number">1</span>] * n] + [[<span class="number">1</span>] + [<span class="number">0</span>]*(n<span class="number">-1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> range(m<span class="number">-1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,n):</span><br><span class="line">                dp[i][j] = dp[i<span class="number">-1</span>][j] + dp[i][j<span class="number">-1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> dp[m<span class="number">-1</span>][n<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> leetcode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础算法题总结8--排序算法</title>
      <link href="/posts/t0008/"/>
      <url>/posts/t0008/</url>
      
        <content type="html"><![CDATA[<h2 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h2><p>排序算法有冒泡排序、选择排序、插入排序、希尔排序、归并排序、快速排序、堆排序、计数排序、桶排序、基数排序等等</p><h2 id="Python实现"><a href="#Python实现" class="headerlink" title="Python实现"></a>Python实现</h2><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><p>不断的遍历，每次将最大的换到未排序的数组的最后</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bubbleSort</span><span class="params">(self,arr)</span>:</span></span><br><span class="line">        n = len(arr)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(n-i<span class="number">-1</span>):</span><br><span class="line">                <span class="keyword">if</span> arr[j] &gt; arr[j+<span class="number">1</span>]: <span class="comment">## 将大的一直向后换</span></span><br><span class="line">                    arr[j],arr[j+<span class="number">1</span>] = arr[j+<span class="number">1</span>],arr[j]</span><br><span class="line">        <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure><h3 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h3><p>不断的遍历，每次选取一个最小的放在未排序的数组的前面</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">selectSort</span><span class="params">(self,arr)</span>:</span></span><br><span class="line">        n = len(arr)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">            min = i</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>,n):</span><br><span class="line">                <span class="keyword">if</span> arr[j] &lt; arr[min]: <span class="comment">## 当找到一个比当前最小的还小的数就进行替换</span></span><br><span class="line">                    min = j</span><br><span class="line">                    arr[min],arr[j] = arr[j],arr[min]</span><br><span class="line">        <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure><h3 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h3><p>从数组的最开始遍历，选中一个值，比较该放在哪个位置就插入,采用in-place排序</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insertSort</span><span class="params">(self,arr)</span>:</span></span><br><span class="line">        n = len(arr)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,n): <span class="comment">## 将第一个元素当做一个有序序列</span></span><br><span class="line">            j = i<span class="number">-1</span></span><br><span class="line">            current = arr[i]  <span class="comment">## 当前要插入的值</span></span><br><span class="line">            <span class="comment">## 遍历前面有序序列，判断小于便插入，继续向前比较</span></span><br><span class="line">            <span class="keyword">while</span> j&gt;=<span class="number">0</span> <span class="keyword">and</span> arr[j] &gt; current: </span><br><span class="line">                arr[j+<span class="number">1</span>] = arr[j]</span><br><span class="line">                j -= <span class="number">1</span></span><br><span class="line">            arr[j+<span class="number">1</span>] = current  <span class="comment">## 当比较结束，将当前的值插入到相应的位置上       </span></span><br><span class="line">        <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(self, arr, left,right)</span>:</span></span><br><span class="line">        pivot = arr[left] <span class="comment">## 选中第一个元素作为基准</span></span><br><span class="line">        <span class="keyword">while</span> left&lt;right:</span><br><span class="line">            <span class="keyword">while</span> left&lt;right <span class="keyword">and</span> arr[right] &gt;= pivot:  <span class="comment">## 从后往前查找小于基准的</span></span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">            arr[left],arr[right] = arr[right],arr[left]</span><br><span class="line">            <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> arr[left] &lt;= pivot:  <span class="comment">## 从前往后找大于基准的</span></span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            arr[left],arr[right] = arr[right],arr[left]</span><br><span class="line">        arr[right] = pivot</span><br><span class="line">        <span class="keyword">return</span> right</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">quickSort</span><span class="params">(self, arr, left, right)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> left &lt; right:</span><br><span class="line">            q = self.partition(arr,left,right)</span><br><span class="line">            self.quickSort(arr,left,q<span class="number">-1</span>)</span><br><span class="line">            self.quickSort(arr,q+<span class="number">1</span>,right)</span><br><span class="line">        <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">quickSort2</span><span class="params">(self, arr)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(arr) &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> arr</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            temp = arr[<span class="number">0</span>]</span><br><span class="line">            more = [i <span class="keyword">for</span> i <span class="keyword">in</span> arr[<span class="number">1</span>:] <span class="keyword">if</span> i &gt; temp]</span><br><span class="line">            less = [i <span class="keyword">for</span> i <span class="keyword">in</span> arr[<span class="number">1</span>:] <span class="keyword">if</span> i &lt;= temp]</span><br><span class="line">        <span class="keyword">return</span> self.quickSort2(less) + [temp] + self.quickSort2(more)</span><br></pre></td></tr></table></figure><h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">heapify</span><span class="params">(self,arr,n,i)</span>:</span>  <span class="comment">## 构建大根堆</span></span><br><span class="line">        largest = i  <span class="comment">## 最大值的节点位置</span></span><br><span class="line">        left = <span class="number">2</span>*i + <span class="number">1</span>  <span class="comment">## 左孩子的位置</span></span><br><span class="line">        right = <span class="number">2</span>*i + <span class="number">2</span> <span class="comment">## 右孩子的位置</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## 如果改成arr[i] &gt; arr[left],则是小顶堆</span></span><br><span class="line">        <span class="keyword">if</span> left &lt; n <span class="keyword">and</span> arr[i] &lt; arr[left]: <span class="comment">## 有左孩子且左孩子比当前的大</span></span><br><span class="line">            largest = left</span><br><span class="line">        <span class="keyword">if</span> right &lt; n <span class="keyword">and</span> arr[largest] &lt; arr[right]: <span class="comment">## 有右孩子且右孩子比当前的大</span></span><br><span class="line">            largest = right</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> largest != i:  <span class="comment">## 如果左右孩子有比当前节点大的值，则要交换</span></span><br><span class="line">            arr[i],arr[largest] = arr[largest],arr[i]</span><br><span class="line">            self.heapify(arr,n,largest) <span class="comment">## 继续遍历下一个节点</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">heapSort</span><span class="params">(self,arr)</span>:</span>  <span class="comment">## 对堆顶进行抽取，实现堆排序</span></span><br><span class="line">        n = len(arr)</span><br><span class="line">        <span class="comment">## 建立一个大根堆</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n,<span class="number">-1</span>,<span class="number">-1</span>): <span class="comment">## 自底向上建堆</span></span><br><span class="line">            self.heapify(arr,n,i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">## 交换元素，堆顶元素和最后的元素互换</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>,<span class="number">0</span>,<span class="number">-1</span>):</span><br><span class="line">            arr[i],arr[<span class="number">0</span>] = arr[<span class="number">0</span>],arr[i]</span><br><span class="line">            <span class="comment">#需注意建堆起始位置为0，堆形状大小为i，即建堆的规模逐步缩小。已经有序的部分不需要再改动</span></span><br><span class="line">            self.heapify(arr,i,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> arr</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> leetcode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习知识总结5--逻辑回归</title>
      <link href="/posts/k0005/"/>
      <url>/posts/k0005/</url>
      
        <content type="html"><![CDATA[<h2 id="逻辑回归-对数几率回归"><a href="#逻辑回归-对数几率回归" class="headerlink" title="逻辑回归(对数几率回归)"></a>逻辑回归(对数几率回归)</h2><p>一句话概述：逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降求解参数，来达到将数据二分类的目的。</p><p>属于对数线性模型，最大熵模型也属于对数线性模型。</p><p>基本假设：假设数据服从伯努利分布<br>逻辑回归的第二个假设是假设样本为正的概率是由sigmod函数计算的</p><p>逻辑回归的思路是，先拟合决策边界(不局限于线性，还可以是多项式)，再建立这个边界与分类的概率联系，从而得到了二分类情况下的概率。</p><p>对数几率函数是任意阶可导的凸函数，有许多数值优化算法都可以求出最优解。</p><h3 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h3><p>伯努利分布:是一个离散型概率分布，若成功，则随机变量取值1；若失败，随机变量取值为0。成功概率记为p，失败为q = 1-p。<br>$$<br>f_X (x) = p^x (1-p)^{(1-x)} = \left\{<br>\begin{matrix}<br>p &amp; if &amp;x = 1 \\<br>q &amp; if &amp;x = 0<br>\end{matrix}<br>\right.<br>$$</p><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><p>通过已知的结果去反推最大概率导致该结果的参数</p><p>利用实验结果$D=\{x_1,x_2,…,x_N\}$,得到某个参数值$\theta$,使得样本出现的概率最大。<br>$L(\theta) = P(D|\theta) = P(x_1,x_2,…,x_N|\theta) = \prod_{i=1}^N P(x_i|\theta)$<br>求解参数值$\hat\theta$，则就是求解最大值即，$\hat\theta= argmax(L(\theta))$</p><p>用似然法估计参数：建立(对数)似然函数；求解使似然函数最大的参数(求偏导等于0)即为结果</p><p>作用：<br>1、想要让每一个样本的预测都要得到最大的概率<br>2、对极大似然函数取对数以后相当于对数损失函数，对数损失函数的训练求解参数的速度是比较快的，只和x,y有关，比较稳定</p><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p>梯度是一个方向向量，表示某一函数在某点处沿着该方向(梯度的方向)变化最快。<br>梯度记为$grad(x,y)$,其数学定义为<br>$grad(x,y) = \nabla{f(x,y)} = \{\frac{\partial{f(x,y)}}{\partial x} ,\frac{\partial{f(x,y)}}{\partial y} \}$</p><p><strong>梯度下降法的步骤</strong>：</p><p>1、 初始化回归系数向量w<br>2、 重复迭代max_iter次：<br>计算本次迭代的梯度$\frac{\partial{J(w)}}{\partial{w}}$，计算$w_tmp = w - \alpha*梯度$，用$w_tmp$更新w</p><h3 id="逻辑回归的损失函数"><a href="#逻辑回归的损失函数" class="headerlink" title="逻辑回归的损失函数"></a>逻辑回归的损失函数</h3><p>设<br>$P(Y=1|x) = \pi(x)$<br>$P(Y=0|x) = 1-\pi(x)$<br>那么$\pi(x) = \frac{1}{1+exp(-x)}$，也就是sigmod function</p><p>所以逻辑回归的似然函数为：<br>$L(w) = \prod_{i=1}^N [\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$</p><p>逻辑回归的目标是寻找使得对数似然函数最大的参数值，也就是求解参数值$\hat w$,其中$\hat w= argmax(L(w))$。参数w是一个向量，也就是$w = [w_1,w_2,w_3,…,w_n]^T$。(因为我们希望结果的可能性最大，所以要极大化似然函数来获取想要的参数)</p><p>为了便于求解，将似然函数取对数，其对数似然函数为：<br>$$<br>\begin{align}<br>log(L(w)) &amp;= \sum_{i=1}^{N}[y_i log(\pi(x_i)) + (1-y_i)log(1-\pi(x_i))] \\<br>&amp;= \sum_{i=1}^{N}[y_i log(\frac{\pi(x_i)}{1-\pi(x_i)}) + log(1-\pi(x_i))] \\<br>&amp;= \sum_{i=1}^{N}[y_i(w \cdot x_i) - log(1+ exp(w \cdot x_i))]<br>\end{align}<br>$$</p><p>至此我们就可以对对数似然函数求极大值，得到参数w的估计值。</p><p>通用的解法就是计算损失函数关于参数向量每个$w_i$的偏导，令这些导函数为0，建立方程组求解。即</p><p>$$\frac{\partial{L(w)}}{\partial{w}} ==&gt; \left\{<br>\begin{array}{rcl}<br>\frac{\partial{L(w)}}{\partial{w_1}} = 0 \\<br>\frac{\partial{L(w)}}{\partial{w_2}} = 0 \\<br>\frac{\partial{L(w)}}{\partial{w_3}} = 0 \\<br>\vdots \\<br>\frac{\partial{L(w)}}{\partial{w_n}} = 0<br>\end{array} \right.  ==&gt;<br>\begin{array}{rcl}<br>w_1 = …\\<br>w_2 = …\\<br>w_3 = …\\<br>\vdots \\<br>w_n = …<br>\end{array}<br>$$</p><p>因为这样计求解太过复杂，为了简化运算，所以引入梯度。因为我们要极大化对数似然函数，得到w的估计值，所以可以使用梯度上升法来求解。</p><p>机器学习中的损失函数衡量的是模型预测错误的程度，我们学习的目标就是最小化损失函数，所以如果取整个数据集上的平均对数似然损失，则就可以得到$J(w) = - \frac{1}{N} logL(w)$(将对数似然函数前面添加负号取平均得到)，我们将$J(w)$作为我们的损失函数。</p><p>在LR模型中，最大化似然函数和最小化损失函数实际上是等价的。</p><p>因为我们的损失函数是$J(w) = - \frac{1}{N} \sum_{i=1}^{N}[y_i(w \cdot x_i) - log(1+ exp(w \cdot x_i))]$,那么他对于w求偏导的导函数为</p><p>$$<br>\begin{align}<br>\frac{\partial{J(w)}}{\partial{w}} &amp;= - \frac{1}{N} \sum_{i=1}^{N}(y_i x_i - \frac{exp(w \cdot x_i)x_i}{1+exp(w \cdot x_i)}) \\<br>&amp;= - \frac{1}{N} \sum_{i=1}^{N}[x_i (y_i - \frac{exp(w \cdot x_i)}{1+exp(w \cdot x_i)}] \\<br>&amp;= - \frac{1}{N} \sum_{i=1}^{N}[x_i (y_i - \frac{1}{1+exp(- w \cdot x_i)}] \\<br>&amp;= - \frac{1}{N} \sum_{i=1}^{N}(x_i(y_i - \pi(x))) \\<br>&amp;= - \frac{1}{N} \sum_{i=1}^{N}(x_i(y_i - \hat{y_i})) \\<br>&amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i(\hat{y_i} - y_i)) \\<br>&amp;= \frac{1}{N} \sum_{i=1}^{N}(x_i(\pi(x_i) - y_i))<br>\end{align}<br>$$<br>其中$\hat{y_i} = \pi(x)$</p><p>所以用梯度下降法求解的表达式为$w := w - \alpha \frac{1}{N} \sum_{i=1}^{N}(x_i(\hat{y_i} - y_i)) $</p><p>那么梯度下降法求解逻辑回归的伪代码为：</p><figure class="highlight nix"><table><tr><td class="code"><pre><span class="line">[X,y] = load_data()</span><br><span class="line"><span class="attr">w</span> = rand(m,<span class="number">1</span>) <span class="comment">## 初始化参数</span></span><br><span class="line"><span class="attr">alpha</span> = <span class="number">0.3</span></span><br><span class="line"><span class="attr">max_iter</span> = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">for i <span class="keyword">in</span> range(max_iter):</span><br><span class="line">    <span class="attr">y_pred</span> = sigmod_func(x)</span><br><span class="line">    <span class="attr">grad</span> = X*(y - y_pred)</span><br><span class="line">    <span class="attr">w_tmp</span> = w + alpha*grad</span><br><span class="line">    <span class="attr">w</span> = w_tmp</span><br></pre></td></tr></table></figure><h3 id="细节思考"><a href="#细节思考" class="headerlink" title="细节思考"></a>细节思考</h3><p><span color = "red">为什么使用极大似然函数作为损失函数？</span><br>损失函数一般有4种，分别是0-1损失、平方损失、绝对损失以及对数损失。<br>将极大似然函数取对数以后等同于对数损失函数，在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。<br>因为更新速度只和x,y相关，和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。<br>为什么不选平方损失呢？<br>因为梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。</p><p>我们使用对数几率的意义在哪？通过上述推导我们可以看到 Logistic 回归实际上是使用线性回归模型的预测值逼近分类任务真实标记的对数几率，其优点有：</p><p>直接对分类的概率建模，无需实现假设数据分布，从而避免了假设分布不准确带来的问题；<br>不仅可预测出类别，还能得到该预测的概率，这对一些利用概率辅助决策的任务很有用；<br>对数几率函数是任意阶可导的凸函数，有许多数值优化算法都可以求出最优解。</p><p>LR FM区别<br>FM（factorization machine）模型是一种基于矩阵分解的机器学习模型，对于稀疏数据具有很好的学习能力</p><p>FM模型与LR模型的区别在于引进了特征组合</p><p>LR里面做归一化、离散化之类的好处：<br>因为如果数据不进行归一化，那么使用梯度下降法寻找最优解的时候，有可能是走之字形路线，需要迭代很多次才能收敛，甚至可能不能收敛，所以需要对数据做归一化，这样能收敛并且加快收敛的速度。</p><p>逻辑回归特征重复了一维会有什么影响</p><p>逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？</p><p>结论：如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。<br>但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。<br>如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。</p><p>优点缺点</p><p>优点：形式简单、模型的可解释性非常好；模型效果不错；训练速度较快；资源占用小，尤其是内存；方便输出结果调整。<br>缺点：准确率不是很高；很难处理数据不平衡的问题；处理非线性数据比较麻烦；逻辑回归本身无法筛选特征</p><p>FM相比于LR的优势（自交叉、稀疏特征不太影响训练、可以得到embedding，进行高维交叉，推理未出现过的特征组合）</p><p>训练时样本不平衡问题如何解决；小样本问题如何解决</p><p>参考资料：<br><a href="https://blog.csdn.net/sinat_33231573/article/details/99709837" target="_blank" rel="noopener">https://blog.csdn.net/sinat_33231573/article/details/99709837</a><br><a href="https://www.cnblogs.com/mfmdaoyou/p/7392352.html" target="_blank" rel="noopener">https://www.cnblogs.com/mfmdaoyou/p/7392352.html</a><br><a href="https://my.oschina.net/u/4687686/blog/4665928" target="_blank" rel="noopener">https://my.oschina.net/u/4687686/blog/4665928</a><br><a href="https://www.cnblogs.com/XDU-Lakers/p/11853034.html" target="_blank" rel="noopener">https://www.cnblogs.com/XDU-Lakers/p/11853034.html</a>  ++<br><a href="https://www.cnblogs.com/XDU-Lakers/p/11853034.html" target="_blank" rel="noopener">https://www.cnblogs.com/XDU-Lakers/p/11853034.html</a><br><a href="https://zhuanlan.zhihu.com/p/38853901" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/38853901</a></p>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java</title>
      <link href="/posts/36523/"/>
      <url>/posts/36523/</url>
      
        <content type="html"><![CDATA[<h2 id="java基础"><a href="#java基础" class="headerlink" title="java基础"></a>java基础</h2><p>Arrays.sort实现原理和Collection实现原理<br>foreach和while的区别(编译之后)<br>线程池的种类，区别和使用场景<br>分析线程池的实现原理和线程的调度过程<br>线程池如何调优<br>线程池的最大线程数目根据什么确定<br>动态代理的几种方式<br>HashMap的并发问题<br>了解LinkedHashMap的应用吗<br>反射的原理，反射创建类实例的三种方式是什么？<br>cloneable接口实现原理，浅拷贝or深拷贝<br>Java NIO使用<br>hashtable和hashmap的区别及实现原理，hashmap会问到数组索引，hash碰撞怎么解决<br>arraylist和linkedlist区别及实现原理<br>反射中，Class.forName和ClassLoader区别<br>String，Stringbuffer，StringBuilder的区别？<br>有没有可能2个不相等的对象有相同的hashcode<br>简述NIO的最佳实践，比如netty，mina<br>TreeMap的实现原理</p><h2 id="JVM相关"><a href="#JVM相关" class="headerlink" title="JVM相关"></a>JVM相关</h2><p>类的实例化顺序，比如父类静态数据，构造函数，字段，子类静态数据，构造函数，字段，它们的执行顺序<br>JVM内存分代<br>Java 8的内存分代改进<br>JVM垃圾回收机制，何时触发MinorGC等操作<br>jvm中一次完整的GC流程（从ygc到fgc）是怎样的，重点讲讲对象如何晋升到老年代，几种主要的jvm参数等<br>你知道哪几种垃圾收集器，各自的优缺点，重点讲下cms，g1<br>新生代和老生代的内存回收策略<br>Eden和Survivor的比例分配等<br>深入分析了Classloader，双亲委派机制<br>JVM的编译优化<br>对Java内存模型的理解，以及其在并发中的应用<br>指令重排序，内存栅栏等<br>OOM错误，stackoverflow错误，permgen space错误<br>JVM常用参数<br>tomcat结构，类加载器流程<br>volatile的语义，它修饰的变量一定线程安全吗<br>g1和cms区别,吞吐量优先和响应优先的垃圾收集器选择<br>说一说你对环境变量classpath的理解？如果一个类不在classpath下，为什么会抛出ClassNotFoundException异常，如果在不改变这个类路径的前提下，怎样才能正确加载这个类？<br>说一下强引用、软引用、弱引用、虚引用以及他们之间和gc的关系</p><h2 id="JUC-并发相关"><a href="#JUC-并发相关" class="headerlink" title="JUC/并发相关"></a>JUC/并发相关</h2><p>ThreadLocal用过么，原理是什么，用的时候要注意什么<br>Synchronized和Lock的区别<br>synchronized 的原理，什么是自旋锁，偏向锁，轻量级锁，什么叫可重入锁，什么叫公平锁和非公平锁<br>concurrenthashmap具体实现及其原理，jdk8下的改版<br>用过哪些原子类，他们的参数以及原理是什么<br>cas是什么，它会产生什么问题（ABA问题的解决，如加入修改次数、版本号）<br>如果让你实现一个并发安全的链表，你会怎么做<br>简述ConcurrentLinkedQueue和LinkedBlockingQueue的用处和不同之处<br>简述AQS的实现原理<br>countdowlatch和cyclicbarrier的用法，以及相互之间的差别?<br>concurrent包中使用过哪些类？分别说说使用在什么场景？为什么要使用？<br>LockSupport工具<br>Condition接口及其实现原理<br>Fork/Join框架的理解<br>jdk8的parallelStream的理解<br>分段锁的原理,锁力度减小的思考</p><h2 id="Spring"><a href="#Spring" class="headerlink" title="Spring"></a>Spring</h2><p>Spring AOP与IOC的实现原理<br>Spring的beanFactory和factoryBean的区别<br>为什么CGlib方式可以对接口实现代理？<br>RMI与代理模式<br>Spring的事务隔离级别，实现原理<br>对Spring的理解，非单例注入的原理？它的生命周期？循环注入的原理，aop的实现原理，说说aop中的几个术语，它们是怎么相互工作的？<br>Mybatis的底层实现原理<br>MVC框架原理，他们都是怎么做url路由的<br>spring boot特性，优势，适用场景等<br>quartz和timer对比<br>spring的controller是单例还是多例，怎么保证并发的安全</p><h2 id="分布式相关"><a href="#分布式相关" class="headerlink" title="分布式相关"></a>分布式相关</h2><p>Dubbo的底层实现原理和机制<br>描述一个服务从发布到被消费的详细过程<br>分布式系统怎么做服务治理<br>接口的幂等性的概念<br>消息中间件如何解决消息丢失问题<br>Dubbo的服务请求失败怎么处理<br>重连机制会不会造成错误<br>对分布式事务的理解<br>如何实现负载均衡，有哪些算法可以实现？<br>Zookeeper的用途，选举的原理是什么？<br>数据的垂直拆分水平拆分。<br>zookeeper原理和适用场景<br>zookeeper watch机制<br>redis/zk节点宕机如何处理<br>分布式集群下如何做到唯一序列号<br>如何做一个分布式锁<br>用过哪些MQ，怎么用的，和其他mq比较有什么优缺点，MQ的连接是线程安全的吗<br>MQ系统的数据如何保证不丢失<br>列举出你能想到的数据库分库分表策略；分库分表后，如何解决全表查询的问题。</p><h2 id="算法和数据结构以及设计模式"><a href="#算法和数据结构以及设计模式" class="headerlink" title="算法和数据结构以及设计模式"></a>算法和数据结构以及设计模式</h2><p>海量url去重类问题（布隆过滤器）<br>数组和链表数据结构描述，各自的时间复杂度<br>二叉树遍历<br>快速排序<br>BTree相关的操作<br>在工作中遇到过哪些设计模式，是如何应用的<br>hash算法的有哪几种，优缺点，使用场景<br>什么是一致性hash<br>paxos算法<br>在装饰器模式和代理模式之间，你如何选择，请结合自身实际情况聊聊<br>代码重构的步骤和原因，如何理解重构到模式？</p><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><p>MySQL InnoDB存储的文件结构<br>索引树是如何维护的？<br>数据库自增主键可能的问题<br>MySQL的几种优化<br>mysql索引为什么使用B+树<br>数据库锁表的相关处理<br>索引失效场景<br>高并发下如何做到安全的修改同一行数据，乐观锁和悲观锁是什么，INNODB的行级锁有哪2种，解释其含义<br>数据库会死锁吗，举一个死锁的例子，mysql怎么解决死锁</p><h2 id="Redis-amp-缓存相关"><a href="#Redis-amp-缓存相关" class="headerlink" title="Redis&amp;缓存相关"></a>Redis&amp;缓存相关</h2><p>Redis的并发竞争问题如何解决了解Redis事务的CAS操作吗<br>缓存机器增删如何对系统影响最小，一致性哈希的实现<br>Redis持久化的几种方式，优缺点是什么，怎么实现的<br>Redis的缓存失效策略<br>缓存穿透的解决办法<br>redis集群，高可用，原理<br>mySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据<br>用Redis和任意语言实现一段恶意登录保护的代码，限制1小时内每用户Id最多只能登录5次<br>redis的数据淘汰策略</p><h2 id="网络相关"><a href="#网络相关" class="headerlink" title="网络相关"></a>网络相关</h2><p>http1.0和http1.1有什么区别<br>TCP/IP协议<br>TCP三次握手和四次挥手的流程，为什么断开连接要4次,如果握手只有两次，会出现什么<br>TIME_WAIT和CLOSE_WAIT的区别<br>说说你知道的几种HTTP响应码<br>当你用浏览器打开一个链接的时候，计算机做了哪些工作步骤<br>TCP/IP如何保证可靠性，数据包有哪些数据组成<br>长连接与短连接<br>Http请求get和post的区别以及数据包格式<br>简述tcp建立连接3次握手，和断开连接4次握手的过程；关闭连接时，出现TIMEWAIT过多是由什么原因引起，是出现在主动断开方还是被动断开方。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>maven解决依赖冲突,快照版和发行版的区别<br>Linux下IO模型有几种，各自的含义是什么<br>实际场景问题，海量登录日志如何排序和处理SQL操作，主要是索引和聚合函数的应用<br>实际场景问题解决，典型的TOP K问题<br>线上bug处理流程<br>如何从线上日志发现问题<br>linux利用哪些命令，查找哪里出了问题（例如io密集任务，cpu过度）<br>场景问题，有一个第三方接口，有很多个线程去调用获取数据，现在规定每秒钟最多有10个线程同时调用它，如何做到。<br>用三个线程按顺序循环打印abc三个字母，比如abcabcabc。<br>常见的缓存策略有哪些，你们项目中用到了什么缓存系统，如何设计的<br>设计一个秒杀系统，30分钟没付款就自动关闭交易（并发会很高）<br>请列出你所了解的性能测试工具<br>后台系统怎么防止请求重复提交？</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Python基础</title>
      <link href="/posts/14702/"/>
      <url>/posts/14702/</url>
      
        <content type="html"><![CDATA[<h4 id="java和Python的区别"><a href="#java和Python的区别" class="headerlink" title="java和Python的区别"></a>java和Python的区别</h4><p>Python比java简单，学习成本低，开发效率高<br>java运行效率比Python高<br>java是一种静态类型的语言，Python则是动态类型的语言<br>java中的所有变量需要先声明类型才能使用，Python中的变量不需要声明类型<br>java编译后才能运行，Python直接就可以运行</p><h4 id="Python的生成器"><a href="#Python的生成器" class="headerlink" title="Python的生成器"></a>Python的生成器</h4><p>python生成器是一个返回可以迭代对象的函数,可以被用作控制循环的迭代行为。生成器类似于返回值为数组的一个函数,这个函数可以接受参数,可以被调用,一般的函数会返回包括所有数值的数组,生成器一次只能返回一个值,这样消耗的内存将会大大减小。</p><h4 id="python中is和-的区别"><a href="#python中is和-的区别" class="headerlink" title="python中is和==的区别"></a>python中is和==的区别</h4><p>is是用来判断两个变量引用的对象是否为同一个,==用于判断引用对象的值是否相等。可以通过id()函数查看引用对象的地址。</p><h4 id="python方法解析顺序"><a href="#python方法解析顺序" class="headerlink" title="python方法解析顺序"></a>python方法解析顺序</h4><p>Python的方法解析顺序优先级从高到低为:实例本身-&gt;类-&gt;继承类<br>(继承关系越近,越先定义,优先级越高)</p><h4 id="pytorch中cuda-作用-两个Tensor-一个加了cuda-一个没加-相加后很怎样"><a href="#pytorch中cuda-作用-两个Tensor-一个加了cuda-一个没加-相加后很怎样" class="headerlink" title="pytorch中cuda()作用,两个Tensor,一个加了cuda(),一个没加,相加后很怎样?"></a>pytorch中cuda()作用,两个Tensor,一个加了cuda(),一个没加,相加后很怎样?</h4><p>cuda()将操作对象放在GPU内存中,加了cuda()的Tensor放在GPU内存中而没加的Tensor放在CPU内存中,所以这两个Tensor相加会报错。</p><h4 id="python中dict和list的区别-dict的内部实现"><a href="#python中dict和list的区别-dict的内部实现" class="headerlink" title="python中dict和list的区别,dict的内部实现"></a>python中dict和list的区别,dict的内部实现</h4><ul><li>dict查找速度快,占用的内存较大,list查找速度慢,占用内存较小</li><li>dict不能用来存储有序集合</li><li>dict用{}表示,list用[]表示。<br>dict是通过hash表实现的,dict为一个数组,数组的索引键是通过hash函数处理后得到的,hash函数的目的是使键值均匀的分布在数组中。</li></ul><h4 id="python-dict按照value进行排序"><a href="#python-dict按照value进行排序" class="headerlink" title="python dict按照value进行排序"></a>python dict按照value进行排序</h4><p>按照value从大到小排序:<br>sorted(d.items(),key = lambda x:x[1],reverse = True)</p><p>按照value从小到大排序:<br>sorted(d.items(),key = lambda x:x[1],reverse =False)</p><h4 id="虚函数和纯虚函数的区别"><a href="#虚函数和纯虚函数的区别" class="headerlink" title="虚函数和纯虚函数的区别"></a>虚函数和纯虚函数的区别</h4><ul><li>含有纯虚函数的类称为抽象类,只含有虚函数的类不能称为抽象类。</li><li>虚函数可以直接被使用,也可以被子类重载以后以多态形式调用,而纯虚函数必须在子类中实现该函数才可使用,因为纯虚函数在基类中只有声明而没有定义。</li><li>虚函数必须实现,对虚函数来说父类和子类都有各自的版本。</li></ul><h4 id="Python多进程"><a href="#Python多进程" class="headerlink" title="Python多进程"></a>Python多进程</h4><p>方式一: os.fork()<br>方式二:使用multiprocessing模块:创建Process的实例，传入任务执行函数作为参数<br>方式三:使用multiprocessing模块:派生Process的子类，重写run方法<br>方式四:使用进程池Pool</p><h4 id="python-锁"><a href="#python-锁" class="headerlink" title="python 锁"></a>python 锁</h4><p>全局解释器锁(GIL):<br>功能：在 CPython 解释器中执行的每一个 Python线程，都会先锁住自己，以阻止别的线程执行。<br>CPython 不可能容忍一个线程一直独占解释器，它会轮流执行 Python 线程。这样一来，用户看到的就是“伪”并行，即 Python 线程在交替执行，来模拟真正并行的线程。</p><p>每个线程在执行时候都需要先获取GIL，保证同一时刻只有一个线程可以执行代码，即同一时刻只有一个线程使用CPU，也就是说多线程并不是真正意义上的同时执行。</p><p>什么时候会释放Gil锁？</p><ul><li>遇到像 i/o操作这种 会有时间空闲情况 造成cpu闲置的情况会释放Gil</li><li>会有一个专门ticks进行计数 一旦ticks数值达到100 这个时候释放Gil锁 线程之间开始竞争Gil锁(说明:ticks这个数值可以进行设置来延长或者缩减获得Gil锁的线程使用cpu的时间)</li></ul><p>互斥锁和Gil锁的关系？<br>Gil锁  : 保证同一时刻只有一个线程能使用到cpu<br>互斥锁 : 多线程时,保证修改共享数据时有序的修改,不会产生数据修改混乱</p><p>同步锁<br>死锁<br>递归锁<br>乐观锁<br>悲观锁</p><p>常用的加锁方式：互斥锁、可重入锁、迭代死锁、互相调用死锁、自旋锁。</p><h4 id="python类初始化过程"><a href="#python类初始化过程" class="headerlink" title="python类初始化过程"></a>python类初始化过程</h4><p>在执行类的初始化的时候，比如a = A()，先是执行了<strong>new</strong>(cls) 方法，根据传入的cls来分配空间，然后再执行对象的<strong>init</strong>(self) 方法。cls为类，self为类实例，类函数需要传入self，类似于C++里的this指针。</p><h4 id="python中的new和init区别"><a href="#python中的new和init区别" class="headerlink" title="python中的new和init区别"></a>python中的new和<strong>init</strong>区别</h4><p>均是Python面向对象语言中的函数<br><strong>new</strong>是在实例创建之前被调用的，因为它的任务就是创建实例然后返回该实例对象，是个静态方法。<br><strong>init</strong>是当实例对象创建完成后被调用的，然后设置对象属性的一些初始值，通常用在初始化一个类实例的时候。是一个实例方法。</p><h4 id="python什么特性你觉得比较好"><a href="#python什么特性你觉得比较好" class="headerlink" title="python什么特性你觉得比较好"></a>python什么特性你觉得比较好</h4><p>迭代器</p><h4 id="python的迭代器"><a href="#python的迭代器" class="headerlink" title="python的迭代器"></a>python的迭代器</h4><p>在Python中，迭代器是遵循迭代协议的对象，用来表示一连串数据流。重复调用迭代器的<strong>next</strong>()方法（或将其传给内置函数 next()）将逐个返回数据流中的项。当没有数据可用时则将引发 StopIteration 异常。</p><h4 id="xrange和range的区别"><a href="#xrange和range的区别" class="headerlink" title="xrange和range的区别"></a>xrange和range的区别</h4><p>xrange返回的是一个可迭代的对象(生成器)<br>range返回的是一个列表</p><p>xrange函数在Python3中已经取消，在python3中，range()这种实现被移除了，保留了xrange()的实现，且将xrange()重新命名成range()。所以Python3不能使用xrange，只能使用rangexrange</p><h4 id="dictionary中key和value的限制"><a href="#dictionary中key和value的限制" class="headerlink" title="dictionary中key和value的限制"></a>dictionary中key和value的限制</h4><p>键必须是唯一的，但值则不必。值可以取任何数据类型。<br>key可以用字符串、整数、浮点型、true|false、元组来定义，不可以使用列表和字典来定义</p><h4 id="Python-元组和列表的区别，元组怎么转列表"><a href="#Python-元组和列表的区别，元组怎么转列表" class="headerlink" title="Python 元组和列表的区别，元组怎么转列表"></a>Python 元组和列表的区别，元组怎么转列表</h4><p>元组比列表的运算速度快，而且元组的数据比较安全，因为元组是不可改变的，为了包括其内容不被外部接口修改；列表是可以更改的。</p><p>直接转换<br>lists = [1,2,3]<br>tup = (1,2)<br>元组–&gt;列表 list(tup)<br>列表–&gt;元组 tuple(lists)</p><h4 id="python-垃圾回收机制"><a href="#python-垃圾回收机制" class="headerlink" title="python 垃圾回收机制"></a>python 垃圾回收机制</h4><p>python内部实现了自动的垃圾回收，而无需用户考虑什么时候销毁一个对象或变量。python的垃圾回收机制是以引用计数机制为主，标记-清除和分代收集两种机制为辅的综合方案。</p><p>引用计数–&gt;标记-清除–&gt;分代回收</p><p>引用计数：当一个对象的引用计数为0，那说明没有任何变量使用了该对象，所以这个对象就是垃圾对象被销毁。引用计数简单方便但是也有弊端：计数占用内存、在相互引用的对象中计数永远不会变为0，所以还引入了标记-清除和分代回收。<br>还会存在循环引用的问题，当若干对象相互引用，会造成每个对象的引用计数都不会为0，这样这些对象所占用的内存永远不会被回收。</p><p>标记-清除：为了解决引用计数器循环引用的不足。GC会对所有对象打上标记，分为活动对象和非活动对象。Python中通过一个directed graph进行对所有对象的标记维护。沿着途中的所有引用，从root object遍历，能到达的对象为活动对象，无法到达的对象为非活动对象。GC首先会把这些标记为非活动对象的object回收</p><p>分代回收：分代回收是一种以空间换时间的操作方式，Python将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，Python将内存分为了3“代”，分别为年轻代（第0代）、中年代（第1代）、老年代（第2代），他们对应的是3个链表，它们的垃圾收集频率与对象的存活时间的增大而减小。新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。同时，分代回收是建立在标记清除技术基础之上。分代回收同样作为Python的辅助垃圾收集技术处理那些容器对象.</p><p>之字形打印</p><p>多层MLP可以拿来训练吗，初始化全是同一值有什么问题，relu的缺点，tensorflow如何处理不可导的情况</p><p>一个矩阵从左上到右下最小路径和<br>一颗二叉树最长路径</p><p>不用加减乘除做加法(只做各位相加不进位；做进位；将前面两个结果相加)<br>堆排序<br>最大的子序和(简单的动态规划)<br>学生成绩的第1000名(成绩是整数且有最大值，可以统计一遍信息，然后二分查找)</p><p>聊聊项目中的一些bad case</p><p>给一个任务：垃圾邮件分类，分阶段和流程细致描述一下每一步应该怎么做，以及最终要优化的效果。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>基础算法题总结7--分治算法</title>
      <link href="/posts/t0007/"/>
      <url>/posts/t0007/</url>
      
        <content type="html"><![CDATA[<h2 id="分治算法"><a href="#分治算法" class="headerlink" title="分治算法"></a>分治算法</h2><p>将原问题递归地分成若干个子问题，直到子问题满足边界条件，停止递归。将子问题解决然后合并得到原来原问题的答案。</p><h2 id="相关练习"><a href="#相关练习" class="headerlink" title="相关练习"></a>相关练习</h2><p>LeetCode中相关的题目：</p><ul><li><a href="https://leetcode-cn.com/problems/majority-element/" target="_blank" rel="noopener">169.多数元素</a>:可以使用哈希表、排序、随机化、分治算法以及Boyer-Moore 投票算法</li><li><a href="https://leetcode-cn.com/problems/maximum-subarray/" target="_blank" rel="noopener">53.最大子序和</a>:可以使用动态规划和分治算法</li><li><a href="https://leetcode-cn.com/problems/kth-largest-element-in-an-array/" target="_blank" rel="noopener">215.数组中的第K个最大元素</a></li></ul><h3 id="题目详细描述"><a href="#题目详细描述" class="headerlink" title="题目详细描述"></a>题目详细描述</h3><p><strong>多数元素</strong>：<br> 分治算法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>****:</p>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> leetcode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础算法题总结6--python输入输出格式</title>
      <link href="/posts/t0006/"/>
      <url>/posts/t0006/</url>
      
        <content type="html"><![CDATA[<h2 id="python涉及的函数"><a href="#python涉及的函数" class="headerlink" title="python涉及的函数"></a>python涉及的函数</h2><p>python的基础数据结构有：列表、元组、序列、集合、字典等<br>Python的输入输出包括了从控制台上获取的数据以及链表、栈、树等自输入的设计</p><p>map()函数会根据提供的函数对指定序列做映射，返回一个迭代器<br>map(function, iterable, …)</p><ul><li>function 函数</li><li>iterable 一个或多个序列</li></ul><p>input()函数接受一个标准输入数据，返回为 string 类型<br>input([prompt])</p><ul><li>prompt: 提示信息</li></ul><p>list()方法用于将元组或字符串转换为列表，返回列表<br>list(seq)</p><ul><li>seq: 要转换为列表的元组或字符串</li></ul><p>join()方法用于将序列中的元素以指定的字符连接生成一个新的字符串，返回通过指定字符连接序列中元素后生成的新字符串<br>str.join(sequence)</p><ul><li>sequence: 要连接的元素序列</li></ul><p>测试用例如果以行的形式输入<br>list(map(int,input().split()))</p><p>有可能会出现EOF的错误，为了避免这个错误的出现，需要将输入套上try…except…异常检测</p><h2 id="具体的题目练习"><a href="#具体的题目练习" class="headerlink" title="具体的题目练习"></a>具体的题目练习</h2><p>其中的a+b和字符串的输入输出问题的来源是<a href="https://ac.nowcoder.com/acm/contest/5657#question" target="_blank" rel="noopener">牛客网</a></p><h3 id="a-b-1"><a href="#a-b-1" class="headerlink" title="a+b(1)"></a>a+b(1)</h3><p>输入描述: 输入包括两个正整数a,b(1 &lt;= a, b &lt;= 10^9),输入数据包括多组。<br>输入格式：<br>1 5<br>10 20<br>输出格式：<br>6<br>30</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    a = line.split()</span><br><span class="line">    print(int(a[<span class="number">0</span>]) + int(a[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        a = list(map(int,input().split(<span class="string">' '</span>)))</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    print(a[<span class="number">0</span>]+a[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h3 id="a-b-2"><a href="#a-b-2" class="headerlink" title="a+b(2)"></a>a+b(2)</h3><p>输入描述:<br>输入第一行包括一个数据组数t(1 &lt;= t &lt;= 100)<br>接下来每行包括两个正整数a,b(1 &lt;= a, b &lt;= 10^9)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        t = int(input())</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(t):</span><br><span class="line">            nums = list(map(int, input().split(<span class="string">' '</span>)))</span><br><span class="line">            print(nums[<span class="number">0</span>]+nums[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        t = int(input())</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(t):</span><br><span class="line">            a,b = map(int, input().split(<span class="string">' '</span>))</span><br><span class="line">            print(a+b)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h3 id="a-b-3"><a href="#a-b-3" class="headerlink" title="a+b(3)"></a>a+b(3)</h3><p>输入描述:输入包括两个正整数a,b(1 &lt;= a, b &lt;= 10^9),输入数据有多组, 如果输入为0 0则结束输入</p><p>输入格式：<br>1 5<br>10 20<br>0 0</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        a,b = map(int, input().split())</span><br><span class="line">        <span class="keyword">if</span> a == b == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        print(a+b)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h3 id="a-b-4"><a href="#a-b-4" class="headerlink" title="a+b(4)"></a>a+b(4)</h3><p>输入描述:<br>输入数据包括多组。<br>每组数据一行,每行的第一个整数为整数的个数n(1 &lt;= n &lt;= 100), n为0的时候结束输入。<br>接下来n个正整数,即需要求和的每个正整数。</p><p>输入格式：<br>4 1 2 3 4<br>5 1 2 3 4 5<br>0</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        nums = list(map(int, input().split()))</span><br><span class="line">        <span class="keyword">if</span> nums[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(sum(nums[<span class="number">1</span>:]))</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h3 id="a-b-5"><a href="#a-b-5" class="headerlink" title="a+b(5)"></a>a+b(5)</h3><p>输入描述:<br>输入的第一行包括一个正整数t(1 &lt;= t &lt;= 100), 表示数据组数。<br>接下来t行, 每行一组数据。<br>每行的第一个整数为整数的个数n(1 &lt;= n &lt;= 100)。<br>接下来n个正整数, 即需要求和的每个正整数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        n = int(input())</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(n):</span><br><span class="line">            nums = list(map(int, input().split()))</span><br><span class="line">            print(sum(nums[<span class="number">1</span>:]))</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h3 id="a-b-6"><a href="#a-b-6" class="headerlink" title="a+b(6)"></a>a+b(6)</h3><p>输入描述:<br>输入数据有多组, 每行表示一组输入数据。<br>每行的第一个整数为整数的个数n(1 &lt;= n &lt;= 100)。<br>接下来n个正整数, 即需要求和的每个正整数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        nums = list(map(int, input().split()))</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    n = nums[<span class="number">0</span>]</span><br><span class="line">    sumvalue = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,n+<span class="number">1</span>):</span><br><span class="line">        sumvalue += nums[i]</span><br><span class="line">    print(sumvalue)</span><br></pre></td></tr></table></figure><h3 id="a-b-7"><a href="#a-b-7" class="headerlink" title="a+b(7)"></a>a+b(7)</h3><p>输入描述:<br>输入数据有多组, 每行表示一组输入数据。<br>每行不定有n个整数，空格隔开。(1 &lt;= n &lt;= 100)。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        nums = list(map(int, input().split()))</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="comment">## print(sum(nums))</span></span><br><span class="line">    sumValue = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> nums:</span><br><span class="line">        sumValue += _</span><br><span class="line">    print(sumValue)</span><br></pre></td></tr></table></figure><h3 id="字符串排序-1"><a href="#字符串排序-1" class="headerlink" title="字符串排序(1)"></a>字符串排序(1)</h3><p>输入描述：<br>输入有两行，第一行n<br>第二行是n个空格隔开的字符串</p><p>输出描述:<br>输出一行排序后的字符串，空格隔开，无结尾空格</p><p>输入:<br>5<br>c d a bb e</p><p>输出:<br>a bb c d e</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        n = int(input())</span><br><span class="line">        arr = input().split(<span class="string">' '</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    arr = sorted(arr)</span><br><span class="line">    out = <span class="string">' '</span>.join(arr)</span><br><span class="line">    print(out)</span><br></pre></td></tr></table></figure><h3 id="字符串排序-2"><a href="#字符串排序-2" class="headerlink" title="字符串排序(2)"></a>字符串排序(2)</h3><p>输入描述：<br>多个测试用例，每个测试用例一行。<br>每行通过空格隔开，有n个字符，n＜100<br>输出描述:<br>对于每组测试用例，输出一行排序过的字符串，每个字符串通过空格隔开</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        arr = input().split(<span class="string">' '</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    arr = sorted(arr)</span><br><span class="line">    out = <span class="string">' '</span>.join(arr)</span><br><span class="line">    print(out)</span><br></pre></td></tr></table></figure><h3 id="字符串排序-3"><a href="#字符串排序-3" class="headerlink" title="字符串排序(3)"></a>字符串排序(3)</h3><p>输入描述:<br>多个测试用例，每个测试用例一行。<br>每行通过,隔开，有n个字符，n＜100<br>输出描述:<br>对于每组用例输出一行排序后的字符串，用’,’隔开，无结尾空格</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        arr = input().split(<span class="string">','</span>)</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    arr = sorted(arr)</span><br><span class="line">    out = <span class="string">','</span>.join(arr)</span><br><span class="line">    print(out)</span><br></pre></td></tr></table></figure><h3 id="自测本地提交为0"><a href="#自测本地提交为0" class="headerlink" title="自测本地提交为0"></a>自测本地提交为0</h3><p>输入描述:<br>输入有多组测试用例，每组空格隔开两个整数<br>输出描述:<br>对于每组数据输出一行两个整数的和</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        a,b = map(int,input().split(<span class="string">' '</span>))</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    print(a+b)</span><br></pre></td></tr></table></figure><h3 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ListNode</span>:</span>  <span class="comment">## 链表的节点</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,val)</span>:</span></span><br><span class="line">        self.val = val</span><br><span class="line">        self.next = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinkList</span>:</span>  <span class="comment">## 链表</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.head = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initList</span><span class="params">(self,data)</span>:</span>  <span class="comment">## 初始化链表，将列表转换成链表</span></span><br><span class="line">        self.head = ListNode(data[<span class="number">0</span>])</span><br><span class="line">        start = self.head</span><br><span class="line">        p = self.head</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> data[<span class="number">1</span>:]:</span><br><span class="line">            node = ListNode(i)</span><br><span class="line">            p.next = node</span><br><span class="line">            p = p.next</span><br><span class="line">        <span class="keyword">return</span> start</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printList</span><span class="params">(self,head)</span>:</span>  <span class="comment">## 输出链表</span></span><br><span class="line">        <span class="keyword">if</span> head == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        node = head</span><br><span class="line">        <span class="keyword">while</span> node != <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> node.next:</span><br><span class="line">                print(node.val, end=<span class="string">'--&gt;'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(node.val, end=<span class="string">''</span>)</span><br><span class="line">            node = node.next</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    l = LinkList() <span class="comment">## 初始化链表对象</span></span><br><span class="line">    data1 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">    data2 = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">    l1 = l.initList(data1) <span class="comment">## 创建链表</span></span><br><span class="line">    l2 = l.initList(data2)</span><br><span class="line">    l.printList(l1)  <span class="comment">## 输出链表</span></span><br><span class="line">    print(<span class="string">"\r"</span>)</span><br><span class="line">    l.printList(l2)</span><br><span class="line">    print(<span class="string">"\r"</span>)</span><br></pre></td></tr></table></figure><h3 id="栈-后进先出"><a href="#栈-后进先出" class="headerlink" title="栈 (后进先出)"></a>栈 (后进先出)</h3><p>使用顺序表实现栈</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stack</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.stack = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">empty</span><span class="params">(self)</span>:</span>  <span class="comment">## 判断栈是否为空</span></span><br><span class="line">        <span class="keyword">return</span> len(self.stack) == <span class="number">0</span></span><br><span class="line">        <span class="comment"># return self.stack == []</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">length</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.stack)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self,item)</span>:</span>  <span class="comment">##进栈</span></span><br><span class="line">        self.stack.append(item)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pop</span><span class="params">(self)</span>:</span>  <span class="comment">## 出栈</span></span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> self.stack.pop()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">top</span><span class="params">(self)</span>:</span>  <span class="comment">## 查看栈顶元素</span></span><br><span class="line">      </span><br><span class="line">        <span class="keyword">if</span> self.empty():</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> self.stack[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    stack = Stack() <span class="comment">## 初始化栈对象</span></span><br><span class="line">    stack.push(<span class="number">1</span>)</span><br><span class="line">    stack.push(<span class="number">2</span>)</span><br><span class="line">    stack.push(<span class="number">3</span>)</span><br><span class="line">    stack.push(<span class="number">4</span>)</span><br><span class="line">    print(stack.pop()) <span class="comment">## 4</span></span><br><span class="line">    print(stack.top()) <span class="comment">## 3</span></span><br><span class="line">    print(stack.length()) <span class="comment">## 3</span></span><br></pre></td></tr></table></figure><h3 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h3>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> leetcode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础算法题总结5--滑动窗口</title>
      <link href="/posts/t0005/"/>
      <url>/posts/t0005/</url>
      
        <content type="html"><![CDATA[<h2 id="滑动窗口"><a href="#滑动窗口" class="headerlink" title="滑动窗口"></a>滑动窗口</h2><h2 id="相关练习"><a href="#相关练习" class="headerlink" title="相关练习"></a>相关练习</h2><p>LeetCode中相关的题目：</p><ul><li><a href="https://leetcode-cn.com/problems/longest-substring-without-repeating-characters/" target="_blank" rel="noopener">3.无重复字符的最长子串</a>:哈希集合+双指针+滑动窗口</li><li><a href="https://leetcode-cn.com/problems/minimum-window-substring/" target="_blank" rel="noopener">76.最小覆盖子串</a>:滑动窗口以及优化 哈希表+双指针+滑动窗口</li><li><a href="https://leetcode-cn.com/problems/sliding-window-maximum/" target="_blank" rel="noopener">239.滑动窗口最大值</a>：堆 + 滑动窗口</li><li><a href="https://leetcode-cn.com/problems/longest-substring-with-at-least-k-repeating-characters/" target="_blank" rel="noopener">395.至少有 K 个重复字符的最长子串</a></li><li><a href="">340.至多包含 K 个不同字符的最长子串</a> </li></ul><h3 id="题目详细描述"><a href="#题目详细描述" class="headerlink" title="题目详细描述"></a>题目详细描述</h3><p><strong>无重复字符的最长子串</strong>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        occ = set() <span class="comment">## 哈希集合，记录每个字符是否出现过</span></span><br><span class="line">        rk,ans = <span class="number">-1</span>,<span class="number">0</span> <span class="comment">## rk表示右指针</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span>:</span><br><span class="line">                occ.remove(s[i<span class="number">-1</span>]) <span class="comment">## 左指针向右移动一位，则将哈希集合中元素去除</span></span><br><span class="line">            <span class="comment">## 不断移动右指针，判断右指针元素是否出现过，直到到达边界或者元素已存在</span></span><br><span class="line">            <span class="keyword">while</span> rk+<span class="number">1</span>&lt;n <span class="keyword">and</span> s[rk+<span class="number">1</span>] <span class="keyword">not</span> <span class="keyword">in</span> occ:</span><br><span class="line">                occ.add(s[rk+<span class="number">1</span>])</span><br><span class="line">                rk += <span class="number">1</span></span><br><span class="line">            <span class="comment">## 当前获取的长度如果比之前保存的长则更新结果</span></span><br><span class="line">            <span class="keyword">if</span> rk-i+<span class="number">1</span> &gt; ans:</span><br><span class="line">                ans = rk-i+<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><p><strong>最小覆盖子串</strong>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minWindow</span><span class="params">(self, s: str, t: str)</span> -&gt; str:</span></span><br><span class="line">        n,m = len(s),len(t)</span><br><span class="line">        <span class="keyword">if</span> n &lt; m: <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line"></span><br><span class="line">        need = collections.defaultdict(int) <span class="comment">## 初始化字典，哈希表</span></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> t:</span><br><span class="line">            need[c] += <span class="number">1</span></span><br><span class="line">        need_cnt = m</span><br><span class="line">        res = [<span class="number">0</span>,n] <span class="comment">## res是用来记录起点终点，初始化为[0,n]</span></span><br><span class="line"></span><br><span class="line">        left = <span class="number">0</span> <span class="comment">## 左指针</span></span><br><span class="line">        <span class="keyword">for</span> right,c <span class="keyword">in</span> enumerate(s): <span class="comment">## 增加右边界使得滑动窗口包含t</span></span><br><span class="line">            <span class="keyword">if</span> need[c] &gt; <span class="number">0</span>:</span><br><span class="line">                need_cnt -= <span class="number">1</span></span><br><span class="line">            need[c] -= <span class="number">1</span></span><br><span class="line">            <span class="comment">## 1、找到右指针，目前字符串中已经包含了t中的所有的值</span></span><br><span class="line">            <span class="keyword">if</span> need_cnt == <span class="number">0</span>:</span><br><span class="line">                <span class="comment">## 2、开始收缩左指针，将多余的不在t中的元素去掉</span></span><br><span class="line">                <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                    <span class="keyword">if</span> need[s[left]] == <span class="number">0</span>: <span class="comment">## 当再去掉就不全包含t了，到达边界条件</span></span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                    need[s[left]] += <span class="number">1</span></span><br><span class="line">                    left += <span class="number">1</span>  <span class="comment">## 左指针右移</span></span><br><span class="line">                <span class="comment">## 当找到最小长度的起点终点，更新res</span></span><br><span class="line">                <span class="keyword">if</span> right - left &lt; res[<span class="number">1</span>] - res[<span class="number">0</span>]:</span><br><span class="line">                    res = [left,right]</span><br><span class="line">                <span class="comment">## 3、已经更新结果，然后将left向右移动一位，开始下一次循环,寻找新的满足条件的滑动窗口</span></span><br><span class="line">                need[s[left]] += <span class="number">1</span></span><br><span class="line">                need_cnt += <span class="number">1</span> <span class="comment">## 此时移除的一定是所需的元素，所以需要增加1</span></span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">        <span class="comment">## 如果res没有更新，就返回"",否则就返回更新的结果</span></span><br><span class="line">        <span class="keyword">return</span> s[res[<span class="number">0</span>]:res[<span class="number">1</span>]+<span class="number">1</span>] <span class="keyword">if</span> res != [<span class="number">0</span>,n] <span class="keyword">else</span> <span class="string">""</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> leetcode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础算法题总结4--二分查找</title>
      <link href="/posts/t0004/"/>
      <url>/posts/t0004/</url>
      
        <content type="html"><![CDATA[<h2 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h2><p>基本写法</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binarySearch</span><span class="params">(nums,target)</span>:</span></span><br><span class="line">    n = len(nums)</span><br><span class="line">    left,right = <span class="number">0</span>,n<span class="number">-1</span></span><br><span class="line">    mid = (left + right) // <span class="number">2</span> <span class="comment">## 取中心</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        <span class="keyword">if</span> nums[mid] == target:</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">        <span class="keyword">if</span> nums[mid] &gt;= target:</span><br><span class="line">            right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><h2 id="相关练习"><a href="#相关练习" class="headerlink" title="相关练习"></a>相关练习</h2><p>LeetCode中相关的题目：</p><ul><li><a href="https://leetcode-cn.com/problems/search-in-rotated-sorted-array/" target="_blank" rel="noopener">33.搜索旋转排序数组</a>: 二分查找，一半有序，另一半半有序</li><li><a href="https://leetcode-cn.com/problems/find-first-and-last-position-of-element-in-sorted-array/" target="_blank" rel="noopener">34.在排序数组中查找元素的第一个和最后一个位置</a></li></ul><h3 id="题目详细描述"><a href="#题目详细描述" class="headerlink" title="题目详细描述"></a>题目详细描述</h3><p><strong>搜索旋转排序数组</strong>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, nums: List[int], target: int)</span> -&gt; int:</span></span><br><span class="line">        n = len(nums)</span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = n<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span>(left &lt;= right): </span><br><span class="line">            mid = (left + right) // <span class="number">2</span> <span class="comment">## 取中心</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] == target: <span class="comment">## 当找到就返回所在的下标</span></span><br><span class="line">                <span class="keyword">return</span> mid</span><br><span class="line">            <span class="comment">## 当[l,mid-1]是有序数组的时候 0表示最左侧，也可以使用left</span></span><br><span class="line">            <span class="keyword">if</span> nums[<span class="number">0</span>] &lt;= nums[mid]: </span><br><span class="line">                <span class="comment">## 如果target在[0,mid]中，则在[l,mid-1]中查找</span></span><br><span class="line">                <span class="keyword">if</span> nums[<span class="number">0</span>] &lt;= target &lt; nums[mid]: </span><br><span class="line">                    right  = mid - <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>: <span class="comment">## 否则在[mid+1,r]中查找</span></span><br><span class="line">                    left = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>: <span class="comment">## 当[mid, r] 是有序数组的时候, n-1表示最右侧，也可以使用right</span></span><br><span class="line">                <span class="comment">## 如果target在[mid+1,r]中，则在[mid+1,r]中查找</span></span><br><span class="line">                <span class="keyword">if</span> nums[mid] &lt;  target &lt;= nums[n<span class="number">-1</span>]: </span><br><span class="line">                    left = mid + <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>: <span class="comment">## 否则在[l,mid-1]中查找</span></span><br><span class="line">                    right = mid - <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> leetcode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础算法题总结3--动态规划、中心扩散、Manacher算法</title>
      <link href="/posts/t0003/"/>
      <url>/posts/t0003/</url>
      
        <content type="html"><![CDATA[<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><p>动态规划中的子问题往往不是相互独立的,子问题是重叠的，在求解的过程中，许多子问题的解被反复的使用。为了避免重复计算，动态规划算法使用填表来保存子问题解的方法。</p><p>适合动态规划来解决的问题具有三个特点：<strong>最优化原理</strong>、<strong>无后效性</strong>、<strong>有重叠子问题</strong>。</p><p>1、 <strong>最优化原理</strong>：如果问题的最优解所包含的子问题的解也是最优的，就称该问题具有最优子结构，即满足最优化原理。<br>2、 <strong>无后效性</strong>：即某阶段状态一旦确定，就不受这个状态以后决策的影响。也就是说，某状态以后的过程不会影响以前的状态，只与当前状态有关。<br>3、 <strong>有重叠子问题</strong>：即子问题之间是不独立的，一个子问题在下一阶段决策中可能被多次使用到。(该性质并不是动态规划适用的必要条件，但是如果没有这条性质，动态规划算法同其他算法相比就不具备优势。)</p><p>使用动态规划做题时需要考虑：</p><ol><li>状态的定义</li><li>状态如何转移，状态转移方程 [大问题的最优解如何由小问题的最优解得到。]</li><li>初始化</li><li>结果的输出</li><li>优化空间</li></ol><h2 id="python涉及的知识点"><a href="#python涉及的知识点" class="headerlink" title="python涉及的知识点"></a>python涉及的知识点</h2><p>状态矩阵初始化：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dp = [[<span class="literal">False</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br></pre></td></tr></table></figure><h2 id="相关练习"><a href="#相关练习" class="headerlink" title="相关练习"></a>相关练习</h2><p>LeetCode中相关的题目：</p><ul><li><a href="https://leetcode-cn.com/problems/longest-palindromic-substring/" target="_blank" rel="noopener">5.最长回文子串</a>:动态规划、中心扩散、Manacher算法</li><li><a href="https://leetcode-cn.com/problems/palindromic-substrings/" target="_blank" rel="noopener">647.回文子串</a>:可以使用动态规划，但是推荐中心扩散和Manacher算法</li><li><a href="https://leetcode-cn.com/problems/longest-palindromic-subsequence/" target="_blank" rel="noopener">516.最长回文子序列</a></li><li><a href="https://leetcode-cn.com/problems/maximum-subarray/" target="_blank" rel="noopener">53.最大子序和</a>:可以使用动态规划和分治算法</li><li><a href="https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/" target="_blank" rel="noopener">121. 买卖股票的最佳时机</a></li></ul><h3 id="题目详细描述"><a href="#题目详细描述" class="headerlink" title="题目详细描述"></a>题目详细描述</h3><p><strong>最长回文子串</strong>：这个题目可以使用动态规划、中心扩散、Manacher算法，其中动态规划和中心扩散必须掌握，动态规划和中心扩散的复杂度一样，但是空间复杂度不同，且中心扩散的效果好。<br><strong>1、动态规划</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestPalindrome</span><span class="params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        <span class="keyword">if</span> n &lt; <span class="number">2</span>: <span class="comment">## 只有一个字符的时候，本身就是最长回文子串</span></span><br><span class="line">            <span class="keyword">return</span> s </span><br><span class="line">        <span class="comment">## 确定状态：是否是回文子串</span></span><br><span class="line">        dp = [[<span class="literal">False</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)] <span class="comment">## 初始化不能写错，</span></span><br><span class="line">        res = <span class="string">""</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> leng <span class="keyword">in</span> range(n): <span class="comment">## 枚举子串的长度</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(n): <span class="comment">## 枚举子串的初始位置i，那么子串的结束位置就是i+leng</span></span><br><span class="line">                j = i + leng</span><br><span class="line">                <span class="keyword">if</span> j &gt;= n: <span class="comment">## 当无法获取这个长度的子串则结束</span></span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> leng == <span class="number">0</span>: <span class="comment">## 初始化动态规划表的对角线</span></span><br><span class="line">                    dp[i][j] = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">elif</span> leng == <span class="number">1</span>: <span class="comment">## 只有两个元素时，只要两者相同就是回文串</span></span><br><span class="line">                    dp[i][j] = (s[i] == s[j])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = (dp[i+<span class="number">1</span>][j<span class="number">-1</span>] <span class="keyword">and</span> s[i] == s[j]) <span class="comment">## 状态转移</span></span><br><span class="line">                <span class="keyword">if</span> dp[i][j] <span class="keyword">and</span> leng + <span class="number">1</span> &gt; len(res): </span><br><span class="line">                <span class="comment">## 当是回文串，且已经字符串最长的时候，那么这个子串先被认定为最长回文子串</span></span><br><span class="line">                    res = s[i:j+<span class="number">1</span>] <span class="comment">## i是初始位置，j+1是结束位置</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p><strong>2、中心扩散</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">expandAround</span><span class="params">(self,s,left,right)</span>:</span> <span class="comment">## 前后指针来判断回文串的开始截止位置</span></span><br><span class="line">            <span class="comment">## 没有到边界并且左右字符相同，则继续像两遍扩散</span></span><br><span class="line">            <span class="keyword">while</span> left &gt;= <span class="number">0</span> <span class="keyword">and</span> right &lt; len(s) <span class="keyword">and</span> s[left] == s[right]:</span><br><span class="line">                left -= <span class="number">1</span> <span class="comment">## 向左</span></span><br><span class="line">                right += <span class="number">1</span> <span class="comment">## 向右</span></span><br><span class="line">            <span class="comment">## 因为遍历到不同的元素的上一步已经+-1，所以这里需要再-+1获取正确的位置</span></span><br><span class="line">            <span class="keyword">return</span> left+<span class="number">1</span> ,right<span class="number">-1</span> </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestPalindrome</span><span class="params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        start, end = <span class="number">0</span>,<span class="number">0</span> <span class="comment">## 记录开始、结束位置</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            left1,right1 = self.expandAround(s,i,i) <span class="comment">## 奇数</span></span><br><span class="line">            left2,right2 = self.expandAround(s,i,i+<span class="number">1</span>) <span class="comment">## 偶数</span></span><br><span class="line">            <span class="keyword">if</span> right1 - left1 &gt; end - start: <span class="comment">## 当找到比当前的回文子串要长，则更新</span></span><br><span class="line">                start,end = left1,right1</span><br><span class="line">            <span class="keyword">if</span> right2 - left2 &gt; end - start: <span class="comment">## 当找到比当前的回文子串要长，则更新</span></span><br><span class="line">                start,end = left2,right2</span><br><span class="line">        <span class="keyword">return</span> s[start:end+<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p><strong>3、Manacher算法</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment"># Manacher 算法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">longestPalindrome</span><span class="params">(self, s: str)</span> -&gt; str:</span></span><br><span class="line">        <span class="comment"># 特判</span></span><br><span class="line">        size = len(s)</span><br><span class="line">        <span class="keyword">if</span> size &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到预处理字符串</span></span><br><span class="line">        t = <span class="string">"#"</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">            t += s[i]</span><br><span class="line">            t += <span class="string">"#"</span></span><br><span class="line">        <span class="comment">## 或者可以简写为 t = '#' + '#'.join(list(s)) + '#'</span></span><br><span class="line">        <span class="comment"># 新字符串的长度</span></span><br><span class="line">        t_len = <span class="number">2</span> * size + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 数组 p 记录了扫描过的回文子串的信息</span></span><br><span class="line">        p = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> range(t_len)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 双指针，它们是一一对应的，须同时更新</span></span><br><span class="line">        max_right = <span class="number">0</span></span><br><span class="line">        center = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 当前遍历的中心最大扩散步数，其值等于原始字符串的最长回文子串的长度</span></span><br><span class="line">        max_len = <span class="number">1</span></span><br><span class="line">        <span class="comment"># 原始字符串的最长回文子串的起始位置，与 max_len 必须同时更新</span></span><br><span class="line">        start = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(t_len):</span><br><span class="line">            <span class="keyword">if</span> i &lt; max_right:</span><br><span class="line">                mirror = <span class="number">2</span> * center - i</span><br><span class="line">                <span class="comment"># 这一行代码是 Manacher 算法的关键所在，要结合图形来理解</span></span><br><span class="line">                p[i] = min(max_right - i, p[mirror])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 下一次尝试扩散的左右起点，能扩散的步数直接加到 p[i] 中</span></span><br><span class="line">            left = i - (<span class="number">1</span> + p[i])</span><br><span class="line">            right = i + (<span class="number">1</span> + p[i])</span><br><span class="line"></span><br><span class="line">            <span class="comment"># left &gt;= 0 and right &lt; t_len 保证不越界</span></span><br><span class="line">            <span class="comment"># t[left] == t[right] 表示可以扩散 1 次</span></span><br><span class="line">            <span class="keyword">while</span> left &gt;= <span class="number">0</span> <span class="keyword">and</span> right &lt; t_len <span class="keyword">and</span> t[left] == t[right]:</span><br><span class="line">                p[i] += <span class="number">1</span></span><br><span class="line">                left -= <span class="number">1</span></span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 根据 max_right 的定义，它是遍历过的 i 的 i + p[i] 的最大者</span></span><br><span class="line">            <span class="comment"># 如果 max_right 的值越大，进入上面 i &lt; max_right 的判断的可能性就越大，这样就可以重复利用之前判断过的回文信息了</span></span><br><span class="line">            <span class="keyword">if</span> i + p[i] &gt; max_right:</span><br><span class="line">                <span class="comment"># max_right 和 center 需要同时更新</span></span><br><span class="line">                max_right = i + p[i]</span><br><span class="line">                center = i</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> p[i] &gt; max_len:</span><br><span class="line">                <span class="comment"># 记录最长回文子串的长度和相应它在原始字符串中的起点</span></span><br><span class="line">                max_len = p[i]</span><br><span class="line">                start = (i - max_len) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">return</span> s[start: start + max_len]</span><br></pre></td></tr></table></figure><p><strong>回文子串</strong><br><strong>1、动态规划</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">countSubstrings</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        res = <span class="string">""</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        dp = [[<span class="literal">False</span>]*n <span class="keyword">for</span> _ <span class="keyword">in</span> range(n)]</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> leng <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">                j = i + leng</span><br><span class="line">                <span class="keyword">if</span> j &gt;= n:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> leng == <span class="number">0</span>:</span><br><span class="line">                    dp[i][j] = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">elif</span> leng == <span class="number">1</span>:</span><br><span class="line">                    dp[i][j] = (s[i] == s[j])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = (dp[i+<span class="number">1</span>][j<span class="number">-1</span>] <span class="keyword">and</span> s[i] == s[j])</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> dp[i][j]:</span><br><span class="line">                    count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> count</span><br></pre></td></tr></table></figure><p><strong>2、中心扩散</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">countSubstrings</span><span class="params">(self, s: str)</span> -&gt; int:</span></span><br><span class="line">        n = len(s)</span><br><span class="line">        <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">midexpand</span><span class="params">(left,right)</span>:</span></span><br><span class="line">            <span class="keyword">while</span> left &gt;= <span class="number">0</span> <span class="keyword">and</span> right &lt; len(s) <span class="keyword">and</span> s[left] == s[right]:</span><br><span class="line">                left -= <span class="number">1</span></span><br><span class="line">                right += <span class="number">1</span></span><br><span class="line">                self.count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            midexpand(i,i)</span><br><span class="line">            midexpand(i,i+<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.count</span><br></pre></td></tr></table></figure><p><strong>3、Manacher算法</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>最大子序和</strong>：<br><strong>1、动态规划</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxSubArray</span><span class="params">(self, nums: List[int])</span> -&gt; int:</span></span><br><span class="line">        <span class="keyword">if</span> len(nums) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> nums[<span class="number">0</span>]      </span><br><span class="line">        pre = <span class="number">0</span></span><br><span class="line">        maxL = nums[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> nums:</span><br><span class="line">            pre = max(item+pre,item)</span><br><span class="line">            maxL = max(maxL,pre)       </span><br><span class="line">        <span class="keyword">return</span> maxL</span><br></pre></td></tr></table></figure><p><strong>121. 买卖股票的最佳时机</strong><br>O(n) O(1)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int])</span> -&gt; int:</span></span><br><span class="line"></span><br><span class="line">        minprice = <span class="number">1e5</span>  <span class="comment">## 初始化最小的价格</span></span><br><span class="line">        maxprofit = <span class="number">0</span>  <span class="comment">## 初始化最大利润，设为为0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> price <span class="keyword">in</span> prices:</span><br><span class="line">            <span class="keyword">if</span> price &lt; minprice:</span><br><span class="line">                minprice = price</span><br><span class="line">            <span class="keyword">if</span> price - minprice &gt; maxprofit:</span><br><span class="line">                maxprofit = price - minprice</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> maxprofit</span><br></pre></td></tr></table></figure><p>O(n) O(n)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int])</span> -&gt; int:</span></span><br><span class="line">        minprice = <span class="number">1e5</span></span><br><span class="line">        maxprofit = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> price <span class="keyword">in</span> prices:</span><br><span class="line">            minprice = min(minprice,price)  <span class="comment">## 更新最低点</span></span><br><span class="line">            maxprofit = max(maxprofit,price - minprice) <span class="comment">## 求差值，更新最大差值</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> maxprofit</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span><span class="params">(self, prices: List[int])</span> -&gt; int:</span></span><br><span class="line">        n = len(prices)</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>: <span class="keyword">return</span> <span class="number">0</span> <span class="comment"># 边界条件</span></span><br><span class="line">        dp = [<span class="number">0</span>] * n</span><br><span class="line">        minprice = prices[<span class="number">0</span>] </span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, n):</span><br><span class="line">            minprice = min(minprice, prices[i]) <span class="comment">## 最小价格</span></span><br><span class="line">            <span class="comment">## 当天的最大利润，是昨天的最大利润或者当天价格减去最小值的值</span></span><br><span class="line">            dp[i] = max(dp[i - <span class="number">1</span>], prices[i] - minprice)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> leetcode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础算法题总结2--回溯法相关</title>
      <link href="/posts/t0002/"/>
      <url>/posts/t0002/</url>
      
        <content type="html"><![CDATA[<h2 id="回溯法-递归的一种"><a href="#回溯法-递归的一种" class="headerlink" title="回溯法(递归的一种)"></a>回溯法(递归的一种)</h2><p>回溯算法的递归使用以及剪枝策略。<br>回溯法：一种通过探索所有可能的候选解来找出所有的解的算法。如果候选解被确认不是一个解（或者至少不是最后一个解），回溯算法会通过在上一步进行一些变化抛弃该解，即回溯并且再次尝试。</p><p><strong>正规写法</strong>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">t = []</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dps</span><span class="params">(cur,n)</span>:</span><span class="comment">## cur表示当前位置，n表示原序列长度</span></span><br><span class="line">    <span class="keyword">if</span> cur == n:</span><br><span class="line">        <span class="comment">## 保存答案</span></span><br><span class="line">        <span class="comment">## 这里保存答案的时候需要考虑的是不能直接保存t的值，而是需要t.copy(),</span></span><br><span class="line">        <span class="comment">## 否则当t改变的时候，保存的内容也是会改变的</span></span><br><span class="line">        <span class="comment">## 【值传递和引用传递、深拷贝和浅拷贝】，需要深拷贝</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    t.append(cur) <span class="comment">## 考虑当前被选中</span></span><br><span class="line">    dps(cur+<span class="number">1</span>,n)</span><br><span class="line">    t.pop()  <span class="comment">## 考虑当前没有被选中</span></span><br><span class="line">    dps(cur+<span class="number">1</span>,n)</span><br><span class="line">dps(<span class="number">0</span>,n)</span><br></pre></td></tr></table></figure><p><strong>简便写法</strong>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dps</span><span class="params">(i,tmp)</span>:</span> <span class="comment">## i表示当前的位置，tmp列表，临时列表，选中则放入，不选中则返回</span></span><br><span class="line">    <span class="comment">## 保存答案</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(i,n): <span class="comment">## 后部分</span></span><br><span class="line">        dps(j+<span class="number">1</span>, tmp + j) <span class="comment">## 从空到全，j表示选中的位置，可以改用此选中位置的数</span></span><br><span class="line">dps(<span class="number">0</span>,[])</span><br></pre></td></tr></table></figure><p>如果涉及剪枝或避免重复元素：</p><ol><li>先进行排序，保证重复的元素放在一起</li><li>然后回溯选取下一个节点的时候判断一下是否和前面的相同，相同的则不用再继续遍历，相当于剪枝；不同则要继续遍历。</li></ol><p><strong>简便写法</strong>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums.sort() <span class="comment">## 需要先进行排序</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dps</span><span class="params">(i,tmp)</span>:</span> <span class="comment">## i表示当前的位置，tmp列表，临时列表，选中则放入，不选中则返回</span></span><br><span class="line">    <span class="comment">## 保存答案 深拷贝</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(i,n): <span class="comment">## 后部分</span></span><br><span class="line">        <span class="keyword">if</span> j &gt; i <span class="keyword">and</span> nums[j] == nums[j<span class="number">-1</span>]:  </span><br><span class="line">        <span class="comment">## 判断当前要选中的这个元素是否和上一个选中的一样，如果一样则略过不用遍历</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        dps(j+<span class="number">1</span>, tmp+ [nums[j]]) <span class="comment">## 从空到全，j表示选中的位置，可以改用此选中位置的数</span></span><br><span class="line">dps(<span class="number">0</span>,[])</span><br></pre></td></tr></table></figure><h2 id="值传递、引用传递、浅拷贝、深拷贝"><a href="#值传递、引用传递、浅拷贝、深拷贝" class="headerlink" title="值传递、引用传递、浅拷贝、深拷贝"></a>值传递、引用传递、浅拷贝、深拷贝</h2><p>值传递和引用传递</p><p>不可变类型在传递参数时都是传值形式，如int,str,tuple类型。<br>可变类型在传递参数时都是引用传递，如list,set,dict类型。指向的都是同一块内存地址</p><p>值传递：当元素被赋予新值或者修改值的时候，是指向了一个新的对象，和原对象无关。当原值没有对象再指向它，就会被python的GC回收。<br>引用传递：指向的都是同一块内存地址，一旦改变值，所有指向这个内存地址的都会改变</p><p>拷贝的内置函数<br>copy()浅拷贝：不拷贝对象的内容，只拷贝子对象的引用，原数据改变，则子对象会改变<br>deepcopy()深拷贝：对子对象的内存也会拷贝一份，对子对象的修改不会影响源对象，原始数据的改变不会影响深拷贝</p><p>a = b: 引用传递<br>a.copy()：浅拷贝<br>copy.copy(a)：浅拷贝<br>copy.deepcopy(a)：深拷贝</p><p>区别nums和nums[:]<br>nums[:] = nums[0:len(nums)] 可以使用[:]来拷贝全部元素<br>nums[:]在计算机中是新分配了一个地址，当其改变时是不会影响到原数据的，相同的是原数据的改变也不会影响数值的改变。</p><h2 id="相关练习"><a href="#相关练习" class="headerlink" title="相关练习"></a>相关练习</h2><p>LeetCode中相关的题目：</p><ul><li><a href="https://leetcode-cn.com/problems/subsets/" target="_blank" rel="noopener">78.子集</a>: 可以使用回溯法，时间复杂度为O(n*2^n),空间复杂度为O(n).</li><li><a href="https://leetcode-cn.com/problems/subsets-ii/" target="_blank" rel="noopener">90.子集 II</a>:解集不能包含重复的子集（回溯算法+剪枝）</li><li><a href="https://leetcode-cn.com/problems/permutations/" target="_blank" rel="noopener">46.全排列</a>：回溯法，划分左右</li><li><a href="https://leetcode-cn.com/problems/permutations-ii/" target="_blank" rel="noopener">47.全排列II</a>：回溯法+剪枝，可以使用标记列表和回溯相结合(以空间换时间)</li></ul><h3 id="题目详细描述"><a href="#题目详细描述" class="headerlink" title="题目详细描述"></a>题目详细描述</h3><p><strong>子集</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsets</span><span class="params">(self, nums: List[int])</span> -&gt; List[List[int]]:</span>   </span><br><span class="line">    n = len(nums)</span><br><span class="line">    res = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dps</span><span class="params">(i,tmp)</span>:</span> <span class="comment">## 回溯法</span></span><br><span class="line">        res.append(tmp)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i, n):</span><br><span class="line">            dps(j + <span class="number">1</span>, tmp + [nums[j]])            </span><br><span class="line">    dps(<span class="number">0</span>, [])</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p><strong>子集II</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsetsWithDup</span><span class="params">(self, nums: List[int])</span> -&gt; List[List[int]]:</span></span><br><span class="line">    res = []</span><br><span class="line">    n = len(nums)</span><br><span class="line">    nums.sort() <span class="comment">## 先排序</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dps</span><span class="params">(i, tmp)</span>:</span> <span class="comment">## 回溯算法</span></span><br><span class="line">        res.append(tmp)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i,n):</span><br><span class="line">            <span class="keyword">if</span> j &gt; i <span class="keyword">and</span> nums[j] == nums[j<span class="number">-1</span>]: <span class="comment">## 判断剪枝</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            dps(j+<span class="number">1</span>, tmp + [nums[j]])      </span><br><span class="line">    dps(<span class="number">0</span>, [])</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p><strong>全排列</strong>：将题目给定的数组划分成左右两个部分，左边表示已经填过的数，右边表示代填的数，在回溯的时候动态维护这个数组</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">permute</span><span class="params">(self, nums: List[int])</span> -&gt; List[List[int]]:</span></span><br><span class="line">    res = []</span><br><span class="line">    n = len(nums)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">back</span><span class="params">(cur)</span>:</span> <span class="comment">## 假设填到第cur个位置</span></span><br><span class="line">        <span class="keyword">if</span> cur == n: <span class="comment">## 所有的数都填完了</span></span><br><span class="line">            res.append(nums[:]) <span class="comment">## 使用[:]来拷贝全部元素</span></span><br><span class="line">            <span class="keyword">return</span> </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(cur ,n): <span class="comment">##[0,cur-1]是已经填过的数，[cur,n-1]是代填的数</span></span><br><span class="line">            <span class="comment">## 尝试使用[cur,n-1]里面的数来填cur下标当前位置上的数</span></span><br><span class="line">            nums[cur],nums[i] = nums[i],nums[cur] <span class="comment">## 动态维护数组(i表示要代填的数的下标，cur表示要填的位置的下标，两个进行交换，则[0,cur]部分就成了已填过的数)</span></span><br><span class="line">            back(cur+<span class="number">1</span>)  <span class="comment">## 继续递归填下一个数 [cur+1,n-1]</span></span><br><span class="line">            nums[i],nums[cur] = nums[cur],nums[i] <span class="comment">## 撤销</span></span><br><span class="line">    back(<span class="number">0</span>) <span class="comment">## 从头开始填充</span></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p><strong>全排列II</strong>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">permuteUnique</span><span class="params">(self, nums: List[int])</span> -&gt; List[List[int]]:</span></span><br><span class="line">    n = len(nums)</span><br><span class="line">    nums.sort() <span class="comment">## 排序是剪枝的前提</span></span><br><span class="line">    res=[]</span><br><span class="line">    mark = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)] <span class="comment">## 标记</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">back</span><span class="params">(cur,t)</span>:</span> <span class="comment">## 回溯函数</span></span><br><span class="line">        <span class="keyword">if</span> cur == n:</span><br><span class="line">            res.append(t.copy())</span><br><span class="line">            <span class="keyword">return</span> </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="comment">## 剪枝 1.当前已经被标记，则不需要再使用 </span></span><br><span class="line">            <span class="comment">## 2.如果当前和前一个元素相同，且前一个元素未被标记</span></span><br><span class="line">            <span class="keyword">if</span> mark[i] == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment">## i&gt;0保证nums[i-1]是有意义的</span></span><br><span class="line">            <span class="comment">## mark[i-1] == 0 也可以写成 not mark[i-1] </span></span><br><span class="line">            <span class="comment">## mark[i-1]在深度优先遍历的过程中刚刚被撤销选择</span></span><br><span class="line">            <span class="keyword">if</span> i&gt;<span class="number">0</span> <span class="keyword">and</span> nums[i]==nums[i<span class="number">-1</span>] <span class="keyword">and</span> mark[i<span class="number">-1</span>] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment">## 回溯前</span></span><br><span class="line">            t.append(nums[i])</span><br><span class="line">            mark[i] = <span class="number">1</span></span><br><span class="line">            <span class="comment">## 递归</span></span><br><span class="line">            back(cur+<span class="number">1</span>,t)</span><br><span class="line">            <span class="comment">## 回溯后              </span></span><br><span class="line">            t.pop()</span><br><span class="line">            mark[i] = <span class="number">0</span></span><br><span class="line">    back(<span class="number">0</span>,[])</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> leetcode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习知识总结</title>
      <link href="/posts/k1000/"/>
      <url>/posts/k1000/</url>
      
        <content type="html"><![CDATA[<h2 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h2><p>什么是残差网络？残差网络解决了什么问题？残差网络是怎么解决这个问题的？</p><p>残差网络可以解决“随着网络加深，准确度不下降”也就是网络退化的问题。<br>增加一个恒等映射，将原始所需要学的函数H(x)转换成F(x)+x，这个简单的加法并不会给网络增加额外的参数和计算量，同时却可以大大增加模型的训练速度、提高训练效果，并且当模型的层数加深时，这个简单的结构能够很好的解决退化问题。</p><p>对比 DenseNet等，其区别何在（虽然都有 Shortcut，但是 DenceNet是串联，ResNet是相加）</p><h2 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h2><p>神经网络是根据损失函数计算的误差通过梯度反向传播的方式来指导深度网络权值的更新优化的。而误差传递过程是链式法则，当层数越深的时候，梯度是以指数的形式传播的。在根据损失函数计算的误差通过梯度反向传播的方式对深度网络权值进行更新时，得到的梯度值接近0或特别大，也就是梯度消失或爆炸。</p><p>什么是梯度爆炸和梯度消失：<br>在反向传播过程中需要对激活函数进行求导，如果导数大于1，那么随着网络层数的增加梯度更新将会朝着指数爆炸的方式增加这就是梯度爆炸。同样如果导数小于1，那么随着网络层数的增加梯度更新信息会朝着指数衰减的方式减少这就是梯度消失。</p><p>产生的原因：<br>梯度消失产生的原因：深层网络、不合适的损失函数<br>梯度爆炸产生的原因：一般出现在深层网络和权值初始化值太大的情况下。在深层神经网络或循环神经网络中，误差的梯度可在更新中累积相乘。如果网络层之间的梯度值大于 1.0，那么重复相乘会导致梯度呈指数级增长，梯度变的非常大，然后导致网络权重的大幅更新，并因此使网络变得不稳定。</p><p>解决方法：</p><ul><li><strong>预训练+微调</strong></li><li><strong>梯度剪切、权重正则化</strong>：梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。正则化是通过对网络权重做正则限制过拟合，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。</li><li><strong>relu、leakrelu、elu、maxout等激活函数</strong>：如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度。sigmoid函数的梯度随着x的增大或减小和消失，而ReLU不会。</li><li><strong>batch normalization</strong>：Batchnorm本质上是解决反向传播过程中的梯度问题。batchnorm全名是batch normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。反向传播式子中有w的存在，所以w的大小影响了梯度的消失和爆炸，Batch Normalization 就是通过对每一层的输出规范为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。</li></ul><h2 id="BN"><a href="#BN" class="headerlink" title="BN"></a>BN</h2><p>作用：<br>神经网络在训练的时候随着网络层数的加深,激活函数的输入值的整体分布逐渐往激活函数的取值区间上下限靠近,从而导致在反向传播时低层的神经网络的梯度消失。而BatchNormalization的作用是通过规范化的手段,将越来越偏的分布拉回到标准化的分布,使得激活函数的输入值落在激活函数对输入比较敏感的区域,从而使梯度变大,加快学习收敛速度,避免梯度消失的问题。<br>BN层的作用是把一个batch内的所有数据，从不规范的分布拉到正态分布。这样做的好处是使得数据能够分布在激活函数的敏感区域，敏感区域即为梯度较大的区域，因此在反向传播的时候能够较快反馈误差传播。</p><h2 id="RNN、LSTM、GRU-会推导"><a href="#RNN、LSTM、GRU-会推导" class="headerlink" title="RNN、LSTM、GRU(会推导)"></a>RNN、LSTM、GRU(会推导)</h2><h3 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h3><p>一个序列当前的输出与前面的输出也有关,在RNN网络结构中中,隐藏层的输入不仅包括输入层的输出还包含上一时刻隐藏层的输出,网络会对之前的信息进行记忆并应用于当前的输入计算中。</p><p>为什么好？<br>循环神经网络模型（RNN）是一种节点定向连接成环的人工神经网络，是一种反馈神经网络，RNN利用内部的记忆来处理任意时序的输入序列，并且在其处理单元之间既有内部的反馈连接又有前馈连接，这使得RNN可以更加容易处理不分段的文本等。</p><p>RNN容易梯度消失</p><ul><li>梯度裁剪：设定阈值，当梯度小于阈值时，更新的梯度为阈值。</li><li>使用LSTM、GRU</li></ul><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><ul><li>Lstm由输入门,遗忘门,输出门和一个cell组成。第一步是决定从cell状态中丢弃什么信息,然后在决定有多少新的信息进入到cell状态中,最终基于目前的cell状态决定输出什么样的信息。</li><li>Gru由重置门和更新门组成,其输入为前一时刻隐藏层的输出和当前的输入,输出为下一时刻隐藏层的信息。重置门用来计算候选隐藏层的输出,其作用是控制保留多少前一时刻的隐藏层。跟新门的作用是控制加入多少候选隐藏层的输出信息,从而得到当前隐藏层的输出。</li></ul><h3 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h3><h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><p>RNN在处理long term memory的时候存在缺陷，因此LSTM应运而生。LSTM是一种变种的RNN，它的精髓在于引入了细胞状态这样一个概念，不同于RNN只考虑最近的状态，LSTM的细胞状态会决定哪些状态应该被留下来，哪些状态应该被遗忘。</p><p>LSTM与GRU区别：1）GRU和LSTM的性能在很多任务上不分伯仲。2）GRU 参数更少因此更容易收敛，但是数据集很大的情况下，LSTM表达性能更好。3）从结构上来说，GRU只有两个门（update和reset），LSTM有三个门（forget，input，output），GRU直接将hidden state 传给下一个单元，而LSTM则用memory cell 把hidden state 包装起来。</p><p>内部结构是不同的</p><p>可以解决RNN的梯度消失的问题，怎么解决：<br>RNN由于网络较深,后面层的输出误差很难影响到前面层的计算,RNN的某一单元主要受它附近单元的影响。而LSTM因为可以通过阀门记忆一些长期的信息,相应的也就保留了更多的梯度。而GRU也可通过重置和更新两个阀门保留长期的记忆,也相对解决了梯度消失的问题。</p><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><p>卷积神经网络主要由卷积层、激活函数、池化层、全连接层组成。</p><ol><li>卷积层：使用卷积核进行特征提取和特征映射</li><li>激活函数：由于卷积也是一种新型运算，因此需要增加非线性映射</li><li>池化层：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征</li><li>全连接层：连接所有的特征，将输出值送给分类器</li></ol><p>优点：共享卷积核，处理高维数据无压力；可以自动进行特征提取<br>缺点：忽略局部和整体之间关联性；采用梯度下降法很容易使训练结果收敛于局部最小值而非全局最小值。</p><p>卷积的概念：对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的滤波器 filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。</p><p>感受野的概念：CNN每一层输出的特征图(feature map)上的像素点在原始图像上映射的区域大小.<br>权重共享：卷积的时候，卷积核上面的一组权重是恒定不变的，用一个卷积和去卷积一张图，这张图每个位置是被同样数值的卷积核操作的，权重是一样的，也就是参数共享</p><p>特性：局部连接、权值共享、池化操作、多层系统</p><p>pooling的作用：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。<br>主要有max-pooling和average-pooling，max-pooling更常用。通常max-pooling的效果更好，虽然两个都对数据做了下采样，但是max-pooling感觉更像是做了特征选择，选出了分类辨识度更好的特征，提供了非线性。</p><h2 id="训练过程中-若一个模型不收敛-那么是否说明这个模型无效-导致模型不收敛的原因有哪些"><a href="#训练过程中-若一个模型不收敛-那么是否说明这个模型无效-导致模型不收敛的原因有哪些" class="headerlink" title="训练过程中,若一个模型不收敛,那么是否说明这个模型无效?导致模型不收敛的原因有哪些?"></a>训练过程中,若一个模型不收敛,那么是否说明这个模型无效?导致模型不收敛的原因有哪些?</h2><p>并不能说明这个模型无效,导致模型不收敛的原因可能有数据分类的标注不准确,样本的信息量太大导致模型不足以fit整个样本空间。学习率设置的太大容易产生震荡,太小会导致不收敛。可能复杂的分类任务用了简单的模型。数据没有进行归一化的操作。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ol><li>sigmod</li><li>tanh</li><li>relu</li><li>sign(这是感知机中用到的函数)</li></ol><p>函数图像、特点、互相比较、优缺点以及改进方法<br>知道哪些激活函数，都有什么特点，如何使用</p><p>Relu比Sigmoid的效果好在哪里?<br>Sigmoid的导数只有在0的附近时有较好的激活性,而在正负饱和区域的梯度趋向于0,从而产生梯度弥散的现象,而relu在大于0的部分梯度为常数,所以不会有梯度弥散现象。Relu的导数计算的更快。Relu在负半区的导数为0,所以神经元激活值为负时,梯度为0,此神经元不参与训练,具有稀疏性。</p><p>作用：激活函数是用来加入非线性因素的,提高神经网络对模型的表达能力,解决线性模型所不能解决的问题。</p><p>神经网络中权重共享的是？卷积神经网络、循环神经网络</p><h3 id="relu"><a href="#relu" class="headerlink" title="relu"></a>relu</h3><p>在深度神经网络中，通常使用一种叫修正线性单元(Rectified linear unit，ReLU）作为神经元的激活函数。<br>ReLU函数其实是分段线性函数，把所有的负值都变为0，而正值不变，这种操作被成为单侧抑制。<br>有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。<br>通过ReLU实现稀疏后的模型能够更好地挖掘相关特征，拟合训练数据。<br>相比于其它激活函数来说，ReLU有以下优势：</p><ul><li>ReLU的表达能力更强，尤其体现在深度网络中</li><li>对于非线性函数而言，ReLU由于非负区间的梯度为常数，因此不存在梯度消失问题(Vanishing Gradient Problem)，使得模型的收敛速度维持在一个稳定状态</li></ul><h2 id="attention机制"><a href="#attention机制" class="headerlink" title="attention机制"></a>attention机制</h2><p>为了解决LSTM长输入序列在编码成固定长度向量后，解码受限于该固定长度向量的问题。</p><p>概念：核心目标是从众多信息中选择出对当前任务目标更关键的信息。<br>本质上：将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息。<br>物理意义：从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，这个“聚焦”就体现在权重系数上<br>优点：参数少、速度快、效果好</p><p>作用：<br>减少处理高维输入数据的计算负担,结构化的选取输入的子集,从而降低数据的维度。让系统更加容易的找到输入的数据中与当前输出信息相关的有用信息,从而提高输出的质量。帮助类似于decoder这样的模型框架更好的学到多种内容模态之间的相互关系。</p><p>带权求和</p><p>公式<br>Soft attention(global attention)<br>Hard attention<br>local attention</p><p>动态attention、静态attention、<br>self attention</p><p>静态attention“半软半硬”的attention （local attention）</p><p>强制前向attention</p><p>attention的概念，attention的本质是什么(本质是求相似度)</p><h2 id="1-1的卷积作用"><a href="#1-1的卷积作用" class="headerlink" title="1*1的卷积作用"></a>1*1的卷积作用</h2><p>实现跨通道的交互和信息整合,实现卷积核通道数的降维和升维,可以实现多个feature map的线性组合,而且可是实现与全连接层的等价效果。</p><h2 id="提升网络的泛化能力"><a href="#提升网络的泛化能力" class="headerlink" title="提升网络的泛化能力"></a>提升网络的泛化能力</h2><ul><li>从数据上提升性能:收集更多的数据,对数据做缩放和变换,特征组合和重新定义问题。</li><li>从算法调优上提升性能:用可靠的模型诊断工具对模型进行诊断,权重的初始化,用小的随机数初始化权重。对学习率进行调节,尝试选择合适的激活函数,调整网络的拓扑结构,调节batch和epoch的大小,添加正则化的方法,尝试使用其它的优化方法,使用early stopping。</li></ul><h2 id="卷积层和池化层"><a href="#卷积层和池化层" class="headerlink" title="卷积层和池化层"></a>卷积层和池化层</h2><table><thead><tr><th>\</th><th>卷积层</th><th>池化层</th></tr></thead><tbody><tr><td>功能</td><td>提取特征</td><td>压缩特征图，提取主要特征</td></tr><tr><td>操作</td><td>可惜是二维的，对于三维数据比如RGB图像（3通道），卷积核的深度必须同输入的通道数，输出的通道数等于卷积核的个数。 卷积操作会改变输入特征图的通道数。</td><td>池化只是在二维数据上操作的，因此不改变输入的通道数。对于多通道的输入，这一点和卷积区别很大。</td></tr><tr><td>特性</td><td>权值共享：减少了参数的数量，并利用了图像目标的位置无关性。稀疏连接：输出的每个值只依赖于输入的部分值。</td><td></td></tr></tbody></table><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><p>1）SGD；2）Momentum；3）Nesterov；4）Adagrad；5）Adadelta；6）RMSprop；7）Adam；8）Adamax；9）Nadam。（1）对于稀疏数据，尽量使用学习率可自适应的算法，不用手动调节，而且最好采用默认参数。（2）SGD通常训练时间最长，但是在好的初始化和学习率调度方案下，结果往往更可靠。但SGD容易困在鞍点，这个缺点也不能忽略。（3）如果在意收敛的速度，并且需要训练比较深比较复杂的网络时，推荐使用学习率自适应的优化方法。（4）Adagrad，Adadelta和RMSprop是比较相近的算法，表现都差不多。（5）在能使用带动量的RMSprop或者Adam的地方，使用Nadam往往能取得更好的效果。</p>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习知识总结4--决策树</title>
      <link href="/posts/k0004/"/>
      <url>/posts/k0004/</url>
      
        <content type="html"><![CDATA[<p>决策树最常用的三个算法是：</p><ol><li>ID3</li><li>C4.5</li><li>CART</li></ol><p>这三种决策树算法的区别在于ID3是选择信息增益大的属性来对样本进行划分，由于存在缺点(多属性的取值会使得模型的泛化能力变差，决策树容易产生过拟合)，所以C4.5进行改进，选用了信息增益比来对样本进行划分，但是问题还是存在。所以引入CART树，使用基尼系数作为节点的分类依据。</p><ul><li>ID3 选择信息增益大的属性来对样本进行划分(多属性的取值会使得模型的泛化能力变差，决策树容易产生过拟合)</li><li>C4.5 选择信息增益比来对样本进行划分()</li><li>CART树 选择基尼系数作为节点的分类依据(有剪枝)</li></ul><p>信息增益g(D,A)：表示得知特征X的信息而使得类Y的信息的不确定性减少的程度<br>$g(D,A) = H(D) - H(D|A)$</p><ul><li>H(D):集合D的</li></ul><p>信息增益比<br>基尼系数</p><p>决策树的特征选择、生成、剪枝</p><p>ID3和C4.5的区别；RF和GBDT的区别；GBDT是否适合于处理大规模的ID特征</p>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习知识总结1--感知机</title>
      <link href="/posts/k0001/"/>
      <url>/posts/k0001/</url>
      
        <content type="html"><![CDATA[<h2 id="感知机-1957年提出"><a href="#感知机-1957年提出" class="headerlink" title="感知机(1957年提出)"></a>感知机(1957年提出)</h2><p>统计学习方法的三要素：模型+策略+算法<br>线性分类可以分为硬输出和软输出，而感知机和线性判别模型属于硬输出。</p><p>感知机的模型：线性模型<br>策略：基于误分类的损失函数进行极小化<br>算法：梯度下降法</p><p>感知机是二类分类的 <strong>线性分类模型</strong>，属于 <strong>判别模型</strong>，输入是实例的特征向量，输出为实例的类别。</p><p>感知机学习旨在求出将训练数据进行线性划分的分类超平面，是基于 <strong>误分类的损失函数</strong>，利用 <strong>梯度下降法</strong>对损失函数进行 <strong>极小化</strong>求得。</p><p>原始形式和对偶形式</p><p>概念1：数据集的线性可分性:如果存在一个超平面可以将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，则数据集为线性可分数据集，否则数据集线性不可分。</p><p>模型： $f(x) = sign(w^T x)$<br>$$sign(x)<br>\begin{cases}<br> +1, &amp;x&gt;=0\<br> -1, &amp;x&lt;0<br>\end{cases}<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习知识总结</title>
      <link href="/posts/k0000/"/>
      <url>/posts/k0000/</url>
      
        <content type="html"><![CDATA[<h2 id="机器学习算法中的模型"><a href="#机器学习算法中的模型" class="headerlink" title="机器学习算法中的模型"></a>机器学习算法中的模型</h2><h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>感知机是一个二分类线性分类模型,目的是为了找到可以将实例划分为两类的分离超平面，通过基于误分类的损失函数，利用梯度下降法对损失函数进行最优化。是神经网络和支持向量机的基础。<br>感<br>知机是一个二分类线性分类模型，它的目的是求得一个能将训练集正实例点和负实例点完全分开的分离超平面。通过极小化基于误分类的损失函数，运用梯度下降来求解参数。 主要关注的是误分类点距离超平面的距离，误分类点越少，误分类点距离超平面越近，损失函数值就越小。</p><h3 id="K-近邻："><a href="#K-近邻：" class="headerlink" title="K-近邻："></a>K-近邻：</h3><p>KNN，一种分类和回归方法<br>根据最接近预测的那个点的k个点的最大特征结果来表示预测的点的类别。可以分类也可以回归。</p><p>k值的选取：一般取一个比较小的值，采用交叉验证法来选取最优的k值。（k太小，模型复杂，容易过拟合；k太大，近似误差会增大，输入实例较远的训练实例也会对预测起作用，使预测发生错误）<br>距离度量：一般使用欧式距离(欧式距离可适用于不同空间，表示不同空间点之间的距离)<br>分类决策规则：一般为多数表决<br>回归决策规则：选择平均法，k个样本输出的平均值作为预测输出</p><h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><p>朴素贝叶斯是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的待分类项x，通过学习到的模型计算后验概率分布。即：在此项出现的条件下各个目标类别出现的概率，将后验概率最大的类作为x所属的类别。</p><p>引入条件独立假设、<br>假定所有的特征在数据集中的作用是独立同分布的，但这个假设在现实生活中很不真实，因此很“朴素”。</p><h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>决策树是一种树结构，一种分类和回归方法。</p><p>决策树目的是为了让模型的不确定性降低的越快越好，基于评价指标的不同，主要分为ID3、C4.5和CART算法。 ID3选择信息增益大的属性来对样本进行划分，但是容易产生过拟合；所以C4.5选择信息增益比来对样本进行划分；不过这两种算法都没有完全解决过拟合的问题，因为对决策树的生长没有进行合理的限制，所以引入CART树，使用基尼系数作为节点的分类依据。CART树中有剪枝操作。</p><p>剪枝是防止决策树过拟合的方法<br>剪枝分为预剪枝和后剪枝两种，预剪枝是在构建决策树时抑制它的生长，后剪枝是决策树生长完全后再对叶子节点进行修剪。</p><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</p><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><p>SVM 支持向量机是一种二类分类模型，基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器。<br>主要分为线性支持向量机、非线性支持向量机<br>硬间隔、软间隔</p><p>当样本量线性可分时，使用硬间隔最大化学习一个线性分类器，即线性可分支持向量机<br>当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机<br>当训练数据线性不可分时，使用核技巧即软间隔最大化，学习非线性支持向量机。</p><p>比较关键的知识点：几何间隔、原始问题和对偶问题、硬间隔和软间隔、核技巧和核函数</p><h3 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h3><p>最大熵模型是对数线性分类模型。<br>学习概率模型的时候，在所有可能的概率模型(分布)中，熵最大的模型就是最好的模型。</p><p>逻辑回归和最大熵模型没有本质区别；逻辑回归是最大熵对应为二分类时的特殊情况，也就是说，当逻辑回归扩展为多类别的时候，就是最大熵模型。</p><h3 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h3><p>主要会的聚类算法是k-means和DBSCAN。</p><p>k-means：从数据集中随机选择k个聚类样本作为初始的聚类中心,然后计算数据集中每个样本到这k个聚类中心的距离,并将此样本分到距离最小的聚类中心所对应的类中。将所有样本归类后,对于每个类别重新计算每个类别的聚类中心即每个类中所有样本的质心,重复以上操作直到聚类中心不变为止。</p><p>dbscan:一种基于密度的空间聚类算法.将具有足够高密度的区域划分为簇,并在有噪声的数据中发现任意形状的簇</p><p>LR和softmax区别</p><h2 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h2><p>主要有的聚类算法是k-means和DBSCAN。</p><h3 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h3><p>DBSCAN：基于密度的聚类算法：DBSCAN是一种基于密度的空间聚类算法,它不需要定义簇的个数,而是将具有足够高密度的区域划分为簇,并在有噪声的数据中发现任意形状的簇,在此算法中将簇定义为密度相连的点的最大集合。<br>优点：<br>速度快，计算简便<br>缺点：<br>我们必须提前知道数据有多少类/组。<br>具体步骤：</p><p>首先确定半径r和minPoints. 从一个没有被访问过的任意数据点开始，以这个点为中心，r为半径的圆内包含的点的数量是否大于或等于minPoints，如果大于或等于minPoints则改点被标记为central point,反之则会被标记为noise point。<br>重复1的步骤，如果一个noise point存在于某个central point为半径的圆内，则这个点被标记为边缘点，反之仍为noise point。重复步骤1，知道所有的点都被访问过。</p><p>优点：不需要知道簇的数量<br>缺点：需要确定距离r和minPoints</p><h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h3><p>基于划分的聚类算法：给定一个有N个元祖或者记录的数据集，分裂法将构造k个分组，每一个分组就代表一个聚类。<br>从数据集中随机选择k个聚类样本作为初始的聚类中心,然后计算数据集中每个样本到这k个聚类中心的距离,并将此样本分到距离最小的聚类中心所对应的类中。将所有样本归类后,对于每个类别重新计算每个类别的聚类中心即每个类中所有样本的质心,重复以上操作直到聚类中心不变为止。</p><p>k-means存在 <strong>缺点</strong>：</p><p>1）k-means是局部最优的，容易受到初始质心的影响<br>2）同时，k值的选取也会直接影响聚类结果，最优聚类的k值应与样本数据本身的结构信息相吻合，而这种结构信息是很难去掌握，因此选取最优k值是非常困难的。</p><p><strong>算法步骤</strong>：<br>(1) 首先我们选择一些类/组，并随机初始化它们各自的中心点。中心点是与每个数据点向量长度相同的位置。这需要我们提前预知类的数量(即中心点的数量)。<br>(2) 计算每个数据点到中心点的距离，数据点距离哪个中心点最近就划分到哪一类中。<br>(3) 计算每一类中中心点作为新的中心点。<br>(4) 重复以上步骤，直到每一类中心在每次迭代后变化不大为止。也可以多次随机初始化中心点，然后选择运行结果最好的一个。</p><p><strong>k值的选择</strong>：</p><ol><li>拍脑袋法：将样本量除以2再开方出来的值作为K值</li><li>手肘法 使用指标SSE（误差平方和）：当k小于真实聚类数的时候，随着k的增大，SSE会大幅下降；而当k到达真实聚类数的时候，再增加k,SSE的下降幅度骤减，并且越增大，越趋于平缓。关系图是一个手肘的形状。</li><li>间隔统计量（Gap Statistic）</li><li>轮廓系数法：用Xi到某个簇所有样本平均距离作为衡量该点到该簇的距离后，选择离Xi最近的一个簇作为最近簇。求出所有样本的轮廓系数后再求平均值就得到了平均轮廓系数。平均轮廓系数的取值范围为[-1,1]，且簇内样本的距离越近，簇间样本距离越远，平均轮廓系数越大，聚类效果越好。那么，很自然地，平均轮廓系数最大的k便是最佳聚类数。</li><li>Calinski-Harabasz准则</li></ol><h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><ul><li>K均值和DBSCAN都是将每个对象指派到单个簇的划分聚类算法，但是K均值一般聚类所有对象，而DBSCAN丢弃被它识别为噪声的对象。</li><li>K均值使用簇的基于原型的概念，而DBSCAN使用基于密度的概念。</li><li>K均值很难处理非球形的簇和不同大小的簇。DBSCAN可以处理不同大小或形状的簇，并且不太受噪声和离群点的影响。当簇具有很不相同的密度时，两种算法的性能都很差。</li><li>K均值只能用于具有明确定义的质心（比如均值或中位数）的数据。DBSCAN要求密度定义（基于传统的欧几里得密度概念）对于数据是有意义的。</li><li>K均值可以用于稀疏的高维数据，如文档数据。DBSCAN通常在这类数据上的性能很差，因为对于高维数据，传统的欧几里得密度定义不能很好处理它们。</li><li>基本K均值算法等价于一种统计聚类方法（混合模型），假定所有的簇都来自球形高斯分布，具有不同的均值，但具有相同的协方差矩阵。DBSCAN不对数据的分布做任何假定。</li><li>K均值和DBSCAN的最初版本都是针对欧几里得数据设计的，但是它们都被扩展，以便处理其他类型的数据。</li><li>K均值DBSCAN和都寻找使用所有属性的簇，即它们都不寻找可能只涉及某个属性子集的簇。</li><li>K均值可以发现不是明显分离的簇，即便簇有重叠也可以发现，但是DBSCAN会合并有重叠的簇。</li><li>K均值算法的时间复杂度是O(m)，而DBSCAN的时间复杂度是O(m^2)，除非用于诸如低维欧几里得数据这样的特殊情况。</li><li>DBSCAN多次运行产生相同的结果，而K均值通常使用随机初始化质心，不会产生相同的结果。</li><li>K均值聚类可以看作优化问题，即最小化每个点到最近质心的误差平方和，并且可以看作一种统计聚类（混合模型）的特例。DBSCAN不基于任何形式化模型。</li></ul><h2 id="LR-逻辑回归-手动推导"><a href="#LR-逻辑回归-手动推导" class="headerlink" title="LR 逻辑回归(手动推导)"></a>LR 逻辑回归(手动推导)</h2><p>公式推导 Sigmod函数 似然估计法 求对数  求最大值  梯度上升的方法 损失函数</p><p>如何进行多分类？ 如果类别之间有明显的互斥就选用softmax，若不互斥有交叉则使用2</p><ol><li>修改损失函数，使用softmax函数构造模型解决多分类问题</li><li>构建相应类别个数的二分类逻辑回归分类器</li></ol><p><strong>LR和SVM的区别</strong>：</p><ul><li>LR是参数模型,SVM为非参数模型</li><li>使用的损失函数不同。LR采用的损失函数为logisticalloss,而SVM采用的是hingeloss(合页损失函数)。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。</li><li>在学习分类器的时候,SVM只考虑与分类最相关的少数支持向量点。而LR通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。</li><li>LR的模型相对简单,在进行大规模线性分类时比较方便。SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。</li><li>LR 能做的 svm能做，但可能在准确率上有问题，svm能做的LR有的做不了。</li></ul><p><strong>LR和线性回归的区别</strong>：</p><ul><li>LR分类；线性回归 预测</li><li>LR 预测函数；线性回归 拟合函数</li><li>LR 用最大似然估计来计算参数；线性回归用最小二乘法来计算参数</li><li>LR 对异常值有较好的稳定性； 线性回归 更容易受到异常值的影响</li></ul><h2 id="SVM-手动推导"><a href="#SVM-手动推导" class="headerlink" title="SVM(手动推导)"></a>SVM(手动推导)</h2><p>什么是支持向量机：二分类模型，模型为特征空间上的间隔最大的线性分类器，策略是最大化分类间隔，它只关注支持向量到超平面的距离。<br>SVM的目标是寻找一个最优化超平面可以在空间中分割两类数据。距离超平面最近的那些点，也就是支持向量，需要最大化支持向量距离超平面的距离。</p><p>作用：可以解决二分类或者多分类问题<br>物理意义：构造一个最优化的超平面在空间中分割数据</p><p>原问题、对偶问题、拉格朗日乘子法、</p><p>硬间隔、软间隔： 区别在于是否引入松弛变量  公式</p><p>SVM的对偶问题推导</p><p>使用对偶计算的目的： 方便核函数的引入、原问题的求解复杂度与特征的维数有关，而转成对偶问题以后只与问题的变量个数有关，这个变量个数也就是支持向量的个数，相较于特征数来说较少。并且对偶问题是一个凸优化问题，通过拉格朗日算子使带约束的优化目标转为不带约束的优化函数。</p><p>损失函数</p><p>SVM只和分类界限上的支持向量点有关,换而言之 <strong>只和局部数据有关</strong>。</p><h3 id="核函数和核方法"><a href="#核函数和核方法" class="headerlink" title="核函数和核方法"></a>核函数和核方法</h3><p>核函数：线性核、多项式核、高斯核<br>核函数的作用：核函数隐含着一个从低维空间到高维空间的映射,这个映射可以把低维空间中线性不可分的两类点变成线性可分的。<br>本质： 因为实际应用中会遇到线性不可分的情况，所以通常的做法就是将样例特征映射到高维空间中，转换成线性可分问题，但是会遇到维度过高的问题，所以引入核函数，将特征从低位转换到高维，但是避免了直接进行高维空间的复杂计算，可以在低维上进行计算，实质上分类效果表现在高维上。</p><p>线性核和高斯核的选用：当数据和问题是线性可分的，并且数据的特征提取较好，所包含的信息量足够大，则选用线性核；若特征数较少，样本量适中，对于时间不敏感，且问题线性不可分的时候，则使用高斯核的效果更好。</p><p>应用场景：<br>线性核： 特征维数高、样本数量非常多(避免造成庞大的计算量)<br>多项式核<br>高斯核： 样本数量可观、特征少(非线性核)</p><p>核函数的选择：当样本的特征很多且维数很高时可考虑用SVM的线性核函数。当样本的数量较多,特征较少时,一般手动进行特征的组合再使用SVM的线性核函数。当样本维度不高且数量较少时,且不知道该用什么核函数时一般优先使用高斯核函数,因为高斯核函数为一种局部性较强的核函数,无论对于大样本还是小样本均有较好的性能且相对于多项式核函数有较少的参数。</p><p>为什么高斯核能够拟合无穷维度：因为将泰勒展开式代入高斯核,将会得到一个无穷维度的映射。</p><h2 id="朴素贝叶斯-1"><a href="#朴素贝叶斯-1" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><p>要求：贝叶斯定理、特征条件独立假设、<br>朴素贝叶斯属于生成式模型，学习输入和输出的连和分布概率，给定输入x，利用贝叶斯概率定理求出最大的后验概率作为输出y</p><h2 id="机器学习中的距离计算方法"><a href="#机器学习中的距离计算方法" class="headerlink" title="机器学习中的距离计算方法"></a>机器学习中的距离计算方法</h2><ol><li>欧式距离：两点之间相减平方和</li><li>曼哈顿距离：绝对值之和</li><li>余弦距离</li><li>切比雪夫距离</li></ol><h2 id="分类算法"><a href="#分类算法" class="headerlink" title="分类算法"></a>分类算法</h2><p>单一的分类方法： LR、SVM、决策树、朴素贝叶斯、人工神经网络、KNN、<br>集成学习方法：基于Bagging和Boosting算法、随机森林、GBDT、Adaboost、XGboost</p><p>一个问题：有一个数据集，如何进行分类</p><p>（分类模型的优缺点，以及分类模型的构造步骤）<br>根据数据类型选择不同的模型，如Lr或者SVM，决策树。<br>假如特征维数较多，可以选择SVM模型，<br>如果样本数量较大可以选择LR模型，但是LR模型需要进行数据预处理；<br>假如缺失值较多可以选择决策树。<br>选定完模型后，相应的目标函数就确定了。<br>还可以在考虑正负样例比比，通过上下集采样平衡正负样例比。</p><h2 id="数据存在问题怎么处理"><a href="#数据存在问题怎么处理" class="headerlink" title="数据存在问题怎么处理"></a>数据存在问题怎么处理</h2><p>上下采样平衡正负样例比(类不平衡)<br>考虑缺失值，需要进行缺失值的填充(比如使用KNN)<br>数据需要进行归一化</p><h2 id="训练集中类别不均衡，哪个参数最不准确"><a href="#训练集中类别不均衡，哪个参数最不准确" class="headerlink" title="训练集中类别不均衡，哪个参数最不准确?"></a>训练集中类别不均衡，哪个参数最不准确?</h2><p>准确度。（可以举个二分类的极度不平衡的例子来说明）</p><h2 id="分层采样"><a href="#分层采样" class="headerlink" title="分层采样"></a>分层采样</h2><p>适用范围：考虑保持样本结构和总体结构的一致性，当总体有明显差异的几部分组成的时候，适用于分层抽样。</p><h2 id="生成式模型和判别式模型"><a href="#生成式模型和判别式模型" class="headerlink" title="生成式模型和判别式模型"></a>生成式模型和判别式模型</h2><p>生成式模型：朴素贝叶斯、KNN、贝叶斯网络、HMM、马尔科夫随机场<br>判别式模型：线性回归、逻辑回归、SVM、CART、神经网络、CRF、Boosting</p><p>区别：</p><ul><li>判别式模型是针对 <strong>条件分布</strong>建模，而生成式模型则针对 <strong>联合分布</strong>进行建模。</li><li>不管是生成式模型还是判别式模型，它们最终的判断依据都是 <strong>条件概率</strong>，但是生成式模型先计算了 <strong>联合概率</strong>，再由贝叶斯公式计算得到 <strong>条件概率</strong>。因此，生成式模型可以体现更多数据本身的分布信息，其普适性更广。</li></ul><h2 id="决策树-1"><a href="#决策树-1" class="headerlink" title="决策树"></a>决策树</h2><p>决策树最常用的三个算法是：</p><ol><li>ID3</li><li>C4.5</li><li>CART</li></ol><p>区别：</p><ul><li>ID3是选择 <strong>信息增益</strong>大的属性来对样本进行划分，由于存在缺点(多属性的取值会使得模型的泛化能力变差，决策树容易产生过拟合)</li><li>C4.5进行改进，选用了 <strong>信息增益比</strong>来对样本进行划分</li><li>但是问题还是存在。所以引入CART树，使用 <strong>基尼系数</strong>作为节点的分类依据。</li></ul><h2 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h2><p>联合熵：两个随机变量X，Y的联合分布，可以形成联合熵Joint Entropy，用H(X,Y)表示。<br>条件熵：在随机变量X发生的前提下，随机变量Y发生所新带来的熵定义为Y的条件熵，用H(Y|X)表示，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。<br>H(Y|X)=H(X,Y)-H(X)，整个式子表示(X,Y)发生所包含的熵减去X单独发生包含的熵。<br>相对熵：又称互熵，交叉熵，鉴别信息，Kullback熵，Kullback-Leible散度等。在一定程度上，相对熵可以度量两个随机变量的“距离”，且有D(p||q) ≠D(q||p)<br>互信息：两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵，用I(X,Y)表示。I(X,Y)=D(P(X,Y)||P(X)P(Y))</p><p>H(Y)-I(X,Y)=H(Y|X)<br>H(Y|X)=H(X,Y)-H(X)【条件熵的定义】<br>H(Y|X)=H(Y)-I(X,Y)【互信息】<br>则<br>I(X,Y)= H(X)+H(Y)-H(X,Y)</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>L1正则化和L2正则化的区别：</p><ul><li>L1是模型各个参数的绝对值之和,L2为各个参数平方和的开方值。</li><li>L1更趋向于产生少量的特征,其它特征为0,最优的参数值很大概率出现在坐标轴上,从而导致产生稀疏的权重矩阵,而L2会选择更多的矩阵,但是这些矩阵趋向于0。</li></ul><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ul><li>平方损失（预测问题）</li><li>交叉熵（分类问题）</li><li>hinge损失（SVM支持向量机）</li><li>CART回归树的残差损失</li></ul><p>0-1损失、平方损失、绝对损失以及对数损失</p><p>知道哪些损失函数？为什么分类问题不能用均方差？（这也是经典的题目，求导后发现预测值越接近label时返回的梯度越小，越难训练）</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>表达式、损失函数、反向求导推导<br>线性回归y=wx+b，w和x可能是多维。线性回归的损失函数为平方损失函数。</p><h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><p>LDA是一种基于有监督学习的降维方式,将数据集在低维度的空间进行投影,要使得投影后的同类别的数据点间的距离尽可能的靠近,而不同类别间的数据点的距离尽可能的远。</p><p>A/B test如何进行流量分流</p><h2 id="集成算法"><a href="#集成算法" class="headerlink" title="集成算法"></a>集成算法</h2><p><strong>XGBOOST和GDBT的区别</strong></p><ul><li>GDBT在函数空间中利用梯度下降法进行优化而XGB在函数空间中使用了牛顿法进行优化。即GDBT在优化中使用了一阶导数信息,而XGB对损失函数进行了二阶泰勒展开,用到了一阶和二阶倒数信息。</li><li>XGB在损失函数中加入了正则项(树叶子节点个数,每个叶子节点上输出score的L2模平方和。</li><li>对于缺失的样本,XGB可以自动学习出它的分裂方向。GDBT的节点分裂方式使用的是gini系数,XGB通过优化推导出分裂前后的增益来选择分裂节点。</li><li>XGB在处理每个特征列时可以做到并行。</li></ul><p><strong>AdaBoost和GBDT的区别</strong></p><ul><li>AdaBoost通过调整错分的数据点的权重来改进模型,而GBDT是从负梯度的方向去拟合改进模型。</li><li>AdaBoost改变了训练数据的权值,即样本的概率分布,减少上一轮被正确分类的样本权值,提高被错误分类的样本权值,而随机森林在训练每棵树的时候,随机挑选部分训练集进行训练。在对新数据进行预测时,AdaBoost中所有树加权投票进行预测,每棵树的权重和错误率有关,而随机森林对所有树的结果按照少数服从多数的原则进行预测。</li></ul><h2 id="如何防止过拟合"><a href="#如何防止过拟合" class="headerlink" title="如何防止过拟合"></a>如何防止过拟合</h2><p>模型权重减小，模型简单</p><ul><li>增加数据集，数据清洗，防止噪声干扰</li><li>早停法</li><li>L1和L2正则化</li><li>神经网络的dropout【在神经网络的训练过程中,对于神经单元按一定的概率将其随机从网络中丢弃,从而达到对于每个mini-batch都是在训练不同网络的效果,防止过拟合。】</li><li>BN</li><li>决策树剪枝</li><li>SVM的松弛变量</li><li>集成学习</li><li>网络bagging</li></ul><h2 id="时间序列的数据集如何进行交叉验证"><a href="#时间序列的数据集如何进行交叉验证" class="headerlink" title="时间序列的数据集如何进行交叉验证"></a>时间序列的数据集如何进行交叉验证</h2><ul><li>预测后一半</li><li>日向前链技术</li></ul><p>嵌套交叉验证，有两种方式，分别是预测后一半、日前向链。<br>预测后一半嵌套交叉验证方法的一个缺陷是 hold-out 测试集的任意选择会导致在独立测试集上预测误差的有偏估计。（有系统误差）<br>日前向链技术：一种基于前向链（Forward-Chaining）的方法。<br>将最后几天的数据当做测试集，并将以前的所有数据分配到训练集中，然后训练计算误差，最后将误差求平均。</p><h2 id="正负样本不平衡的解决方法，评价指标的参考意义"><a href="#正负样本不平衡的解决方法，评价指标的参考意义" class="headerlink" title="正负样本不平衡的解决方法，评价指标的参考意义"></a>正负样本不平衡的解决方法，评价指标的参考意义</h2><p>上下采样法<br>上采样就是以数据量多的一方的样本数量为标准，把样本数量较少的类的样本数量生成和样本数量多的一方相同，称为上采样。下采样与之相反。</p><p>好的指标：ROC和AUC、F值、G-Mean；不好的指标：Precision、Recall ？</p><p>数据不平衡的解决方法：</p><ul><li>使用正确的评估标准,当数据不平衡时可以采用精度,调用度,F1得分,MCC,AUC等评估指标。</li><li>重新采样数据集,如欠采样和过采样。欠采样通过减少冗余类的大小来平衡数据集。当数据量不足时采用过采样,尝试通过增加稀有样本的数量来平衡数据集,通过使用重复,自举,SMOTE等方法生成新的样本。</li><li>以正确的方式使用K-fold交叉验证,组合不同的重采样数据集,对多数类进行聚类。</li></ul><h2 id="softmax-公式"><a href="#softmax-公式" class="headerlink" title="softmax 公式"></a>softmax 公式</h2><h2 id="SGD-Momentum-Adagard-Adam原理"><a href="#SGD-Momentum-Adagard-Adam原理" class="headerlink" title="SGD,Momentum,Adagard,Adam原理"></a>SGD,Momentum,Adagard,Adam原理</h2><ol><li>SGD为随机梯度下降,每一次迭代计算数据集的mini-batch的梯度,然后对参数进行更新。</li><li>Momentum参考了物理中动量的概念,前几次的梯度也会参与到当前的计算中,但是前几轮的梯度叠加在当前计算中会有一定的衰减。</li><li>Adagard在训练的过程中可以自动变更学习的速率,设置一个全局的学习率,而实际的学习率与以往的参数模和的开方成反比。</li><li>Adam利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率,在经过偏置的校正后,每一次迭代后的学习率都有个确定的范围,使得参数较为平稳。</li></ol><h2 id="L1不可导的时候该怎么办"><a href="#L1不可导的时候该怎么办" class="headerlink" title="L1不可导的时候该怎么办"></a>L1不可导的时候该怎么办</h2><p>当损失函数不可导,梯度下降不再有效,可以使用 <strong>坐标轴下降法</strong>,梯度下降是沿着当前点的负梯度方向进行参数更新,而坐标轴下降法是 <strong>沿着坐标轴的方向</strong>,假设有m个特征个数,坐标轴下降法进参数更新的时候,先固定m-1个值,然后再求另外一个的局部最优解,从而避免损失函数不可导问题。使用Proximal Algorithm对L1进行求解,此方法是去优化损失函数上界结果。</p><h2 id="最大似然估计和最大后验概率的区别"><a href="#最大似然估计和最大后验概率的区别" class="headerlink" title="最大似然估计和最大后验概率的区别"></a>最大似然估计和最大后验概率的区别</h2><ol><li>最大似然估计提供了一种给定观察数据来评估模型参数的方法,而最大似然估计中的采样满足所有采样都是独立同分布的假设。</li><li>最大后验概率是根据经验数据获难以观察量的点估计,与最大似然估计最大的不同是最大后验概率融入了要估计量的先验分布在其中,所以最大后验概率可以看做规则化的最大似然估计。</li></ol><h2 id="评价指标"><a href="#评价指标" class="headerlink" title="评价指标"></a>评价指标</h2><p>项目如何验证有效性，在业务中怎么用</p><p>F1-score :精准率和召回率的调和平均数</p><p>统计学中用来衡量二分类模型精确度的一种指标，兼顾了分类模型的精确率和召回率，<br>可以看做是模型精确率和召回率的一种调和平均。</p><p>F1 = 2 * precision* recall/ (precision + recall)</p><p>TP:实际为正，预测为正的样本数量<br>FP:实际为负，预测为正的样本数量<br>FN:实际为负，预测为负的样本数量<br>TN:实际为正，预测为负的样本数量</p><p>准确率： TP+FN/ TP+FP+FN+TN<br>精确度： 针对预测样本，计算我们预测出来的某类样本中，有多少是被正确预测的。<br>TP/TP+FP 预测为正的样本中实际上也是正的样本占被预测为正的样本的比例<br>召回率： 针对原先实际样本而言，有多少样本被正确的预测出来了。<br>TP/TP+FN  实际为正的样本中，预测也为正的样本占实际为正的样本的比例</p><p>Micro-F1和Macro-F1</p><p>微平均：第一种计算出所有类别总的Precision和Recall，然后计算F1。<br>宏平均：第二种方式是计算出每一个类的Precison和Recall后计算F1，最后将F1平均。</p><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基础算法题总结1--位运算相关</title>
      <link href="/posts/t0001/"/>
      <url>/posts/t0001/</url>
      
        <content type="html"><![CDATA[<h2 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h2><p><strong>异或运算</strong>:相同为0，不同为1<br>0^0 = 0<br>1^1 = 0<br>1^0 = 1<br>0^1 = 1</p><p>涉及到python中 <strong>内置函数</strong>：</p><figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line">异或：x^y</span><br><span class="line">转换成二进制：bin()</span><br><span class="line">计算字符串中某个字符的个数：<span class="keyword">str</span>.<span class="keyword">count</span>(<span class="string">'1'</span>)</span><br></pre></td></tr></table></figure><p>自己实现内置函数的功能</p><ol><li><p>计算二进制中1的个数(时间复杂度为O(k),k大多数为32)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> t:</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line">    t = t &amp; (t<span class="number">-1</span>)</span><br><span class="line">print(count)</span><br></pre></td></tr></table></figure></li><li><p>下降到</p></li></ol><p>LeetCode中相关的题目：</p><ul><li><a href="https://leetcode-cn.com/problems/hamming-distance/" target="_blank" rel="noopener">461.汉明距离</a> ：异或、统计1的个数</li><li><a href="https://leetcode-cn.com/problems/single-number/" target="_blank" rel="noopener">136.只出现一次的数字</a>：异或、相同的可以抵消，唯一不同的就可以留下</li><li><a href="https://leetcode-cn.com/problems/counting-bits/" target="_blank" rel="noopener">338.比特位计数</a>：原来方法（O(k*N)）动态规划+(最高有效位、最低有效位、最低设置位)（O(N)）</li><li><a href="https://leetcode-cn.com/problems/majority-element/" target="_blank" rel="noopener">169.多数元素</a>:可以使用，但是最优方法最好使用摩尔投票法</li><li><a href="https://leetcode-cn.com/problems/subsets/" target="_blank" rel="noopener">78.子集</a>: 可以使用二进制作为标记数组，来确定当前的位置是否会被选中。时间复杂度为O(n*2^n),空间复杂度为O(n)。也可以使用<a href="">回溯法</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> summary </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> leetcode </tag>
            
            <tag> algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deep learning for intelligent trafc sensing and prediction: recent</title>
      <link href="/posts/35331/"/>
      <url>/posts/35331/</url>
      
        <content type="html"><![CDATA[<p>Deep learning for intelligent traffic sensing and prediction: recent<br>advances and future challenges</p><h2 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h2><p>随着智能城市和智能交通系统概念的不断涌现，准确的交通感知和预测已成为支持城市管理和交通控制的关键。近年来，车辆互联网的迅速普及和移动服务的普及程度不断提高，为交通传感和预测应用提供了前所未有的数据。但是，要满足<br>日益复杂和多样化的大流量数据。深度学习凭借在表示学习和多级抽象方面的强大功能，最近已成为许多智能传感系统中最有效的方法。<br>本文对智能深度学习的最先进的研究工作进行最新的文献综述。<br>交通感应和预测。</p><h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><p>在将信息和通信技术(ICT)应用到现实世界的不同城市领域中，智能城市的概念已经变得流行起来。“智慧城市”指的是技术密集型的生态系统，旨在提供广泛的无所不在的服务和公用事业应用，如智能交通、家庭自动化、智能电网、电子健康、环境监测和智能物流。随着人口的快速增长和车辆数量的空前增长，智能交通管理已经成为智能城市可持续发展的关键。</p><p>本文的贡献：</p><ul><li>我们提供了一个系统的回顾深度学习，特别是在ITS的智能交通感知和预测。</li><li>我们研究了不同类型的代表性深度学习模型，并提供了针对不同ITS应用的定制的详细分析。</li><li>从数百篇相关论文中对ITS的交通感知与预测进行应用层面的细致的研究，从不同角度进行深入分析。</li><li>我们深入讨论了深度学习在ITS多个重要领域中出现的研究挑战，并展望了这一有前景的研究领域的未来方向。</li></ul><h2 id="Traffic-sensing-and-prediction-in-ITS-an-overview"><a href="#Traffic-sensing-and-prediction-in-ITS-an-overview" class="headerlink" title="Traffic sensing and prediction in ITS: an overview"></a>Traffic sensing and prediction in ITS: an overview</h2><p>复杂的交通预测问题不能简单的通过现有的传统的机器学习技术实现，原因如下：</p><ol><li>传统的机器学习模型只有较浅的表示学习空间，无法为大型交通数据集保留足够有用的特征。</li><li>浅层的机器学习模型依赖于手工制作的特征，不能自动提取高维表示来进行联合学习。</li><li>尽管输入的交通感知数据爆炸式增长，经典的机器学习模型不能通过开发更多有价值的交通预测表示来提高其性能。<br>因此，深度学习驱动的交通预测成为必然、迫切和可行的。</li></ol><h3 id="Key-components-in-ITS"><a href="#Key-components-in-ITS" class="headerlink" title="Key components in ITS"></a>Key components in ITS</h3><p>ITS结构中有四个主要的组件：</p><ul><li>sensor networks:负责从车辆和移动设备(主要通过无线传感)收集路网交通信息的主要子系统</li><li>transmission technologies:无线通信技术是在交通传感器和交通监控系统之间传输实时交通数据的关键技术</li><li>deep-learning models:深度学习模型是深度神经网络处理其信息的核心组成部分。实质上，深度学习是机器学习(ML)的一个子领域。</li><li>traffic management operations:交通管理操作是将来自交通感知和深度学习模型的信息付诸实践的最后一步。交通管理单元包括交通预测(本文的一个基本范围)、交通优化和拥塞控制。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="" alt=""> </p><h3 id="Previous-efforts-of-related-reviews-and-surveys"><a href="#Previous-efforts-of-related-reviews-and-surveys" class="headerlink" title="Previous efforts of related reviews and surveys"></a>Previous efforts of related reviews and surveys</h3><p>讲述了一下先前的研究，并总结目前还缺乏一份最新的关于ITS的综述。</p><h2 id="Deep-learning-preliminaries"><a href="#Deep-learning-preliminaries" class="headerlink" title="Deep learning preliminaries"></a>Deep learning preliminaries</h2><h3 id="A-brief-introduction-to-deep-learning"><a href="#A-brief-introduction-to-deep-learning" class="headerlink" title="A brief introduction to deep learning"></a>A brief introduction to deep learning</h3><h3 id="Deep-learning-for-trafc-sensing-and-prediction-a-brief-chronology"><a href="#Deep-learning-for-trafc-sensing-and-prediction-a-brief-chronology" class="headerlink" title="Deep learning for trafc sensing and prediction: a brief chronology"></a>Deep learning for trafc sensing and prediction: a brief chronology</h3><h3 id="Deep‑learning-models-for-ITS"><a href="#Deep‑learning-models-for-ITS" class="headerlink" title="Deep‑learning models for ITS"></a>Deep‑learning models for ITS</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>时间序列分类的深度学习：综述</title>
      <link href="/posts/63541/"/>
      <url>/posts/63541/</url>
      
        <content type="html"><![CDATA[<p>Time Series Classification(TSC) 时间序列分类</p><h3 id="时间序列分类定义"><a href="#时间序列分类定义" class="headerlink" title="时间序列分类定义"></a>时间序列分类定义</h3><p>定义1： Definition 1<br>A univariate tie series $X =[x_1,x_2,…,x_T]$ is an ordered set of realvalues.<br>The length of X is equal to the number of real values T.<br>一个单变量时间序列$X =[x_1,x_2,…,x_T]$是一个实数值的有序集。X的长度等于实值T的个数。</p><p>定义2：Definition 2<br>An M-dimensional MTS, $X =[X^1, X^2,…,X^M]$ consists of M different univariate time series with $X^i \in \mathbb R^T$.<br> 一个M维MTS， $X =[X^1, X^2,…,X^M]$由M个不同的单变量时间序列组成，其中$X^i \in \mathbb R^T$。</p><p>定义3：Definition 3<br> A dataset $D = {(X_1,Y_1),(X_2,Y_2),…,(X_N,Y_N) }$ is a collection of pairs $(X_i,Y_i)$ where $X_i$ could either be a univariate or multivariate time series with $Y_i$ as its corresponding one-hot label vector.<br> For a dataset containing K classes, the one-hot label vector $Y_i$ is a vector of length K where each element $j \in [1,K]$ is equal to 1 if the class of $X_i$ is j and 0 otherwise.<br> 一个数据集 $D = {(X_1,Y_1),(X_2,Y_2),…,(X_N,Y_N) }$ 是对$(X_i,Y_i)$的集合，其中$X_i$可以是单变量时间序列，也可以是多变量时间序列。$Y_i$是对应的一个热标记向量。<br> 对于包含K个类的数据集，单热标签向量$Y_i$是一个长度为K的向量，其中类为X时，每个元素$j \in [1,K]$都等于1，如果$X_i$为j，其他为0。</p><p>TSC的任务是在数据集D上训练分类器，以便从可能输入的空间映射到类变量值(标签)的概率分布。</p><h3 id="时间序列分类的深度学习"><a href="#时间序列分类的深度学习" class="headerlink" title="时间序列分类的深度学习"></a>时间序列分类的深度学习</h3><p><img src= "/img/loading.gif" data-lazy-src="image/survey/f001.png" alt=""></p><p>$ f_L(\theta_L,x) = f_{L-1}(\theta_{L-1},f_{L-2}(\theta_{L-2},…,f_1(\theta_1,x)))$</p><p> feed-forward propagation 前馈传播<br> transfer learning 转移学习<br> 首先，权值被随机初始化(LeCun et al. 1998b)，尽管一个健壮的替代方法是对源数据集使用一个预先训练好的模型，然后对目标数据集进行微调(Pan和Yang 2010)。这个过程被称为转移学习。</p><p>MLP<br>CNN<br>ESN</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>特征提取挖掘</title>
      <link href="/posts/8726/"/>
      <url>/posts/8726/</url>
      
        <content type="html"><![CDATA[<p>任务：</p><p>1.离十字路口距离(跳段)<br>2.每个link的出度入度(多跳)<br>3.出度入度的比值</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>driver behavior datasets</title>
      <link href="/posts/44404/"/>
      <url>/posts/44404/</url>
      
        <content type="html"><![CDATA[<h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><h3 id="传感器、驾驶事件"><a href="#传感器、驾驶事件" class="headerlink" title="传感器、驾驶事件"></a>传感器、驾驶事件</h3><table><thead><tr><th>Name</th><th>Provider</th><th>Samples</th><th>Sensor</th><th>label</th><th>Object</th><th>paper model</th></tr></thead><tbody><tr><td>Vehicle driving behavior(noget)</td><td>IEEE DataPort</td><td>10,000</td><td>accelerometer、gyroscope</td><td>5<br>acceleration <br> normal driving<br>collision<br>left turn<br>right turn</td><td>idenepsy the driver’s driving behavior</td><td>MV-CNN</td></tr><tr><td>driverbehavior datasets(get)</td><td>github</td><td>69</td><td>accelerometer、gyroscope、aceleracaoLinear、campoMagnetico</td><td>7<br>Aggressive breaking<br>Aggressive acceleration<br>Aggressive left turn<br>Aggressive right turn<br>Aggressive left lane change<br>Aggressive right lane change<br>Non-aggressive event</td><td>driving behavior classification</td><td>ANN 、SVM、RF、BN</td></tr><tr><td>Mendeley Data - Driving Behavior Dataset(get)</td><td>mendeley</td><td>1,149</td><td>accelerometer、gyroscope</td><td>4<br>Sudden Acceleration<br>Sudden Left turn<br>Sudden Right turn<br>Sudden Break</td><td>driving behavior classification</td><td>—</td></tr><tr><td>DBASA(get)</td><td>github</td><td>accelerometer、gyroscope</td><td>—</td><td>AA AR AL AB(无法进行标签对应，还需要再研究一下)</td><td>driving behavior classification</td><td>—</td></tr></tbody></table><ol><li><a href="https://ieee-dataport.org/documents/vehicle-driving-behavior" target="_blank" rel="noopener">Vehicle driving behavior</a></li><li><a href="https://github.com/jair-jr/driverBehaviorDataset" target="_blank" rel="noopener">driverbehavior datasets</a></li><li><a href="https://data.mendeley.com/datasets/jj3tw8kj6h/2" target="_blank" rel="noopener">Mendeley Data - Driving Behavior Dataset</a></li><li><a href="https://github.com/absarar/ruhdriversdataset" target="_blank" rel="noopener">DBASA</a></li></ol><h3 id="自动驾驶行为预测的数据集（带有视频的数据集、车载数据）"><a href="#自动驾驶行为预测的数据集（带有视频的数据集、车载数据）" class="headerlink" title="自动驾驶行为预测的数据集（带有视频的数据集、车载数据）"></a>自动驾驶行为预测的数据集（带有视频的数据集、车载数据）</h3><p><img src= "/img/loading.gif" data-lazy-src="/image/survey/g.png" alt=""></p><ol><li><a href="https://github.com/driving-behavior/DBNet" target="_blank" rel="noopener">DBNet</a></li><li><a href="https://usa.honda-ri.com/hdd" target="_blank" rel="noopener">HDD(发邮件获取)</a> 需要发送邮件获取</li><li><a href="https://github.com/commaai/comma2k19" target="_blank" rel="noopener">comma2k19</a><a href="https://academictorrents.com/details/65a2fbc964078aff62076ff4e103f18b951c5ddb" target="_blank" rel="noopener">原始数据get</a></li><li><a href="https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub" target="_blank" rel="noopener">DDD17(需翻墙)</a>  <a href="https://sites.google.com/view/davis-driving-dataset-2020/home" target="_blank" rel="noopener">DDD20</a><br><a href="https://github.com/SensorsINI/ddd20-utils" target="_blank" rel="noopener">数据查看工具github</a><br><a href="https://www.pkuml.org/resources/pku-ddd17-car.html" target="_blank" rel="noopener">PKU-DDD17-CAR Dataset</a></li><li><a href="">Brain4Cars</a> <a href="https://arxiv.org/pdf/1601.00740.pdf">论文</a></li><li><a href="https://github.com/ndrplz/dreyeve" target="_blank" rel="noopener">Dr(eye)ve</a> <a href="https://arxiv.org/pdf/1705.03854.pdf" target="_blank" rel="noopener">论文</a></li><li><a href="http://data.nvision2.eecs.yorku.ca/JAAD_dataset/" target="_blank" rel="noopener">JAAD</a> [论文]</li><li><a href="">UAH</a></li><li><a href="">Elektra(DrivFace)</a></li><li><a href="https://github.com/udacity/self-driving-car/tree/master/datasets" target="_blank" rel="noopener">Udacity</a></li><li><a href="https://github.com/commaai/research" target="_blank" rel="noopener">comma.ai</a></li><li><a href="">DIPLECS Surrey</a></li><li><a href="">DIPLECS Sweden</a></li><li><a href="">EISATS(Set 11)</a></li></ol><hr><h2 id="Vehicle-driving-behavior-IEEE-DataPort"><a href="#Vehicle-driving-behavior-IEEE-DataPort" class="headerlink" title="Vehicle driving behavior | IEEE DataPort"></a>Vehicle driving behavior | IEEE DataPort</h2><p><a href="https://ieee-dataport.org/documents/vehicle-driving-behavior" target="_blank" rel="noopener">数据集来源</a></p><h3 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h3><p>提供了两个txt文件,一个是原始数据集(RAW_DataSet.txt),另一个是增强数据集(Augmented_dataset.txt)</p><p>首次使用这个数据集的论文是”Vehicle Driving Behavior Recognition Based on Multi-View Convolutional Neural Network (MV-CNN) with Joint Data Augmentation”<br>我们开发了一个实验系统来收集驾驶行为数据和评估我们提出的方法。数据采集模块选择六轴传感器(MPU 6050)，利用Wi-Fi模块实时传输数据。集成的三轴加速度计和三轴陀螺仪在动态环境下精确输出模块当前姿态。<br>数据采集设备采用STM32与MPU 6050集成，实现运动处理器的实时数据采集。分别安装SD卡和Wi-Fi无线传输模块进行存储和实时数据传输。<br>MPU 6050芯片被焊接在车辆的中心，其正面朝上。芯片x轴的正方向就是车辆头部的方向。正的y轴方向为左侧方向，正的z轴方向垂直于水平方向。<br>收集装置安装在遥控车上，然后接通电源。持续采集数据，通过Wi-Fi模块以10hz的频率发送到接收服务器。同时，我们在收集过程中记录了各种动作的日志信息，包括当前动作标记、开始时间戳和结束时间戳。在车辆加速、碰撞、左转弯、右转弯和正常行驶过程中进行数据采集。每个操作都存储在一个十六进制数据文件中，该文件还包括示例的特定操作标签和时间戳。<br>最终收集1032个事件，其中加速、碰撞、左转弯、右转弯和正常驾驶事件146、224、196、200和266个。<br>本数据集使用拉格朗日插值将每个事件调整为300个样本。采集的原始数据分别包含x轴、y轴和z轴的加速度计和陀螺仪值。</p><p>使用MAWF从1032个原始样本中生成2500个扩增样本。应用BNF算法后，2500个样本扩展到5000个样本。随后，采用RC算法将5000个样本扩展到10000个样本。将原始不平衡数据转换为平衡数据集，其中每个事件样本的数量相等。最后我们得到了10,000个样本用于我们的实验。</p><h3 id="数据集格式"><a href="#数据集格式" class="headerlink" title="数据集格式"></a>数据集格式</h3><p>Event: {‘acc’: array([[x_axis], [y_axis], [z_axis], ‘gyr’,array([x_axis], [y_axis], [z_axis], ‘label’: No ]</p><ul><li>No =1 means acceleration.</li><li>No =2 means normal driving.</li><li>No =3 means collision.</li><li>No =4 means left turn.</li><li>No =5 means right turn.</li></ul><p>包含两个传感器：accelerometer 和 gyroscope 以及对应的label(5个)</p><h2 id="driverbehavior-datasets-已获得"><a href="#driverbehavior-datasets-已获得" class="headerlink" title="driverbehavior datasets(已获得)"></a>driverbehavior datasets(已获得)</h2><p><a href="https://github.com/jair-jr/driverBehaviorDataset" target="_blank" rel="noopener">数据集来源</a></p><p>包含四个传感器：accelerometer 和 gyroscope 以及对应的label(7个)</p><p>智能手机四个传感器数据，包含7个标签</p><h2 id="Mendeley-Data-Driving-Behavior-Dataset（已获得）"><a href="#Mendeley-Data-Driving-Behavior-Dataset（已获得）" class="headerlink" title="Mendeley Data - Driving Behavior Dataset（已获得）"></a>Mendeley Data - Driving Behavior Dataset（已获得）</h2><p><a href="https://data.mendeley.com/datasets/jj3tw8kj6h/2" target="_blank" rel="noopener">数据集来源</a></p><p>包含下面两个传感器的数据：</p><ol><li>accelerometer (X,Y,Z axis in meters per second squared (m/s2)) </li><li>gyroscope (X,Y, Z axis in degrees per second (°/s) )</li></ol><p>采样率：平均每秒2个样本(行)<br>车：Ford Fiesta 1.4,  Ford Fiesta 1.25, Hyundai i20<br>驾驶员：3 different drivers with the ages of 27, 28 and 37<br>Driver Behaviors: </p><ul><li>Sudden Acceleration (Class Label: 1)</li><li>Sudden Right Turn (Class Label: 2)</li><li>Sudden Left Turn (Class Label: 3)</li><li>Sudden Break (Class Label: 4)</li></ul><p>Best Window Size: 14 seconds<br>Sensor: MPU6050<br>Device: Raspberry Pi 3 Model B</p><p>包含两个传感器：accelerometer 和 gyroscope 以及对应的label(4个)</p><h2 id="comma2k19"><a href="#comma2k19" class="headerlink" title="comma2k19"></a>comma2k19</h2><p><a href="https://github.com/commaai/comma2k19" target="_blank" rel="noopener">数据来源</a></p><p>这是加利福尼亚280高速公路上超过33小时通勤的数据集。<br>在加利福尼亚州圣何塞和旧金山之间20公里的高速公路上行驶了2019段，每段1分钟。<br>comma2k19是一个完全可重现且可扩展的数据集。<br>数据采用comma EONs收集，其传感器类似于任何现代智能手机，包括道路相机，手机GPS，温度计和9轴IMU。<br>此外，EON还使用comma  grey panda捕获原始GNSS测量值和汽车发送的所有CAN数据。</p><p>数据被分成10块，每一块大约200分钟的车程。数据集的1-2块是RAV4，其余的是civic。RAV4的dongle_id是b0c9d2329ad1606b, civic的dongle_id是99c94dc769b5d96e。</p><h2 id="DBASA-已获得"><a href="#DBASA-已获得" class="headerlink" title="DBASA(已获得)"></a>DBASA(已获得)</h2><p><a href="https://github.com/absarar/ruhdriversdataset" target="_blank" rel="noopener">数据来源</a></p><p>15个驾驶员，每个里面包含两个传感器：accelerometer 和 gyroscope<br>有个最终的数据集，记录AR、AL、AA、AB的值以及是否Aggressive</p><p>暂时无法对应相应的标签(是否Aggressive)</p><h2 id="一些其他驾驶行为方面的数据集"><a href="#一些其他驾驶行为方面的数据集" class="headerlink" title="一些其他驾驶行为方面的数据集"></a>一些其他驾驶行为方面的数据集</h2><ol><li><a href="https://github.com/driving-behavior/DBNet" target="_blank" rel="noopener">DBNet</a>：厦门大学 SCSC 实验室李军教授团队与上海交大 MVIG 实验室卢策吾教授团队联合发布大规模驾驶行为数据集。DBNet 是专为研究驾驶行为的策略学习而设置的。DBNet 数据集记录了视频、激光雷达点云，以及对应的资深驾驶员（驾龄超过 10 年）的真实驾驶行为。</li><li><a href="https://usa.honda-ri.com/HDD" target="_blank" rel="noopener">HDD</a> hdd@honda-ri.com<br>3个摄像头：记录影像最终被压缩成1280*720分辨率、30fps；<br>1个水平激光雷达：水平视角360度，垂直视角26.9度；<br>1个汽车动力运动分析器：记录陀螺仪、加速计和GPS信号；<br>1个汽车控制器区域网络（CAN）：记录油门角度、刹车压力、轮胎角度、偏航率和速度等信息。</li><li><a href="https://usa.honda-ri.com/h3d" target="_blank" rel="noopener">H3D</a></li><li><a href="http://www.cvlibs.net/datasets/kitti/raw_data.php" target="_blank" rel="noopener">KTTI</a>:数据集的语义标签包括‘Road’，‘City’，‘Person’，‘Campus’和‘Residential’五大类。对于3D物体检测，label细分为car, van, truck, pedestrian, pedestrian(sitting), cyclist, tram以及misc组成。</li><li><a href="http://bdd-data.berkeley.edu" target="_blank" rel="noopener">伯克利DeepDrive</a></li><li><a href="https://github.com/commaai/research" target="_blank" rel="noopener">comma.ai</a></li><li><a href="https://github.com/udacity/self-driving-car/tree/master/datasets" target="_blank" rel="noopener">Udacity</a></li><li><a href="https://github.com/cloudpose/smartphone_driving_dataset" target="_blank" rel="noopener">smartphone_driving_dataset</a>: image、CAN、smartphone</li><li><a href="http://www.cvl.isy.liu.se/en/research/datasets/amuse/" target="_blank" rel="noopener">AMUSE</a></li><li><a href="https://www.nhtsa.gov/" target="_blank" rel="noopener">NHTSA</a></li><li><a href="https://www.cis.fordham.edu/wisdm/dataset.php" target="_blank" rel="noopener">WISDM</a> 人类行为标签、 加速计和gps</li><li><a href="https://www.kaggle.com/malekzadeh/motionsense-dataset" target="_blank" rel="noopener">MotionSense Dataset : Smartphone Sensor Data - HAR</a> 人类行为标签、 加速计和gps</li><li><a href="https://lbd.udc.es/research/real-life-HAR-dataset/" target="_blank" rel="noopener">A Public Domain Dataset For Real-life Human Activity Recognition Using Smartphone Sensors</a></li><li><a href="https://insight.shrp2nds.us/profile/applyForQualifiedResearcher" target="_blank" rel="noopener">SHPR2</a></li></ol><h2 id="视频类数据，推断车辆行为："><a href="#视频类数据，推断车辆行为：" class="headerlink" title="视频类数据，推断车辆行为："></a>视频类数据，推断车辆行为：</h2><ol><li><p><a href="http://apolloscape.auto/scene.html" target="_blank" rel="noopener">Apollo-scape数据集</a>:1):Apollo-scape拥有广泛的驾驶场景/机动，如超车、变道。里面包含了大量复杂行为的实例，如超车，变道。一段驾驶视频</p></li><li><p><a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php" target="_blank" rel="noopener">KITTI</a>:<br>该数据集仅限于一个城市和城市周边地区，并且只有很少的高速公路。<a href="http://www.cvlibs.net/datasets/kitti/eval_tracking.php" target="_blank" rel="noopener">get</a> </p><h2 id="自动驾驶行为预测的数据集"><a href="#自动驾驶行为预测的数据集" class="headerlink" title="自动驾驶行为预测的数据集"></a>自动驾驶行为预测的数据集</h2><ol><li><a href="https://github.com/driving-behavior/DBNet" target="_blank" rel="noopener">DBNet</a></li><li><a href="https://usa.honda-ri.com/hdd" target="_blank" rel="noopener">HDD(发邮件获取)</a> 需要发送邮件获取</li><li><a href="https://github.com/commaai/comma2k19" target="_blank" rel="noopener">comma2k19</a><a href="https://academictorrents.com/details/65a2fbc964078aff62076ff4e103f18b951c5ddb" target="_blank" rel="noopener">原始数据get</a></li><li><a href="https://docs.google.com/document/d/1HM0CSmjO8nOpUeTvmPjopcBcVCk7KXvLUuiZFS6TWSg/pub" target="_blank" rel="noopener">DDD17(需翻墙)</a>  <a href="https://sites.google.com/view/davis-driving-dataset-2020/home" target="_blank" rel="noopener">DDD20</a><br><a href="https://github.com/SensorsINI/ddd20-utils" target="_blank" rel="noopener">数据查看工具github</a><br><a href="https://www.pkuml.org/resources/pku-ddd17-car.html" target="_blank" rel="noopener">PKU-DDD17-CAR Dataset</a></li><li><a href="">Brain4Cars</a> <a href="https://arxiv.org/pdf/1601.00740.pdf">论文</a></li><li><a href="https://github.com/ndrplz/dreyeve" target="_blank" rel="noopener">Dr(eye)ve</a> <a href="https://arxiv.org/pdf/1705.03854.pdf" target="_blank" rel="noopener">论文</a></li><li><a href="http://data.nvision2.eecs.yorku.ca/JAAD_dataset/" target="_blank" rel="noopener">JAAD</a> [论文]</li><li><a href="">UAH</a></li><li><a href="">Elektra(DrivFace)</a></li><li><a href="https://github.com/udacity/self-driving-car/tree/master/datasets" target="_blank" rel="noopener">Udacity</a></li><li><a href="https://github.com/commaai/research" target="_blank" rel="noopener">comma.ai</a></li><li><a href="">DIPLECS Surrey</a></li><li><a href="">DIPLECS Sweden</a></li><li><a href="">EISATS(Set 11)</a></li></ol></li></ol><h2 id="一些获取数据集的网站"><a href="#一些获取数据集的网站" class="headerlink" title="一些获取数据集的网站"></a>一些获取数据集的网站</h2><ol><li><a href="http://www.shujujishi.com/dataset.html" target="_blank" rel="noopener">数据集市</a></li><li><a href="https://www.polymtl.ca/wikitransport/index.php?title=Public_Transportation_Datasets" target="_blank" rel="noopener">Public Transportation Datasets</a></li><li><a href="http://archive.ics.uci.edu/ml/datasets.php" target="_blank" rel="noopener">UCI</a></li><li><a href="https://tianchi.aliyun.com/dataset" target="_blank" rel="noopener">天池</a></li><li><a href="https://www.graviti.cn/open-datasets" target="_blank" rel="noopener">格物钛</a></li><li><a href="https://grouplens.org/datasets/" target="_blank" rel="noopener"></a></li><li><a href="http://yacvid.hayko.at/" target="_blank" rel="noopener">YACVID</a></li><li><a href="https://www.utwente.nl/en/eemcs/ps/research/dataset/" target="_blank" rel="noopener">手机相关方面的数据</a></li><li><a href="http://sensors.ini.uzh.ch/databases.html" target="_blank" rel="noopener">sensor Dataset</a></li></ol><h2 id="驾驶事件识别一些应用"><a href="#驾驶事件识别一些应用" class="headerlink" title="驾驶事件识别一些应用"></a>驾驶事件识别一些应用</h2><p><a href="https://github.com/amrit015/Driver-Behavior-Analysis" target="_blank" rel="noopener">Driver-Behavior-Analysis</a><br><a href="https://github.com/mohebbihr/Recognition-of-Driver-behavior-using-Smartphone" target="_blank" rel="noopener">Recognition-of-Driver-behavior-using-Smartphone</a><br><a href="https://github.com/keencyclist/driving" target="_blank" rel="noopener">driving</a><br><a href="https://www.sentiance.com/2016/02/11/driving-behavior/" target="_blank" rel="noopener">driving-behavior</a><br><a href="https://github.com/mghatee/Overall-Driving-Behavior-Recognition-By-Smartphone" target="_blank" rel="noopener">Overall-Driving-Behavior-Recognition-By-Smartphone</a></p><p>驾驶风格识别。驾驶风格的定义有很多种，包括油耗、刹车、保持距离和侵略性。建立司机风格可以用来调整驾驶策略，如车道合并或提醒司机，如果他是鲁莽的[9]。最后，一些数据集[52]也收集了行人意向数据，如图3.10所示。然后可以训练算法来识别行人是否想过马路，并帮助防止碰撞。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>智能手机对理解驾驶行为的当前挑战</title>
      <link href="/posts/18430/"/>
      <url>/posts/18430/</url>
      
        <content type="html"><![CDATA[<p>智能手机感知理解驾驶行为:目前的实践和挑战</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>理解驾驶行为——即使是在自动驾驶技术迅速出现的情况下——仍然是人们关注的焦点，这有助于分解复杂的驾驶动力学，开发用户友好且可接受的自动驾驶汽车，并确保自动驾驶汽车和传统汽车在道路上安全共存。移动众感技术已经成为理解和模拟驾驶行为的一种手段。虽然通过智能手机收集数据的优势有很多(速度、准确性、成本低等)，但挑战包括但不限于准备速度、处理需求，以及方法、立法和安全问题，都是巨大的。本文旨在回顾基于智能手机传感器数据流分析驾驶行为的研究。我们首先建立一个包容性的逐步框架来描述从数据收集到知情决策的路径。接下来，对现有的文献进行了深入分析，并对数据收集和数据挖掘实践方面的挑战进行了批判性的讨论，特别强调了使用手机收集驾驶数据以及使用人群感知数据进行特征提取的局限性和关注点。随后，建模驾驶行为实践和端到端解决方案的司机协助和推荐系统也进行了回顾。本文最后讨论了文献中出现的最关键的挑战和未来的研究步骤。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>驾驶行为对于维护安全(Sagberg et al.， 2015)和可持续交通(Huang et al.， 2015)起着决定性的作用。2018)。除此之外，它还会影响交通流量、燃油消耗、空气污染、公众健康以及个人心理健康。<br>对于“驾驶行为”或“驾驶风格”这一概念已经给出了许多定义(Sagberg et al.， 2015)。这里我们接受了Lajunen和ozkan(2011)给出的定义:<strong>“驾驶风格涉及个人驾驶习惯——即驾驶员选择驾驶的方式”</strong>。大量的文献强调了在交通条件、排放、压力缓解等方面采用安全和环保驾驶行为的好处。</p><p>从<strong>宏观</strong>的角度，特别是在交通管理和控制领域，驾驶行为是非常重要的，因为特定驾驶行为与交通拥堵水平显著相关；<br>从<strong>微观</strong>的角度，推荐和驾驶辅助系统对于改善个人驾驶行为和提高驾驶员对其驾驶方式影响的认识至关重要。</p><p>驾驶员在选择加速和减速的方式、与前车的距离以及是否超过限速(超速)方面存在差异(Miyajima et al.， 2007;Mantouka等人，2019年)。<br>因此，在有关交通和道路安全的文献中已经确定了几种驾驶概况。激进驾驶已经得到了广泛的研究，</p><ul><li>大部分研究集中在识别严重的加速和制动事件(Kalra和Bansal, 2014;Eboli 2016)</li><li>大量的研究对危险驾驶进行了调查，这些研究大多是指超速驾驶，甚至无视交通标志和规则(Harbeck and Glendon, 2018)。</li><li>在过去的几十年里，在温室效应的影响下，许多研究都侧重于通过识别所谓的生态驾驶来分析能有效消耗燃料，从而减少空气污染物的驾驶习惯。此外，还会对驾驶员的驾驶行为进行调查，如超速、酒后驾驶和驾驶时使用手机(Ma et al.， 2017)。</li></ul><p>最普遍的驾驶概况包括:</p><ul><li><span class="inline-tag red"><strong>Aggressive driving</strong></span>:<strong>激进驾驶</strong>，如紧跟在后tailgating，剧烈加速harsh acclerating，刹车braking和转弯cornering，不正确的换道improper lane changing等等（史密斯等，2016)</li><li><span class="inline-tag red"><strong>Distracted driving</strong></span>:<strong>分心驾驶</strong>，如分心驾驶，如发短信、吃东西、喝饮料或打电话，也就是驾驶员在驾驶时注意力不集中（chen et al. 2015）</li><li><span class="inline-tag red"><strong>Risk taking</strong></span>：<strong>风险承担/勇于冒险</strong>，例如超速驾驶，违反交通规则或与前车距离过近</li><li><span class="inline-tag red"><strong>Eco-driving</strong></span>：<strong>环保驾驶</strong>，即节能驾驶，从而最大限度地减少污染</li><li><span class="inline-tag red"><strong>Safe driving</strong></span>: <strong>安全驾驶</strong>，正常低风险驾驶行为</li></ul><p>在自动驾驶汽车时代，除了确保交通安全和可持续发展，了解驾驶行为对于开发用户友好且容易接受的机器仍然至关重要。对于用户的接受度来说，自动驾驶汽车不仅要安全可靠，更要提供舒适的用户体验。类人自动驾驶的风格是这项新技术被大众接受的关键。为了训练机器像人一样驾驶，更好地理解驾驶员的驾驶方式非常有用(库德勒等人，2015:Dong等人，2016)。开发能够模拟实际驾驶员行为并以类似方式做出决策的适当软件，是将自动驾驶完全融入日常生活中最具挑战性的任务之一。</p><p>综上所述，很容易理解理解和彻底调查驾驶行为是非常重要的。智能手机的高渗透率极大地推动了数据收集的速度、频率和准确性。这样就可以收集到大量的自然驾驶数据，这些数据通常用于<strong>调查驾驶行为</strong>，<strong>检测不安全驾驶习惯</strong>，<strong>预测路网状况</strong>。<br>为了从多个传感器和设备的数据中提取有意义的信息，以及是否有可能依靠智能手机数据中的驾驶信息来决定政策和决策，这些任务的实施产生了许多问题。<br>尽管最近的一些研究试图回答这些问题(Ferreira Júnior et al., 2017; Chan et al. 2019)，研究人员大多只关注这一现象的一个方面，而没有解决从数据收集到结果利用的所有必要阶段出现的问题。</p><ul><li>Chan et al.(2019)对驾驶行为检测算法进行了全面综述，特别强调了它们的性能和精度度量。</li><li>在同样的背景下，其他研究人员FerreiraJúnior et al.(2017)列出了当前的机器学习(ML)并进行了一个真实世界的实验，以评估比较结果在驾驶事件分类<a href="/posts/600001/" title="全文笔记">全文笔记</a></li></ul><p>在这项工作中，我们扩展了对驾驶行为的分析，将重点放在从数据收集到建模和决策的海量智能手机传感中揭示行为洞察力的具有挑战性的方面。<br>最后，本文的<strong>目的</strong>是提出一个包容性的方法框架，以理解驾驶行为通过智能手机传感和协助研究人员选择最合适的工具和方法时，处理驾驶分析。</p><ul><li>首先，概述了所提议的框架。</li><li>随后,最近的文献分析和批判性评估一个显式的关注的主要挑战出现在数据收集和存储、数据准备、数据挖掘、建模的驾驶行为和司机作出最终决策和推荐系统,我们将讨论未来的研究方向和总体的结论。</li></ul><h2 id="从智能手机数据到知情决策-一个分析框架"><a href="#从智能手机数据到知情决策-一个分析框架" class="headerlink" title="从智能手机数据到知情决策:一个分析框架"></a>从智能手机数据到知情决策:一个分析框架</h2><p>将智能手机传感器转换成有意义的驾驶行为信息，不是一个简单的过程。</p><p><strong>数据方面，理解数据</strong>   </p><ol><li>首先，从智能手机收集的数据通常<strong>存在质量问题</strong>，并且含有<strong>大量的噪音</strong>，因为它们受到道路状况和智能手机在车中的位置的显著影响。</li><li>此外，在监测阶段使用的传感器种类繁多，如加速度计、陀螺仪、GPS等，导致在内容和质量方面具有<strong>大量不确定度的多源数据集</strong>。</li><li>利用这些数据集进行驾驶行为调查的首要步骤包括<strong>对数据的理解</strong>。这一步包括所有程序的<strong>数据清洗</strong>，<strong>数据噪声排除</strong>和<strong>几个其他预处理技术</strong>。</li></ol><p><strong>提取特征</strong>  </p><ol><li>随后，可以从与行程和驾驶相关的数据中提取有趣的特征。</li><li>在驾驶分析中，与行程相关的特征是指<strong>模式检测</strong>和<strong>驾驶员-乘客的区别</strong>。</li><li>在一些研究中，模式检测系统已经被开发出来。以<strong>聚合的水平(机动化和非机动化)</strong> 或<strong>分解的水平(公交、地铁、汽车、步行等)</strong> 进行预测(Nour等，2015:Shin等，2015)。</li><li>其他研究集中在通过使用ML技术利用智能手机数据来<strong>识别用户是司机还是乘客</strong>(Ahmad等，2019) </li></ol><p><strong>建模</strong><br>一旦研究人员完全了解他们掌握的数据类型，驾驶行为的建模就会随之展开。<br>现代计算机巨大的计算和存储能力允许处理大量数据和开发能够确定最复杂行为和模式的复合模型。<br>到目前为止，研究人员大多在寻找提高计算时间(即并行计算或高性能计算(HPC)资源)的方法来管理主要来自出租车GPS数据的大型城市规模数据集(Hu et al.， 2017;Ma等人，2019)。<br>为了进一步开发计算机的无限计算能力，先进的机器学习技术也被开发。分析驾驶模式和识别不安全驾驶行为特征的任务已经在driving analytics (Vlahogianni and)的文献中得到了广泛的研究（Barmpounakis，2017a：Kang and Baneriee，2018）</p><p>理解驾驶行为和检测不安全、异常驾驶特征的能力，催生了大量旨在改善驾驶行为、帮助驾驶员养成更高效、更安全驾驶习惯的应用。<br>此外，驾驶行为分析对于道路运营商确保交通效率和改善交通状况的交通管理和控制非常有用(Hu et al.2015)。<br>最后，利用保险telematics市场的不安全驾驶行为检测系统销售基于使用的保险方案</p><p>上面的复杂性用图1所示的逐步方法来描述。这种逐步的方法是接下来分析的基础。</p><p><img src= "/img/loading.gif" data-lazy-src="/image/test/g001.jpg" alt=""></p><h2 id="移动众感的关键问题-mobile-crowdsensing"><a href="#移动众感的关键问题-mobile-crowdsensing" class="headerlink" title="移动众感的关键问题 mobile crowdsensing"></a>移动众感的关键问题 mobile crowdsensing</h2><p>大规模的现象，比如理解驾驶行为，并不能简单地由一个人来衡量(Ma et al.， 2015)，因此，研究人员利用当今智能手机惊人的感知能力来收集大量的基本驾驶数据。</p><p>根据Abdulazim等人(2013)，智能手机传感器可以分为三组:   </p><ul><li><span class="inline-tag red"><strong>运动传感器</strong>，包括加速度计、陀螺仪和磁强计(如罗盘)</span></li><li><span class="inline-tag red"><strong>位置传感器</strong>，例如全球定位系统(GPS)，通常用于户外设置和基于网络的位置服务</span></li><li><span class="inline-tag red"><strong>环境传感器</strong>，包括光传感器、麦克风和接近传感器</span></li></ul><p>由于有了这些组件，智能手机已经被用作感知和计算数据的有用工具，引入了移动众感技术。</p><p>智能手机的一个<strong>明显优势</strong>是有大量的潜在参与者有资格参与数据收集的程序。这依赖于智能手机更有可能向用户提供即时反馈，因为用户总是在骚扰他们，这可能会增加长期参与的意愿(Jariyasunant et al, 2012)。</p><p>值得注意的是，<strong>收集实时数据的能力扩大了研究人员检测高度动态事件的机会</strong>(Mashhadi和Capra, 2011)。人们无论走到哪里都愿意随身携带手机，这一事实为开发用于大规模传感目的的成本效益高的系统提供了机会(Ganti等人，2011年)。<br>不过要让移动众感系统发挥最大潜力，还需要充分解决一些问题和顾虑，比如<strong>如何保证数据质量</strong>，<strong>如何解决电池耗尽问题</strong>，以及<strong>如何保证隐私和安全问题</strong>。下面几节将进一步讨论这些问题。</p><h3 id="Data-quality-数据质量"><a href="#Data-quality-数据质量" class="headerlink" title="Data quality 数据质量"></a>Data quality 数据质量</h3><p>从文献来看，<strong>数据质量</strong>受到两个主要因素的显著影响</p><ol><li><p><span class="inline-tag red"><strong>用户的可靠性</strong></span>，通常从<strong>可信度</strong>和<strong>代表性</strong>两个维度来考察。</p><ul><li>数据的可信度是指当参与者误解了所要求的任务，犯了错误，甚至故意欺骗系统，从而导致错误的结果(Kanhere, 2013)。因此，捕获的数据可能本质上是嘈杂的，可能需要额外的验证或审查。</li><li>还考虑了<strong>数据完整性</strong>的挑战，即验证收集的数据确实来自用户的设备，并在声称的位置收集(Mashhadi和Capra, 2011)。为了实现完整性，感知信息必须能够代表用户的行为和习惯。</li><li>移动众感系统的<strong>关键因素</strong>是<strong>任何人都可以参与其中</strong>。但是，应该考虑到参加者的<strong>代表性</strong>，并不是每个人都有兴趣参加这种制度。特别是在使用移动众感技术来监测大规模现象时，<strong>人群的代表性</strong>是一个重要的因素(Ganti et al.， 2011)。只有当许多个人提供相关信息时，才能准确地测量这些现象。</li><li>代表性的关键问题通常被对数据质量和整体系统性能有重大影响的研究人员忽视。</li></ul></li><li><p>影响数据质量的第二个因素来自<span class="inline-tag red"><strong>智能手机的技术特征</strong></span>。</p><ul><li>移动设备的集合，他们的感知，计算，存储和通信能力可能会有很大的不同。</li><li>智能手机运行几种<strong>不同的操作系统</strong>，最著名的是安卓(Android)和iOS，这些系统经常进行升级，以提高它们的功能和能力。在传感过程中应该考虑到这一点，因为它们直接关系到数据的质量。</li><li>一些设备<strong>电池能量</strong>有限，计算能力低，传输带宽有限(Cao和Lin, 2017)。</li><li>此外，<strong>同一类型的数据可能会从不同的传感器收集</strong>，例如位置数据可能会从GPS或Wi-Fi收集，因此质量会有所不同。</li><li>需要考虑的另一个方面是，<strong>不同类型的数据可以用于相同的目的，但质量和资源消耗的权衡不同</strong>(Ganti et al.， 2011)。</li></ul></li></ol><p>处理群众感知数据时的另一个问题是，如果把智能手机放在口袋或袋子里，可能会收集周围的<span class="inline-tag red"><strong>噪音</strong></span>。<br><strong>相同的传感器可以在不同的条件下感知相同类型的数据</strong>，例如，如果该设备被自由地放置在车辆中或是手持的。</p><ul><li>关键的考虑也应该给予设备安置在车辆。</li><li>设备的位置和方向显著影响收集数据的质量，因此最近的研究集中在如何将智能手机传感器收集的数据与车辆的运动相匹配(Vlahogianni and Barmpounakis, 2017a)。</li><li>在Wahlstrom等人(2019)中，研究人员遇到了推断智能手机在车内的位置的问题，例如，通过开发一种基于核聚类的方法，手机是否被放在乘客座位上，是否在中控台上，是否由驾驶员持有，是否被放在坐骑上。</li><li>Ghose等人(2016)开发了一个方向校正模块，以便将智能手机的加速度计读数从设备的坐标系统转换到车辆的坐标系统。</li></ul><h3 id="Energy-limitations-能量限制"><a href="#Energy-limitations-能量限制" class="headerlink" title="Energy limitations 能量限制"></a>Energy limitations 能量限制</h3><p>尽管大多数用户每天都要给智能手机充电，但在引入众感应用时，<span class="inline-tag red"><strong>电池消耗</strong></span>的显著增加是一个重要问题。</p><ul><li>从感知、处理到数据传输，众感平台的各个方面都在<strong>消耗能量</strong>(Kanhere, 2013)，智能手机的电池寿命相对较短，限制了设备用于连续感知目的的使用(Birenboim和Shoval, 2015)。</li><li>因此，<strong>在需要连续实时数据的情况下，很难获得准确的数据</strong>。更具体地说，不同的传感器以不同的方式消耗智能手机电池，而与此同时，它们感知相同类型数据的准确性却不同。根据功率测量，可以将传感器从最好到更坏(Lin et al.， 2010);</li><li>例如，在<strong>电池消耗</strong>的情况下，最好的选择是使用Wi-Fi，然后是3G，最后是GPS，而在<strong>精确度</strong>方面，最好的传感器是GPS，然后是Wi-Fi，最后是3G。需要特别关注<strong>GPS采样率</strong>，因为它不仅会显著影响能量消耗，而且还会影响数据的质量(Byon et al.， 2007)。</li></ul><h3 id="Privacy-and-security-隐私和安全"><a href="#Privacy-and-security-隐私和安全" class="headerlink" title="Privacy and security 隐私和安全"></a>Privacy and security 隐私和安全</h3><p>Boguslaw和Westin(1968)或许给出了对信息隐私最相关的定义:“<strong>信息隐私涉及个人决定何时、如何以及在何种程度上将有关他或她的信息传递给他人的权利。</strong>”<br>在移动众感方案中，<span class="inline-tag red"><strong>隐私</strong></span>是参与者对自己敏感信息的发布保持控制的保证。<br>这包括对信息的保护，这些信息可以从传感器读数本身以及用户与参与式传感系统的交互中推断出来(Christin等人，2011)。<br>大量的研究工作集中在解决隐私问题，特别是在基于位置的应用程序和检测移动模式的系统。<br>大多数研究提出了几种保护参与者隐私的技术，如<strong>匿名、假名、空间隐身、数据干扰和聚集</strong>(Christin, 2016)。<br>然而,所有这些方法仍然侧重于提供一种机制，每次解决一种特定的隐私威胁，但没有一种方法能够解决全部的隐私问题。</p><h2 id="Understanding-data-理解数据"><a href="#Understanding-data-理解数据" class="headerlink" title="Understanding data 理解数据"></a>Understanding data 理解数据</h2><p>如果有效地使用移动众感，就可以获得大量数据进行进一步分析。<br>然而，上述约束和限制的存在使得在使用数据调查驾驶行为之前必须插入一系列数据清理程序。<br>这些步骤通常包括<strong>噪声排除</strong>和<strong>特征提取</strong>。<strong>特征工程</strong>是训练机器学习算法时最需要的数据准备过程(Etemad et al. 2018)。</p><h3 id="Data-preparation-数据准备"><a href="#Data-preparation-数据准备" class="headerlink" title="Data preparation 数据准备"></a>Data preparation 数据准备</h3><p><span class="inline-tag red"><strong>处理噪音</strong></span>被认为是众包智能手机数据的第一个挑战。</p><ul><li>噪音可能是<strong>来自其他交通工具(公交、地铁等)、步行、智能手机使用、与公共交通相关的基础设施以及乘客出行的数据</strong>。</li><li>噪声数据的另一个重要来源是<strong>重力对加速度计测量值的影响</strong>(Hemminki et al. 2013;Shin等，2015)。<ul><li>GPS测量的水平精度，与所有必要的值一起返回，已用于去除噪声数据(Ghose等)。</li><li>而其他研究人员则使用了更复杂的方法，如输入延迟神经网络(IDNN) (Noureldin et al. 2011)或卡尔曼滤波器(Wahlstrom et al. 2015b)。</li><li>在最近的一项研究中，研究人员进行了以下数据预处理序列(Lu等，2018)<ul><li>i)使用低通滤波器进行数据滤波</li><li>ii)从智能手机重新定位到车辆坐标</li><li>ii)滑动道(滑动道)以提高所收集数据的质量</li></ul></li><li>在多篇论文中(Eren et al.， 2012: Bose et al..)(2018)使用低通滤波器或简单的移动平均来消除噪声。</li><li>其他研究人员使用磁强计数据从加速度计和陀螺仪数据中过滤有噪声的样本(Eftekhari和Chatee. 2016)。</li></ul></li><li>智能手机噪音这个问题<strong>依赖于许多因素</strong>。比如<strong>不同的设备，传感器，灵敏性,定位或记录频率</strong>。</li><li>应遵循不同的技术，而不是基于数据来源，而是基于这些数据将如何对待每个案例(Paefgen et al. 2012: He et al. 2014: Kanarachos et al..)</li></ul><h3 id="Feature-engineering-for-trip-information-出行信息的特征工程"><a href="#Feature-engineering-for-trip-information-出行信息的特征工程" class="headerlink" title="Feature engineering for trip information 出行信息的特征工程"></a>Feature engineering for trip information 出行信息的特征工程</h3><p>数据预处理之后通常是<span class="inline-tag red"><strong>特征提取</strong></span>。<br>应用不同的建模工具是为了:(Stipancic et al.， 2018)</p><ul><li><strong>识别来自不同模式的出行</strong></li><li><strong>识别用户是司机还是乘客</strong>,并保持与司机相对应的有效出行以供进一步分析</li><li><strong>检测碰撞</strong></li></ul><h4 id="Mode-detection-模式检测"><a href="#Mode-detection-模式检测" class="headerlink" title="Mode detection 模式检测"></a>Mode detection 模式检测</h4><p><span class="inline-tag red"><strong>检测汽车行程</strong></span>是一项相对简单的任务，主要基于GPS和加速计的速度信息(Feng and Timmermans, 2013;Shin等，2015)。<br>然而，在相关研究中可以发现一些挑战。<br>例如，在拥堵的城市道路中识别不同模式时，<strong>GPS信息甚至可能会让先进的机器学习工具感到困惑</strong>(Efthymiou等，2019)。为了解决这一问题，研究人员将GPS与地理信息系统(GIS)平台的使用相结合(Stenneth et al.， 2011)。<br>由于GPS传感器会耗尽智能手机的电池，而且在没有优化的情况下，GPS传感器的准确性也存在问题，所以其他研究人员转向了只能从加速度计获得数据的其他解决方案(Hemminki等，2013)。<br>当单独使用加速度计数据时，研究人员也转向<strong>基于峰值或基于段的特征</strong>(Hemminki等，2013)。<br>然而,大多数时候,GPS和加速度计数据的统计特性,同时检测到的变化模式最后一组特性中扮演一个重要的角色(吴et al ., 2016)。<br>在回顾的大多数情况下，<strong>将数据分割为时间窗口是一种可以得到准确结果的做法</strong>。<br>根据Nikolic和Bierlaire(2017)的说法，<strong>时域和频域特征</strong>是从智能手机数据中检测出行模式最常见的特征。<br>在用于行程模式检测的方法方面，由于加速度值所提供的信息有限，研究人员强调了概率方法的价值。<br>具体来说，几种旅行模式的相似值范围使得很难对模式进行准确的分类并区分公交车和汽车，或检测自行车(Wu et al.， 2016)。<br><strong>基于规则和概率的方法</strong>似乎超越了Eftekhari和Chatee(2016)中所述的限制。</p><h4 id="Passenger-driver-recognition-乘客-司机-识别"><a href="#Passenger-driver-recognition-乘客-司机-识别" class="headerlink" title="Passenger/driver recognition 乘客/司机 识别"></a>Passenger/driver recognition 乘客/司机 识别</h4><p>乘客/司机旅行识别比模式检测更复杂,而且对于准确识别司机的配置文件至关重要,因为它需要精确的高分辨率数据，以便分析和识别用户的微活动，这些模式可能有助于检测驾驶(Wahlstrom et al .， 2015b)。<br>一些研究强调了在试图确定乘客行程时所面临的挑战，并提出了在同一行程中需要来自多个设备的数据的方法(Wahlstrom等。Torres等人2019)使用机器学习模型来区分司机和乘客。<br>根据作者的说法，使他们的任务具有挑战性的两个原因是:<br>1)<strong>事件发生的时间窗短</strong><br>2)<strong>在事件期间驾驶员行为的可变性</strong><br>他们的结论是，<strong>使用来自不同传感器的数据可以增加模型的可预测性</strong>。</p><p>另一方面，在lohnson和Rajamani(2019)中，作者使用了来自智能手机运动传感器的数据。<br>可以看出，虽然不需要使用多个设备或外部硬件就可以很好地检测到智能手机的位置，但是<strong>当智能手机不在静态位置时，算法的可检测性降低了</strong>。<br>在Bo et al.(2013)中，驾驶员检测是基于进入车辆时的数据，从哪边发生，是在前座还是在后座。<br>He等人(2014)提供了另一种有趣的方法，当使用多个设备时，可以检测相对的左右和前后位置。</p><h3 id="feature-engineering-for-driving-performance-驾驶性能的特征工程"><a href="#feature-engineering-for-driving-performance-驾驶性能的特征工程" class="headerlink" title="feature engineering for driving performance 驾驶性能的特征工程"></a>feature engineering for driving performance 驾驶性能的特征工程</h3><ol><li><p>应用数据挖掘技术是为了理解通过智能手机传感器监控的一些基本机动和驾驶事件。更具体地说，机器学习方法被用于<strong>检测攻击性行为</strong>，如剧烈刹车、加速和转弯事件，并通过识别驾驶时的移动使用来识别驾驶员的分心。</p><ul><li>大多数关注于驾驶分析的研究已经开发出了识别<strong>严重加速和刹车事件</strong>的系统。</li><li>Ghose等人(2016)提出了一种融合算法，该算法以GPS、加速度计、车辆位置数据和连续行驶点的时差作为输入，能够区分一个行驶周期内的正常加速与剧烈加速和刹车。</li><li>Predic和Stojanovic(2015)开发了一种先进的机器学习分类器，用于检测异常机动行为，如<strong>过度刹车、突然变道、不安全转弯、超速和路线偏离</strong>。</li><li>Wahlstrom等人(2015b)也发现了<strong>harsh cornering</strong>，GPS测量用于检测驾驶员是否发生了<strong>危险的转弯事件</strong>。</li></ul></li><li><p>另一个有趣的<strong>特点</strong>是<strong>开车时使用手机</strong>。这些信息主要用于判断驾驶员是否在实际驾驶任务中分心(Mantouka et al.， 2019;Papadimitriou等，2019年)。</p><ul><li>识别次要任务(如开车时发短信、打电话)不仅可以更好地了解司机的行为和分心，还可以清除手机粗暴移动所产生的噪音。</li><li>具体来说，驾驶时使用手机可能会触发智能手机传感器，从而影响harsh事件的检测过程和关键事件的阈值。对这些运动的识别可以减少false positive事件，提高方法的检测能力(Vahogianni and Barmpounakis. 2017a)</li><li>在Ahmed(2018)研究人员已经识别出司机是否在开车时发短信、打电话或阅读。为了检测这类任务，他们使用基于过滤器的方法进行特征提取，使用基于线性相关和信息增益的两步过程。</li></ul></li><li><p>目前的<strong>研究重点</strong>是<strong>识别能够减少燃料消耗，从而减少空气污染物的环保驾驶习惯</strong>。</p><ul><li>生态驾驶可以通过描述车辆和交通状况的指标来检测，如发动机转速和齿轮对以及车辆动能的变化(Andrieu et al.， 2012)。</li><li>在缺乏这些数据的情况下，数据挖掘技术仅应用于速度和加速措施，以检测导致高效燃料消耗的驾驶模式。</li><li>为了了解驾驶员是否采取了生态驾驶行为，提取的主要特征为:<strong>超速、加速和转向率</strong>(Castignani et al.， 2013)。</li></ul></li></ol><h2 id="Modeling-driver’s-behavior-驾驶员行为建模"><a href="#Modeling-driver’s-behavior-驾驶员行为建模" class="headerlink" title="Modeling driver’s behavior 驾驶员行为建模"></a>Modeling driver’s behavior 驾驶员行为建模</h2><p>理解驾驶员行为是提高道路安全、优化网络服务水平的关键。<br>对智能手机的大规模研究表明，当司机受到监控时，他的行为相对安全(Johnson and Trivedi, 2011)。<br>从统计方法到先进的机器学习和计算智能，研究人员使用了多种方法来高效准确地模拟驾驶行为。<br>表1详细总结了驾驶行为分析领域中最重要的文献的<strong>特征、智能手机传感器、方法和输出</strong>。<br><img src= "/img/loading.gif" data-lazy-src="/image/survey/t001-1.png" alt=""><br><img src= "/img/loading.gif" data-lazy-src="/image/survey/t001-2.png" alt=""><br><img src= "/img/loading.gif" data-lazy-src="/image/survey/t001-3.png" alt=""></p><h3 id="统计方法"><a href="#统计方法" class="headerlink" title="统计方法"></a>统计方法</h3><p>一些研究人员试图在<strong>描述驾驶行为的特征之间发现有意义的相关性</strong>。</p><ul><li>Paefgen等人(2012)提出了一系列重要的统计指标，以及手机和一个参考IMU单元事件记录之间的相关性。</li><li>Chakravarty等(2013)计算了特定时间内个体驾驶员的风险指数。这种情况下的风险函数考虑了风险和异常操作的数量，如急转弯，剧烈刹车和加速，硬颠簸。</li><li>Bejani和Ghatee(2018)将几种驾驶特征的统计分析作为数据准备和更复杂的驾驶行为建模的中间步骤。</li></ul><p>另一种研究开发了一种<strong>混合效应模型</strong>，以了解超速、剧烈机动、剧烈加速和剧烈刹车等行为指标是否可以作为驾驶员分心的预测指标，更具体地说，驾驶员驾驶时是否使用手机(Papadimitriou et al.， 2019)。<br>研究结果显示，<strong>超速驾驶和严重驾驶事件的数量与驾驶时使用手机呈负相关，这表明分心驾驶在没有其他危险驾驶行为的情况下可以被检测到</strong>。</p><p>一些研究人员已经应用了简化的阈值方法，以检测几个异常驾驶事件。在某些情况下，这种方法也被用于识别道路异常，如颠簸和坑洞(Fazeen et al.， 2012;Bose等，2018年)。<br>这种检测方法主要依赖于<strong>应用于加速度计数据的固定阈值</strong>。实验结果表明，<strong>加速度计数据的精度在很大程度上取决于环境特征(车内移动位置、车辆条件、道路类型等)，这构成了检测方法的不灵活性</strong>(Ouyang等。2018)。<br>由于这一事实，很难在识别危险驾驶事件的传感器数据上定义通用的阈值。<br>为了克服这种局限性，研究人员将注意力转向了更有前途的机器学习技术，这些技术通常容易转移，对环境的变化更有活力。</p><p>用于识别驾驶行为的模式识别方法，如<strong>Dynamic Time Warping(DTW)</strong> 也经常出现在文献中。</p><p><strong>DTW</strong>  </p><ul><li>DTW允许分组相似的移动模式，即使在两个系列中对应的元素不是完全一致的。</li><li>首先，对两个给定的时间序列构造网格，然后计算各元素之间的距离。DTW算法估计通过网格的最优路径，使总距离最小。</li></ul><p><strong>DTW的相关研究</strong>  </p><ul><li>lohnson和Trivedi(2011)提出了一种利用DTW检测强势转弯、加速、刹车和变道事件的新系统。</li><li>Engelbrecht et al。(2016)作者DIW用来检测驾驶事件,然后利用启发式、安全等事件分类器进行分类或鲁莽的,该方法的结果与极大似然方法和结果显示,后者表现更好的分类不同的驾驶动作。</li><li>最近的研究使用DTW对横向机动进行分类，利用陀螺仪和重力传感器的融合来获取角速度(Singh等。2017)。</li></ul><p><strong>DTW的优缺点</strong>  </p><ul><li>虽然DTW由于在比较timeseries数据方面的快速和容易实现而被广泛使用，但由于它高度依赖于预定义的阈值，因此不容易转移。</li></ul><h3 id="机器学习和计算智能"><a href="#机器学习和计算智能" class="headerlink" title="机器学习和计算智能"></a>机器学习和计算智能</h3><p>尽管大量的文献使用统计分析方法来调查几种驾驶行为，但在过去的几十年里，机器学习方法已经在这个领域取得了进展。<br>最近的研究彻底研究了用于识别驾驶行为的各种<strong>机器学习技术</strong>(Chan et al.， 2019;Elassad et al . .)<br>本文的目的是讨论与智能手机感知数据相关的最重要的好处和挑战。</p><ul><li><p>Bhoraskar等人(2012)使用了机器学习(ML)技术来识别碰撞和刹车事件，尽管有乐观的结果，作者指出过滤和机器学习技术(k-means聚类和支持向量机-SVM)对于更好地识别恶劣事件的重要性。</p></li><li><p>Hong et al.(2014)采用朴素Baves分类器识别攻击性驾驶风格，准确率达90%。</p></li><li><p>在另一项研究(Saiprasert et al.， 2013)中使用的模式匹配算法在纵向事件和横向事件中都优于基于规则的算法，</p></li><li><p>而在Saiprasert和Pattara-Atikom(2013)中使用了相同的技术来识别异常超速事件。</p></li><li><p>Fazeen等人(2012)利用三轴加速度计来<strong>分析驾驶行为和检测道路异常(坑洼、凹凸不平的道路)</strong>。它们的分类系统具有较高的识别精度，特别是对粗糙或不平坦道路的识别。</p></li><li><p>Saiprasert等人(2017)利用加速度计和陀螺仪数据，采用模糊逻辑和基于规则的算法来<strong>检测驾驶行为，如剧烈加速、刹车或主动转向</strong>。</p></li><li><p>Koh和Kang(2015)研究人员使用带周期图的高斯混合模型(CMM)对驾驶行为从平稳到攻击性行为的梯度，特别关注老年司机，进行分类。由于他们的数据集(指老年驾驶员的行为)的敏感性，他们强调了GMM在准确分类驾驶风格方面的局限性。</p></li></ul><p>这些研究的<strong>主要问题</strong>是<strong>硬件(传感器和智能手机)的多样性</strong>，<strong>数据收集期间的天气条件</strong>，<strong>智能手机的位置(即使它不是固定的)</strong>等，使这些技术难以转移，并提出作为一个通用的解决方案</p><p>还有一些研究者致力于通过实现分类算法来识别驾驶员的状态。</p><ul><li>Yi等人(2019)比较了几种分类算法在识别三种不同驾驶状态(正常、嗜睡和攻击性)方面的表现。结果表明，与其它分类器(k近邻、决策树、支持向量机)相比，随机森林具有更高的总体准确率。</li><li>在Vlahogianni and Barmpounakis (2017a)的研究中，MODLEM算法比其他分类算法在利用智能手机加速度计的数据检测恶劣事件方面取得了最大的精确度。</li><li>Mantouka等人(2019)开发了一种两步k-means聚类算法，最初将主动出行与非主动出行区分开来，然后根据驾驶员的分心和风险承担进一步对出行进行聚类。</li></ul><p>由于异常驾驶行为比正常驾驶行为出现的频率要低，因此建模的<strong>主要挑战</strong>是<strong>处理不平衡数据集</strong>。此外，还<strong>没有明确如何定义ground truth</strong>;因此，不同方法和数据集之间的比较不可能是通用的。</p><ul><li>Meseguer等人(2017)使用神经网络通过速度和加速度测量来识别驾驶员的攻击性程度。</li><li>Eftekhari和Ghatee(2019)评估了两种分类算法(决策树和朴素贝叶斯)的性能，并将其与具有3个隐藏层的神经网络进行了比较。他们的发现表明，<strong>神经网络在检测驾驶动作方面优于其他方法</strong>。然而，由于模型的训练和验证所需的计算能力显著增加，一些任务需要离线执行。</li></ul><p>从不同的运输相关研究尝试中可以看出，ML技术比统计方法具有显著优势(Karlaftis和Vlahogianni, 2011)。<br>随着大量智能手机数据集的可用性，我们可以看到，统计方法可以提供对数据集的第一个洞察，需要更先进的技术，以设计准确和高效的解决方案，以应对不同的挑战。</p><ul><li>在Predic和Stojanovic(2015)中，他们开发了先进的ML分类器来检测恶劣的驾驶模式，并报告了与基于标准差熵、能量、均值等统计指标的加速度计数据活性分析的经典方法相比，改进的结果。<br>此外，如文献所示，研究人员通常在检测到单个事件或将异常驾驶行为与安全驾驶分离时使用统计方法。相反，当驾驶行为的整个范围被调查和许多不同的驾驶剖面被检测时，ML方法被使用。</li><li>Chan et al.(2019)在分类精度方面比较了不同的方法。在这里，我们强调的指标，作为输入驾驶行为识别和补充。提供有关数据收集过程的进一步信息。<br>值得注意的是，在这项工作中，我们避免使用绝对的精度度量来比较不同的方法，因为仍然存在一些挑战，可能导致完全误导的结果。</li></ul><h2 id="Applications-assessing-and-assisting-drivers"><a href="#Applications-assessing-and-assisting-drivers" class="headerlink" title="Applications assessing and assisting drivers"></a>Applications assessing and assisting drivers</h2><p>扩展知识领域的推动分析了研究人员和其他人员的机会开发先进的应用程序的评估驾驶行为以及驾驶辅助系统,旨在提高司机的道路安全性能,提高意识和空气污染,改善驾驶体验。<br>最近的一项研究收集了最相关的驾驶辅助系统，可以在Meiring和Myburgh找到(2015)。<br>这里的重点是利用移动人群感知数据开发此类系统，并使用智能手机作为系统与驾驶员之间唯一的通信平台。</p><h3 id="Drivers’-assistance-and-recommendation-systems"><a href="#Drivers’-assistance-and-recommendation-systems" class="headerlink" title="Drivers’ assistance and recommendation systems"></a>Drivers’ assistance and recommendation systems</h3><p>驾驶行为分析的进步导致了<strong>先进驾驶辅助系统(ADAS)</strong> 的发展。由于ADAS是一种创新的、用户友好的技术，能够提供实时驾驶提示，满足安全和生态标准，因此越来越受到人们的广泛关注(Kaur和Sobti, 2017)。<br>一些最广泛使用的ADAS包括自适应巡航控制、导航辅助和改道系统、车道保持辅助和驾驶员困倦检测。这类系统的开发主要是为了提高道路安全性，其次是为了提高旅行舒适度和驾驶体验。<br>驾驶分析技术的进步，加上当今设备无线性能的提高，使得实时ADAS的开发成为可能。具体的研究集中在实时驾驶员分心检测上，该检测主要基于车道位置和转向控制等驾驶性能测量以及速度控制测量(Liu et al.， 2016)。</p><p>虽然大多数ADAS专注于确保道路安全,减少交通事故,在情况下驾驶体验改善或提升生态驾驶的问题,研究人员已经开发出司机推荐系统与ADAS的广泛应用,研究驾驶推荐系统仍处于初级阶段。<br>Magana和Organero(2011)的研究人员开发了一个生态驾驶推荐系统，该系统首先检测驾驶风格，然后使用随机森林对驾驶行为进行分类，并提供有用的生态驾驶提示。<br>Araujo等人(2012)已研发环保驾驶教练，以推广省油高效驾驶。首先，他们识别驾驶行为和车辆状况，然后，他们推荐一个最受欢迎的环保驾驶技巧，如“关闭引擎”，“早些换挡”，“你的加速太高了”。<br>另一项研究开发了一种情境感知驾驶辅助系统，旨在促进节油驾驶(Gilman等。2014年)，研究人员识别攻击性驾驶行为，然后将其与交通和天气状况一起放在地图上，以调查特定的驾驶模式。通过这种方式，他们能够就如何更有效率地行驶到特定的路线提出建议，然后就如何在未来提高他们的驾驶性能提出建议。<br>这类系统已被证明可以改善驾驶行为，并引导人们采用更平稳、更安全的驾驶习惯。<br>正如Staubach等人(2014)所讨论的那样，接受生态驾驶建议的驾驶员会采取一种不那么苛刻的机动行为，并保持恒定的速度。<br>另一项研究强调了在这类系统中<strong>引入游戏化</strong>方面的重要性，以获得驾驶员对系统的参与，并进一步改善他们的行为(Magana和Organero, 2014)。<br>游戏化的概念是指将娱乐和游戏导向的设计方法包含在非游戏环境中，如移动众感应用(Wells et al.， 2014)。<br>一个有效的游戏化框架是建立在一系列量化既定目标(改善驾驶行为、采用环保驾驶习惯、降低油耗等)成功程度的指标之上的，而这些目标是通过一系列任务来实现的。</p><h3 id="Drivers’-scoring-systems"><a href="#Drivers’-scoring-systems" class="headerlink" title="Drivers’ scoring systems"></a>Drivers’ scoring systems</h3><p>在过去的几十年里，也有一个发展<strong>计分和行为评估系统</strong>的趋势。<br>研究人员和应用程序开发人员试图为人们之间的良性竞争和比较创造环境，以提高他们对重大问题的意识，认为后者将改善他们的行为。<br>在这种情况下，评分方法以及排名和基准测试技术的创造得到了加强，如前所述，得分和通过完成特定任务获得名次是最常见的游戏化方面，与排行榜、成就相比、礼物等。在游戏化框架下，用户还可以与他人进行交流。比较他们的表现和竞争(Vlahogianni and Barmpounakis, 2017b)。</p><ul><li>Smith等人(2016)开发了一个系统的驾驶行为评分框架，该框架主要包括三个方面:车队管理应用的风险评分、操作评分和经济评分。</li><li>DriveSafe是一款检测司机何时从驾驶任务中分心并根据驾驶行为给其打分的应用程序(Bergasa等人，2014)。该系统首先识别加速、刹车和转弯以及车道漂移和转弯，然后根据比赛的数量和强度给每位车手打分。</li><li>Castignani等人(2015)基于模糊推理系统使用数据融合来检测危险和攻击性驾驶事件。随后，每次行程中确定的驾驶事件数与天气条件和行程当天的时间相结合，以提供从0到100的行程分数。</li><li>Araujo等人(2012)开发了一种驾驶员评价系统，以提高驾驶员对生态驾驶的认识。为此，智能手机传感器采集速度、油耗和加速度测量值，然后根据预定义的阈值对驾驶条件进行分类，然后使用模糊逻辑方法评估驾驶提示信息的性能并选择最合适的显示给驾驶员。</li></ul><p>虽然计分系统被认为是提高驾驶员意识的有效方法，但一些研究人员<strong>强调了获得真正的节约作为改善个人驾驶行为的激励的重要性</strong>。</p><ul><li>保险收费系统在过去十年中得到了越来越多的关注。保险公司已经建立了多种保险政策，根据车辆使用情况和驾驶行为特征向司机收费，包括PAYD和PHYD系统(Wahlstrom et al, 2017)。</li><li>Tselentis等人(2017)报道了最流行的基于使用的保险(UBI)方案。正如他们所强调的，有证据表明UBI的实施将会激励驾驶员改善他们的驾驶行为，并通过采用安全和更有效的驾驶习惯来改变他们的行为。</li><li>Handel等人(2014)也广泛讨论了基于智能手机传感器收集的驾驶特征来构建保险计划的机会。后者强调了有意识地设计UBI的重要性，因为也存在提供的反馈或建议被错误地感知的风险。</li></ul><h2 id="Main-challenges-and-possible-caveats-警告"><a href="#Main-challenges-and-possible-caveats-警告" class="headerlink" title="Main challenges and possible caveats(警告)"></a>Main challenges and possible caveats(警告)</h2><p>从所提出的方法框架的详细概述，可以清楚地看出驾驶行为分析是一个<strong>多维的问题</strong>。<br>在本节中，我们强调了通过智能手机感知数据理解驾驶行为过程中<strong>最关键的挑战</strong>。<br>我们还讨论了一些大多数时候被忽视的警告。表2提供了一些针对文献中提出的挑战的指示性解决方案。</p><p><img src= "/img/loading.gif" data-lazy-src="/image/survey/t002.png" alt=""><br><img src = "/image/survey/t002.png" width = 80%/></p><table style='border-collapse:collapse'> <tr><td>挑战</td><td>提出的方法</td><td>选择的引用</td></tr><tr><td>通过用户参与增强数据的代表性</td><td><ul><li>激励</li><li>游戏化方面</li><li>说服大众理解驾驶行为在交通和道路安全中的有用性和重要性</li></ul></td><td>(Yen et al.,2019年)</td></tr><tr><td>确保数据的可用性和质量</td><td><ul><li>利用低功率无线网络</li><li>当Wi-Fi连接可用时上传数据</li><li>在多个系统之间共享传感数据</li><li>改变采样率</li><li>匿名,使用假名,空间隐身</li><li>数据扰动和聚合</li><li>特征选择(过滤器、包装方法)</li><li>异常检测技术</li></ul></td><td>(Christin,2016;Etemad et al.,2018;Wang et al.,2018)</td></tr><tr><td>从数据中识别上下文</td><td><ul><li>数据融合</li><li>过滤算法</li><li>工程特性</li></ul></td><td>(Wahlstrom et al.,2015b)</td></tr><tr><td>检测异常模式和危险司机</td><td><ul><li>机器学习方法</li><li>确定每个特征的通用阈值</li></ul></td><td>(Bejani and Ghatee,2018)</td></tr><tr><td>建模效率，迁移学习和可解释性</td><td><ul><li>应用重采样技术</li><li>生成合成样品</li><li>转移学习</li><li>附加特征</li><li>大数据分析，而不是小实验数据集</li><li>异常值检测</li></ul></td><td>(Hu et al.,2018;Maldonado and Lopez,2018;Roy et al.,2018)</td></tr><tr><td>提高意识，改变态度</td><td><ul><li>加入UBI计划</li><li>游戏化</li></ul></td><td>(Tselentics et al.,2017;Vlahogianni and Barmpounakis,2017b)</td></tr><tr><td>实时操作</td><td><ul><li>人工智能</li><li>优先任务</li><li>高效的内存管理</li></ul></td><td>(Shukla et al.,2018)</td></tr></table><h3 id="Challenge-1-Enhancing-data-representativeness-through-user-engagement-通过用户参与增强数据的代表性"><a href="#Challenge-1-Enhancing-data-representativeness-through-user-engagement-通过用户参与增强数据的代表性" class="headerlink" title="Challenge 1: Enhancing data representativeness through user engagement 通过用户参与增强数据的代表性"></a>Challenge 1: Enhancing data representativeness through user engagement 通过用户参与增强数据的代表性</h3><p>在移动众感系统中，用户本身是最基本的组成部分之一，因此，用户与系统的互动至关重要。大多数研究旨在识别通用的驾驶行为并确定控制不安全驾驶行为的一般规则<br>由于驾驶行为的识别依赖于数据驱动的方法，因此数据的可变性非常重要。实际参与数据收集过程的目标人群的比例针对性地影响数据的代表性，也不可避免地影响数据的质量。<br>此外，用户的长期参与也是至关重要的，因为人类的行为与时间有关的复杂性，以及它随着时间和各种刺激的缓慢或不变化。<br>在文献中，有几种技术被确定为确保用户长期参与众感系统的成功干预手段。许多研究人员强调了为用户提供奖励以长期参与众感系统的重要性(Yen et al.， 2019)。<br>在Musicant和Lotan(2016)中，研究人员指出了群体激励在激励年轻司机方面的有效性。虽然最近已经部署了几种类型的激励，但它们能在多大程度上提高人们的意识，并驱动行为改变，还有待研究。</p><p>此外，可用数据集中的行为可变性通常与模型泛化能力的重大影响被忽略。<br>研究人员倾向于找到大型数据集作为锻炼和发展他们的机器学习的实验技能,将意识到他们的模型将很快成为过时(速降精度指标),随着数据的增长,主要是由于这一事实的初始样本不能代表用户的特征。<br>但是，即使我们试图控制样本的统计特征，现实也会再次让我们吃惊:</p><ol><li>首先，由于隐私的限制，大多数时候研究人员无法获得样本特征(比如年龄、性别)。</li><li>其次，收集和处理一个大数据集需要花费大量的时间，特别是如果是为了确保特定用户和行为的代表性。</li></ol><p><span class="inline-tag red"><strong>通过引入不断培训的过程和处理模型对变化的弹性的过程</strong></span>，可以有效地解决上述问题。</p><h3 id="Challenge-2-Ensuring-for-data-availability-and-quality"><a href="#Challenge-2-Ensuring-for-data-availability-and-quality" class="headerlink" title="Challenge 2: Ensuring for data availability and quality"></a>Challenge 2: Ensuring for data availability and quality</h3><p><span class="inline-tag red"><strong>数据可用性和质量</strong></span>是交通文献中每一种数据驱动方法的基本成分。<br>由于几个原因，可用性不能得到保证。</p><ol><li>首先，<strong>数据可能是私有的或限制访问</strong>。最近，在欧盟《一般数据保护条例》(GDPR)的严格规定下，从人群中感知数据变得更加具有挑战性，特别是在数据隐私和安全方面。保护系统不受未授权方侵犯的技术是主要需求。到目前为止，研究人员似乎忽视了制定确保数据隐私策略的重要性和意义。</li><li>此外，<strong>可能无法获得所需分辨率的数据</strong>。事实上，某些驾驶现象(例如，智能手机的干扰，变道)可能需要非常详细的记录，而其他的可以在较粗糙的水平上很容易观察到。与开发的应用程序相关，识别正确的数据分辨率在文献中被严重忽视，并显著影响检测能力和对驾驶任务的理解。但是，即使有人选择获得智能手机传感器的大部分功能(通常是100赫兹的分辨率)，这也可能导致由于电池耗尽数据采集方案不可持续。此外，车辆受驾驶环境、交通条件等不同因素的影响，一些与变道现象或变道选择相关的任务也变得更加具有挑战性。车辆的类型等(Barmpounakis等。(2020)，虽然提高采样率对于提高模型的预测能力是可取的，但这可能会对用户体验产生负面影响，因为设备资源的快速消耗可能会阻碍用户对系统的贡献在以前的挑战</li></ol><p>由于<strong>智能手机设备的不确定性</strong>，数据质量可能会对智能手机感知数据的理解产生深远的影响:</p><ul><li>是多种技术</li><li>可以放置在任何地方的用户或车辆</li><li>配备可以以不同频率异步记录的传感器。</li></ul><p><strong>数据重定向</strong>和<strong>同步策略</strong>通常应用于正确的数据，并将其置于准备建模的表单中，这需要很大的努力，但也不能确保不会出现故障和干扰。<br>智能手机众感框架中的质量保证方案对于驾驶分析尤为必要，因为<strong>现象检测依赖于极值</strong>，通常包括<strong>噪声过滤</strong>、<strong>数据重定向</strong>和<strong>地理标记</strong>(Vlahogianni and Barmpounakis, 2017a)。</p><h3 id="Challenge-3-Identify-the-context-from-the-data"><a href="#Challenge-3-Identify-the-context-from-the-data" class="headerlink" title="Challenge 3: Identify the context from the data"></a>Challenge 3: Identify the context from the data</h3><p>用户行为受<strong>交通方式</strong>、<strong>路网类型</strong>、<strong>交通控制</strong>、<strong>天气条件</strong>等因素的显著影响。<br>为了检测关键模式以及理解和建模行为特征，应该通过联合考虑其他外部信息，从数据本身中提取数据的上下文。<br>处理从数据中提取上下文的问题对模型弹性有重要的附加值，因为从原始数据中提取的特征的质量主要反映了系统的整体准确性(Ignatov, 2018)。<br>但是，我们应该如何通过用户不可知的实验和盲目的众感系统来理解上下文呢?</p><p>在用户不可知的环境中，从多感官数据派生上下文的主要工具是:</p><ol><li><strong>数据融合</strong></li><li><strong>特征工程</strong></li><li><strong>用户/行为分析</strong></li></ol><p>当涉及到确保所有可用信息都能被综合考虑时，<strong>数据融合</strong>是很重要的;</p><ul><li>可以将带有智能手机传感功能的天气数据和社交媒体信息融合，以提取由于天气变化或不同旅行目的而产生的移动模式变化。<br><strong>特征工程</strong>能够识别那些对上下文感知至关重要的特征。</li><li>Mantouka等人(2019)对加速度计信号进行处理，提取每公里剧烈加速和每公里剧烈刹车等特征，然后用于驾驶行为检测。</li><li>另一个通常首先被发现的特征是<strong>旅行模式 travel mode</strong>(Nikolic和Bierlaire, 2017: Efthymiou等人…2019)。<ul><li>首先采用模态检测技术，</li><li>然后根据检测到的模态对数据进行清洗，因为不同的动力学导致不同模态间的噪声不一致。</li><li>显然，更多可用的数据源意味着更强的识别关键模式的能力，从而更清楚地了解数据背后的上下文。</li></ul></li></ul><p>现有众感系统之间的<strong>互操作性</strong>和<strong>透明性</strong>对于数据重用和迁移学习非常重要。</p><h3 id="Challenge-4-Detect-abnorml-patterns-and-risky-drivers"><a href="#Challenge-4-Detect-abnorml-patterns-and-risky-drivers" class="headerlink" title="Challenge 4: Detect abnorml patterns and risky drivers"></a>Challenge 4: Detect abnorml patterns and risky drivers</h3><p>大多数研究遵循的一个共同的研究路线是<strong>基于阈值的检测偏离正常驾驶的方法</strong>。<br>矛盾修饰法在于，许多研究得出的结论是，没有通用的阈值可以应用，因为智能手机传感器的技术变异性影响提取的信号，当然也影响被认为是异常的阈值。<br>事实上，<strong>基于阈值的方法是检测异常驾驶模式的第一步</strong>，特别是在没有先验知识存在的系统中(例如，可用于监督学习的注释样本)。<br>但是，<strong>需要一种与设备无关的上下文无关的方法来设置阈值</strong>，以确保后者不受设备微移动、传感器特性和设备方向和定位的影响。甚至考虑一个非阈值方法也将有利于建立一个通用的驱动行为分析框架.</p><p>在极端行为检测中，另一个通常被忽视的方面是<strong>如何从基于旅行的特征归纳到用户特定的行为</strong>，这意味着需要<strong>监视司机多长时间才能理解他的行为</strong>。这在宏观分析层面上是非常关键的，系统应该向用户提供建议，让用户如何改进自己的驾驶风格，成为更安全、更高效的驾驶员。<br>要对一个人进行多长时间的监控，才能观察到他的驾驶特征的收敛性，这个问题的答案不是单方面的。它取决于用户对道路的感知和态度，也取决于道路、交通和控制条件。<br>最近的一项研究试图确定检测单一驾驶特征(如攻击性)时的基本数据量(Stavrakaki等)。然而，文献中却缺少一种方法框架来估计每个驾驶员需要收集多少驾驶数据，从而对其整体驾驶行为有一个清晰的了解。</p><h3 id="Challenge-5-Modeling-efficiency-transfer-learning-and-explainability"><a href="#Challenge-5-Modeling-efficiency-transfer-learning-and-explainability" class="headerlink" title="Challenge 5: Modeling efficiency, transfer learning and explainability"></a>Challenge 5: Modeling efficiency, transfer learning and explainability</h3><p>有效的模型是既准确又不产生系统误差的模型。在使用数据驱动技术处理的高度不稳定的问题中，获得精确性，但同时注意过度训练是最重要的(Karlaftis and Vlahogianni, 2011)。<br>对于智能手机感知数据集通常是不平衡的(数据集中很少出现异常驾驶甚至事故)，有效模型的开发是一个繁琐的过程。<br>当研究人员转向<strong>重采样技术</strong>或<strong>生成合成数据集</strong>时，当涉及到哪种技术适合每个问题时，不同的挑战就出现了。<br>例如，是否对数据集进行重新采样以使两个类都得到相同的表示，或者是否一个类与另一个类比例过大，如果是，大多少?<br>虽然这些问题已经被部分的文献解决，丰富现有的数据集更多的特征也可以提供显著的改进来应对这一挑战。</p><p>鉴于极端行为缺乏代表性，<strong>迁移学习</strong>的使用似乎也是一条合适的途径。迁移学习和领域适应指的是利用在一种环境中学习到的知识来提高在另一种环境中泛化的情况(Mairai等，2019)。简单地说，迁移学习利用基于另一个问题开发的知识能力，促进对手头问题的学习过程。<br>ML模型，特别是深度学习模型的主要局限性是训练过程耗时。即使使用具有强大计算能力的硬件，在存在大数据的情况下，训练阶段也不能被忽视，尤其对于实时应用来说，这是一个未来的挑战。<br>迁移学习可以很好地解决ML模型训练费时的问题，而ML模型可以很容易地解决一些数据限制，如嘈杂的数据和不平衡的数据集。</p><p>然而，建立准确的模型不应该是唯一的问题。大多数研究人员经常通过比较完全基于模型精度的方法来遵循最小阻力的路径;然而，在建模精度、模型简单性、适用性和可用性之间有一条“细线”。<br>研究人员应该记住，产生的模型应该是<strong>可操作的</strong>，这意味着操作和维护简单，准确到足以产生可靠的结果，并容易集成到复杂的系统。<br>此外，在强调模型的解释力的情况下，建模应该<strong>清楚地解决因果关系</strong>的问题。众所周知，<strong>相关性并不意味着因果关系</strong>。<br>通过ML建模来解决因果关系——不管是用于函数逼近、模式识别还是时间序列分析——都不是一个简单的过程，可以使用几种统计结构来解决这个问题(Karlaftis和Vlahogianni，2011;Lavrenz等人，2018年）</p><h3 id="Challenge-6-Raising-awareness-and-changing-attitudes"><a href="#Challenge-6-Raising-awareness-and-changing-attitudes" class="headerlink" title="Challenge 6: Raising awareness and changing attitudes"></a>Challenge 6: Raising awareness and changing attitudes</h3><p>虽然道路安全是研究人员、从业者甚至司机自己都关心的问题，但并不是每个人都意识到驾驶行为与道路安全水平之间的关系。<br>重要的是让司机意识到他们驾驶行为的影响。冒险性、攻击性和驾驶任务的分心是驾驶员发生交通事故的主要原因(Hong et al.， 2014)。<br>ADAS的目的是在驾驶过程中支持司机，防止这种不安全行为。另一个日益增长的趋势是交通安全和可持续性的结合。<br>因此，如前所述，今天的驾驶建议和辅助系统的目标是双重的:<strong>促进环保驾驶，同时确保道路安全</strong>。</p><ol><li>在<strong>环保驾驶</strong>方面，改变驾驶态度似乎更容易，因为这种驾驶方式与金钱成本和节省燃料有关。</li><li>对于<strong>安全驾驶</strong>来说，改变人们不安全的驾驶习惯似乎更具挑战性，尤其是因为人们往往会忽视自己驾驶行为的影响。<br>在这种情况下，也应该给予强有力的激励，因为最终目标不仅是激励司机参与一项事业，而且还提高了他们的意识。</li></ol><p>另外一个值得关注的问题是，驾驶推荐系统是以驾驶员与系统持续合作为前提的，在这样的系统中，驾驶员不断地收到提高驾驶效率甚至驾驶体验的驾驶提示和建议。<br>如果驾驶员不打算接受系统提供的建议，那么系统就失去了它的潜力。<br>为此，提高驾驶员对其驾驶习惯影响的认识，并确保他们长期参与该系统是至关重要的。<br>大多数研究人员关注教育和培训的重要性，认为教育和培训是改变驾驶员态度的关键(赵等人，2019)，<br>然而，最近的趋势要求<strong>对驾驶行为进行持续监控</strong>，即使不是实时提供建议和提示，也要持续不断。</p><h3 id="Challenge-7-Real-time-operation"><a href="#Challenge-7-Real-time-operation" class="headerlink" title="Challenge 7: Real-time operation"></a>Challenge 7: Real-time operation</h3><p>直到最近，一小部分文献已经开发出能够实时运行的驾驶行为检测和推荐算法。<br>然而，由于需要通过对交通的管理来确保当今运输系统的恢复力，道路事故和排放已经出现。<br>保证驾驶推荐系统的实时运行是迫切需要的。<br>如前所述，驾驶行为不仅是道路安全的主要贡献者，而且是道路网络状况的主要贡献者，因此，只有开发出易于解释和响应的推荐系统，才能有可能改善驾驶行为。<br>考虑到人类行为的复杂性以及前面描述的方法上的挑战，未来的研究应该在开发能够<strong>实时运行的推荐系统</strong>上投入大量精力，特别是如果它们能够<strong>提供个性化的建议和提示</strong>的话。<br>先进的计算智能技术有望取得良好的结果，并有效地应对用户定制的实时推荐系统的挑战性任务。<br>此外，这项任务在联网车辆(CVs)和合作智能交通系统(c-ITS)的新兴环境中至关重要，因为与昂贵的嵌入式相比，智能手机可以在车辆和基础设施之间提供一种成本效益高的通信解决方案。<br>CVs的通信解决方案，特别是对于转型时期的老车，老车和联网车辆将共享相同的驾驶环境和基础设施(Su等。2012)。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>智能手机众感不是一个新兴领域;它作为收集数据的可靠选择正在不断上升。<br>具体工作试图通过建立一个方法论的框架分析现有文献对驾驶行为的识别和理解,包括四个阶段:<strong>数据收集</strong>、<strong>数据准备</strong>、<strong>数据挖掘</strong>、<strong>驾驶行为建模</strong>和最后,有价值的信息开发决策和建议制定协助和提高驾驶行为。</p><p>分析指出，在依赖基于智能手机的数据和模型时，应该考虑一些普遍存在的挑战。<br>这些涉及到<strong>数据的可用性和质量</strong>、<strong>代表性</strong>、<strong>基于上下文的知识提取</strong>、<strong>模式识别</strong>和<strong>建模</strong>,<strong>对行为改变的建议</strong>,以及<strong>影响评估和实时操作</strong>。<br>讨论现有的挑战和可能的警告表明,基准模型和数据处理程序可以提供有用的信息是什么类型的建模适合于不同的应用程序。<br>为此，对于确保基于智能手机的数据收集和建模系统的可转移性和可持续性，模型对上下文规范和数据变化的弹性的新度量非常重要。<br>在数据挖掘和建模方面，<strong>人工智能和迁移学习</strong>的使用可以提高模型的准确性和泛化能力，同时也保证了模型的灵活性和实时性。<br>此外，在没有大数据的情况下，现有众感系统之间的<strong>互操作性和透明性</strong>对于数据重用和迁移学习非常重要。</p><p>驾驶行为分析的进步使得驾驶推荐系统能够提供安全驾驶或生态驾驶技术和提示。<br>尽管有很多证据表明，推荐系统和ADAS有潜力改善交通状况、道路安全和驾驶体验，以充分发挥其潜力，但驾驶员对该系统的接受和长期参与是先决条件。<br>下一步是设计一种影响评估工具，以衡量这些系统对用户的感知、选择以及对运输系统的影响。<br>显然，人们对快速、准确的检测模型以及对司机的实时反馈和帮助有着很高的要求。<br>将最终推动人工智能的部署，以确保从道路上车辆产生的大量数据中，可以出现用户不可知的推荐方案，帮助用户从改进驾驶转向改进移动习惯。</p><table><thead><tr><th>一个普通标题</th><th>一个普通标题</th><th>一个普通标题</th></tr></thead><tbody><tr><td>短文本</td><td>中等文本</td><td>稍微长一点的文本</td></tr><tr><td>稍微长一点的文本</td><td>短文本</td><td>中等文本</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Learning-Based Driving Events Recognition and Its Application to Digital Roads</title>
      <link href="/posts/600002/"/>
      <url>/posts/600002/</url>
      
        <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h2><p>A.<br>如图1所示，本文首先提出了一种基于学习的分类方法，利用小尺度驾驶模式从真实驾驶数据中识别驾驶事件。这些模式包含在几秒钟的驾驶数据中，通常与驾驶动作相对应，从而帮助描述驾驶员的行为。其次，本文将所学的模型应用于数字道路，即将道路的数值描述作为卡车驾驶模拟器的输入。</p><p>本文的贡献有四方面:</p><ul><li>影响驾驶员行为的道路事件的识别和分类</li><li>提出并分析了一套对道路事件敏感的小型驾驶模式相关特征</li><li>使用两种机器学习技术，即决策树和线性逻辑回归，自动识别驾驶事件与真实驾驶数据。此外，驾驶事件 与真实驾驶数据，并且驾驶事件的识别还包括主要识别特征的选择</li><li>在实际驾驶数据和数字道路上进行了模型实验。后者为模拟驾驶数据。</li></ul><p>首先，除了[6]中研究的节流空驱动事件之外，我们考虑更完整的驾驶事件集，包含特别是油门非空事件，即</p><ul><li>driving at a speed limit</li><li>reach a higher speed limit</li><li>urban take-off</li><li>main road or highway take-off</li><li>adjust for stop</li><li>adjust for other reasons</li></ul><p>其次，还引入了新的驾驶特征，例如与油门踏板相关的特征，以进一步对油门= 0和油门&gt;驾驶事件进行分类。最后，我们也给出了该工作的最终应用，即在数字道路上的仿真。这是一个重大的挑战，因为数字道路包含的驾驶信息(例如驱动器)比真实驾驶数据要少，从而使驾驶事件的识别复杂化。</p><h2 id="RELATED-WORK"><a href="#RELATED-WORK" class="headerlink" title="RELATED WORK"></a>RELATED WORK</h2><p>A. 驾驶事件</p><p>Johnson等人提出了一种用于攻击性驾驶[3]智能识别的移动传感器平台。这个平台旨在区分典型的(非侵略性的)驾驶风格和侵略性的驾驶风格，考虑到驾驶员的安全，并提出了8个驾驶事件的自动识别。<br>等。该平台使用手机中各种传感器收集的数据，如后置摄像头、加速度计、陀螺仪和GPS。同样，Eren等人[8]和Paefgen等人。[9]还使用从智能手机传感器收集的驾驶数据来进行驾驶行为分析，例如[8]中的安全驾驶或危险驾驶，以及[9]中的关键事件。另外，Karaduman等人依靠CAN总线数据进行主动/平静驾驶检测[10]。</p><p>Van Ly等人确定了三种不同的驾驶事件，即加速、刹车和转弯[5]。他们提出利用这些驾驶事件的特征向量来区分两名驾驶员，以提供反馈并减少危险的汽车机动次数。Satzoda等人对使用自然驾驶研究(NDS)进行自动驾驶分析[2]的可能性进行了更一般的多模态研究。他们确定了一些驾驶事件，主要集中在横向机动，例如，右/左变道。左/右曲线。左/右转。等。</p><p>这些研究都一致表明，自动识别驾驶事件是可能的。如表I所示。驾驶事件大多限于转弯、加速和刹车事件。此外，驾驶事件的定义因工作的不同而差异很大，这取决于底层应用程序。在我们的工作中，我们关注的是突出卡车司机如何控制车辆的燃料消耗和平均速度问题的驾驶事件。因此，我们特别关注纵向努力，并定义驾驶事件，这与目前最先进的一个有很大不同。事件在第III- b节和表III和IV介绍。与其他研究相比，我们详细说明导致加速或刹车的不同情况。然而，相关研究提出的特征可以用于自动识别我们的驾驶事件。</p><p>B. 驾驶特征</p><p>在文献中，驾驶特征的提取有多种目的，包括上述驾驶事件:道路类型识别[14]，驾驶员分类[15]，油耗估计[16]。我们这里的目的是识别驾驶事件。因此，我们需要提取驾驶模式，即将所有驾驶数据分割成一系列驾驶事件。Engstrom将驾驶模式分为三类，即大规模、小规模和实时规模的驾驶模式[14]。</p><p>大规模的驾驶模式，例如5分钟的速度数据，是持续几分钟的长期驾驶数据，使分析成为可能，而不是全球驾驶事件或。行为。这些模式通常用于确定驾驶环境，如道路类型[14]，识别驾驶员[15]和估计油耗[16]。Ericsson列举了一组driving characteristics来研究[17]对燃料利用率和尾气排放因子的影响。这些驱动参数主要涉及速度、加速、发动机和变速箱特性。虽然这些特征在他们的研究中都是有趣的，以燃料利用为目的，但他们并不都适应我们的驾驶事件识别过程。例如，引擎相关的特性，例如，在发动机转速&lt;1500 rpm时%的时间，是特定于汽车特性的，而我们的目标是识别驾驶事件独立于卡车发动机的物理特性。</p><p>小规模驾驶模式通常包括几秒钟的驾驶数据，因此允许提取在驾驶机动过程中[2]，[3]，[5]，[11，[18]]的重要特征，例如，变道，环形车道制动。</p><p>最后，最后一种模式是实时数据，如眼睛或头部位置，用于分类车辆机动。这类数据持续时间的数量级约为几毫秒。</p><p>在本文中，我们的目标是识别典型持续数秒的驾驶事件，从而使用短尺度驾驶模式。我们关注的是小型最先进的驾驶功能，综合在表II中。从这个表格可以看出，我们把这些驾驶特性分成了8个不同的组。第一组与各种车辆速度统计有关，而第二组与车辆加速度有关。第三组由值得注意的子部分组成，它们描述一个精确的部分驾驶数据，并计算不同比例的驾驶时间，例如，停顿时间，匀速等。第四组被称为“独立”，由不依赖于车辆的特征组成，如驾驶时间和每公里停车次数。第五小组。主要是对能量的分析。第六组是关于发动机转速和换挡的各种参数，下一组是关于横向行驶的信息，即横向和摆幅加速度或颠簸。最后一组是关于驾驶员控制车辆的执行器。</p><p>这些特性已被用于各种目的，如Canale和Malan[18]提出的驾驶员分类。他们通过对驾驶员行为的统计分析，即车辆的平均值和标准差，以及速度、坡度等环境数据来研究驾驶员的风格。然而，他们的研究仅限于城市路段的停车和行走阶段，他们的目标是分类驾驶员的风格，而不是驾驶事件。类似的工作是计算加速度和/或曲率数据的统计特征，这些数据是用加速度计、陀螺仪或GPS[2]、[3]、[5]、[11]获得的。在我们的工作中，我们专注于纵向动力特性，因为这项工作的主要目标之一是不包括横向努力的模拟过程。</p><h2 id="DRIVING-DATA-AND-DEFINITION-OF-DRIVING-EVENTS"><a href="#DRIVING-DATA-AND-DEFINITION-OF-DRIVING-EVENTS" class="headerlink" title="DRIVING DATA AND DEFINITION OF DRIVING EVENTS"></a>DRIVING DATA AND DEFINITION OF DRIVING EVENTS</h2><p>基于学习的驾驶事件识别需要具有代表性的训练数据进行训练和验证。在本节中，我们将介绍所收集的驾驶数据和我们所关心的驾驶事件。</p><p>A. 数据收集<br>当我们开始我们的研究时，我们没有合适的驾驶数据来学习。因此，我们启动了一个数据采集过程，旨在涵盖11名不同司机的尽可能多的驾驶情况。这里的一个关键标准是招募足够多的不同驾驶经验的卡车司机，从而在同一驾驶事件中描述不同的行为。我们选择了一批具有代表性的车辆和道路分布的区域利用率。这些公路位于城市和城市以外地区，包括一些高速公路。所有数据均在法国里昂地区采集。一辆装载13.7吨的沃尔沃FL车辆配备了三个数据采集设备。<br>这些设备是一个CAN数据记录器，记录CAN总线信号，如车辆速度，“油门踏板百分比，等，一个GPS车辆重力定位，和摄像机记录拍摄外部场景。只能使用数据和GPS数据进行分类。摄像机的记录有助于对事件进行标记，也有助于理解事件期间公路上实际发生了什么，从而解释分类结果。实际上，采集到的原始驾驶数据还需要进一步手工后处理，以分割各种驾驶事件，作为后期学习阶段的地面真实数据。然而，由于没有显示交通环境的图像，这种人工分割是非常复杂的。因此，我们实现了aspecific注释工具，以同步的方式显示交通环境和驾驶数据的视频。<br>在数据采集过程中，要求驾驶员正常驾驶，但不使用巡航控制，并始终保持自动变速箱处于自动模式。我们制定这些规则是为了区分驾驶员和车辆的影响。由于驾驶事件的部分分析是通过执行器(见章节II1-B)，我很重要的是，司机可以控制车辆的速度仅与执行器，而不是使用巡航控制。出于同样的原因，我们不希望司机控制变速箱，所以我们要求他保持自动模式。</p><p>B. 驾驶事件</p><p>根据本研究的目的，有许多定义驾驶事件的方法。这项工作的目标是了解驾驶员的行为。因此，我们所关注的驾驶事件中，驾驶员可以通过使用不同的执行器来显示不同的行为。为了控制车辆的速度，驾驶员使用三种不同的执行器，即油门踏板(Throttle pedal)、刹车踏板(brake pedal)和减速踏板(retarders)。油门踏板和刹车踏板是大家都知道的踏板，分别用于加速和刹车。缓速器是辅助主要摩擦制动系统功能的设备。当驾驶员没有使用上述三个驱动器时，就会发生滑行(Coasting)。</p><p>本文根据这些驱动器的使用情况对驾驶事件进行了划分，得出了两种不同的结果。<br>第一种分类包括 Throttle &gt; 0 事件，即车手每次开始使用油门踏板时启动。<br>第二类的Throttle = 0事件时，驾驶员使用刹车和/或减速器。<br>图三显示了两种不同事件类型的例子<br><img src= "/img/loading.gif" data-lazy-src="/image/600002/g003.png" alt=""></p><p>直接应用前面的定义会导致许多远远超出实际情况的驾驶事件。因此，又定义了4条关于司机如何根据道路情况控制车辆的规则:</p><ul><li>当车没有移动(速度等于0km/h),明显没有事件</li><li>如果两个Throttle &gt; 0 事件以滑行周期和他们之间少于3km/h的速度间距所分割，这两个事件被合并。我们在采集的驾驶数据中观察到，当驾驶员在驾驶过程中以匀速行驶为目标时，他们并没有保持准确的目标速度，而是在目标值附近的3公里/小时范围内振荡几公里/小时。因此两个相邻的Throttle &gt; 0 事件速度差距小于3公里/小时的项目必须视为同一项目。相反，如果司机让他的车在滑行过程中显著地增加或减少速度，这意味着他想要改变他的目标速度。因此，这应该被解释为一个新的事件。</li><li>如果两个或两个以上的Throttle = 0事件距离太近，则合并为单个事件(即两个相邻事件在delta t&lt; 5s时，此期间发动机转矩&lt;15 N.m,油门位置百分比也必须小于5%)，这是由于缺乏低油门位置百分比的影响，以及踏板图配置导致的发动机扭矩较低。</li><li>如果两个相邻Throttle = 0事件的滑行阶段重叠，这些事件也会合并为单个事件。事实上，如果两个Throttle = 0事件之间没有发生任何事情，我们可以认为车手在同一事件中仍在控制他的车辆，而不是在一个新的事件中。</li></ul><p>表IIl和表IV展示了在数据收集过程中观察到的所有驾驶事件。我们还与专业司机一起起草了一份所有驾驶情况的详细清单。这表明，在我们的数据采集期间确定的以前的驾驶事件涵盖了主要和最常见的驾驶情况。<br><img src= "/img/loading.gif" data-lazy-src="/image/600002/t003.png" alt=""><br><img src= "/img/loading.gif" data-lazy-src="/image/600002/t004.png" alt=""></p><h2 id="SELECTION-OF-DRIVING-EVENTS-SENSITIVE-REATURES"><a href="#SELECTION-OF-DRIVING-EVENTS-SENSITIVE-REATURES" class="headerlink" title="SELECTION OF DRIVING EVENTS SENSITIVE REATURES"></a>SELECTION OF DRIVING EVENTS SENSITIVE REATURES</h2><p>一旦所有的驾驶事件都被识别出来并被人工标注在所获得的数据上，我们就必须提取对这些驾驶事件敏感的有意义的特征，为机器学习的后一个阶段做准备。</p><p>A. 驾驶事件敏感特征</p><p><img src= "/img/loading.gif" data-lazy-src="/image/600002/t005.png" alt=""></p><p>如表V所示，我们收集了一组潜在敏感的驱动事件特性并将其划分为三组。这三组是由影响车辆速度和油耗的主要部件定义的。</p><ul><li>第一组特征与车辆行为(速度、加速等)有关</li><li>第二组特征与驾驶员行为(刹车、油门等)有关</li><li>最后一组包含与环境有关的特征(道路类型、斜坡等)。</li></ul><p>这些特性中的大多数已经出现在文献中，特别是那些与速度和加速有关的。我们还调整了一些功能，以适应一个重型车辆的动力学。例如，卡车的高加速度阈值为|a| &gt; 0.7 m/s2，而汽车的|a| &gt; 2.5 m/s2。我们添加了一些其他的特征来更精确地描述驾驶事件及其上下文，例如，事件的持续时间和距离。这提供了一个事件的整体图片，而坡度、道路类型和其余的背景特征取决于环境。这些特征是基于一种直接的直觉，即一个事件可能取决于它的背景，例如，收费在高速公路上比在城市道路上更有可能发生。道路类型分为城市道路、主要道路和高速公路三类。</p><p>我们还引入了一些距离特性。这些提供的距离下一个或前一事件相同的类别(例如,油门&gt; 0或节流= 0),原因是我们在收集观察驾驶数据,在一些地区,许多制动事件有序列而在其他地方,有少得多。因此，与下一个/前一个事件的距离可以作为环境的指示器。最后，我们还计算了与驾驶员执行器使用有关的各种特征，特别是驾驶员使用刹车、节流阀或减速器时距离事件发生的时间和距离的百分比。直观上，驾驶员在面对不同的驾驶事件时，可能会使用不同的致动器。</p><p>B. 通过SFS判别驾驶特征的选择</p><p>由于表V中列出的特征数量相当大，而且有些可能是相互关联的，因此一个关键问题是选择这些特征的子集，以充分区分驾驶事件。在机器学习领域，特征选择的目的是排除冗余或不相关的“特征[19]”。这样做的目的是通过减少过拟合来提高模型的可解释性，缩短训练时间，提高学习模型的泛化能力。在本文中，我们使用序列正向选择原则(SFS)，它由测试特征[20]，[21]的增长子集组成。为此，将收集到的驱动数据集用作硬数据。</p><p>1）Throttle = 0事件：为了评估识别Throttle =0事件的最具辨识力的feature，使用SFS在每次迭代中添加一个新feature来选择不断增长的feature子集。对于每个子集，我们使用C4.5[22]和线性logistic回归算法[23]对驱动事件进行分类(见V-A节)，并选择提高分类率的特征子集。用于SFS的工具箱WEKA输出最相关的特性列表。因此，以下特征似乎是最重要的分类:最大速度，最低速度，速度标准偏差，初始和最终速度之间的差距(Aspeed)，道路类型，距离，停止，前一个事件，和下一个事件的距离。基于特征子集增长的实验结果证实了一个众所周知的现象，即利用更多的特征可能会导致识别精度下降，因为有些特征是冗余的，会产生噪声。例如，前几公里的平均速度提供的信息并不比道路类型多。<br>一旦这9个属性出现，我们决定在它们之间做进一步的手工选择。我们的目标是在属性的数量、在真实数据和数字数据上计算这些属性的简便性和分类结果之间取得最好的折衷。例如，分类率(详见V-B节)为73.5%，有以下9个特征(最大速度，最小速度)。(Aspeed)、道路类型、距离、停止、速度标准偏差、前一事件、与下一事件的距离)，而只有以下6个特征(最高速度、最低速度、(Aspeed)、道路类型、距离和停止)，这些事件被正确找到的概率为74%。接下来的小节中给出的Throttle = 0事件的所有结果都是使用这6个特性获得的。直观上，SFS选择的特征在预期的情况下是一致的，即驾驶事件大多可以用速度相关和上下文相关的特征来描述</p><p>2)Throttle &gt; 0事件:对于节流阀= 0事件，我们测试共同特征，特别是与速度和加速有关的特征。表V列出了用于自动识别驾驶事件的所有驾驶特性。由于燃油是在驾驶员踩油门踏板时注入的，所以我们也介绍了一些关注燃油消耗的特性，特别是[17]衍生的特性，比如均匀驾驶的持续时间和突然变速持续时间的百分比。再次应用SFS选择判别能力最强的特征来识别Throttle = 0 events， 以及C4.5[22]和线性logistic回归[23]。因此，我们观察到最常见的属性继续发挥主要作用，特别是速度、最小速度、平均速度和速度标准差。但是，与Throttle = 0事件不同，加速属性的影响更大。燃料消耗也作为一个区分属性出现。通过特征的选择，得到最佳分类精度的8个特征子集:Aspeed、最小速度、速度标准差、最大加速度、突然变速持续时间、平均转矩百分比、油耗和道路类型。<br>就像Throttle = 0事件一样，这一结果被证明与我们预期的特征是一致的，这是基于对车手在这些事件中的行为的了解。例如,区分事件的城市主干道的起飞和起飞:考虑到最终速度应小于50公里/小时城市起飞,而最终速度主要道路起飞约70 - 80公里/小时,该功能最终速度是直觉地预期作为识别特征对这两个事件进行分类。值得注意的是，特别是在我们的后续工作中，当只使用以下四个属性时，我们只观察到性能下降了1%，即Aspeed、最低速度、速度标准偏差和道路类型。</p><h2 id="DRIVING-EVENTS-RECOGNITION-FROM-REAL-DRIVING-DATABASE-ON-TWO-MACHINE-LEARNING-ALGORITHMS"><a href="#DRIVING-EVENTS-RECOGNITION-FROM-REAL-DRIVING-DATABASE-ON-TWO-MACHINE-LEARNING-ALGORITHMS" class="headerlink" title="DRIVING EVENTS RECOGNITION FROM REAL DRIVING DATABASE ON TWO MACHINE LEARNING ALGORITHMS"></a>DRIVING EVENTS RECOGNITION FROM REAL DRIVING DATABASE ON TWO MACHINE LEARNING ALGORITHMS</h2><p>这项工作的目的是自动识别影响驾驶员行为的重要驾驶事件。一旦驾驶事件被定义和鉴别特征被识别，我们就可以提出识别驾驶事件的机器学习算法。为了达到我们的目标，我们使用了V-A节中介绍的两种算法，即决策树和线性逻辑回归。这些应用于真实的驾驶事件，结果将在下面的子节中给出。图4给出了从原始数据集S到算法结果的分类过程。数据集S的每个实例代表一个事件Ev以及每个特性对应的值。</p><p>A. 基于学习的算法</p><ol><li>决策树<br>我们选择这种方法是因为它保留了属性的物理意义，这对我们的工作是非常必要的。事实上，我们的首要任务之一是充分理解为什么有些实例被归类到一个类而不是另一个类。<br>此外，这种方法允许我们通过输出树清楚地看到哪些特征是最重要的。我们使用WEKA数据挖掘软件中的C4.5算法对事件[22]、[24]进行分类。该算法利用信息熵构建决策树:在每个节点上，最具辨别性的属性取决于增益比，即通过每个类中的实例数(分裂信息)加权的信息增益。</li></ol><p>集合S的属性a的增益$G(S, a)$为</p><p>$$<br>G(S, a) = E(S) - \sum_{i=1}^{m} f_s(a_i) \times E(S_{a_i})<br>$$</p><ul><li>$m$ 是S中a的不同值的数量</li><li>$f_s(a_i)$ 为S中具有$a_i$的项对a的值的频率</li><li>$a_i$ 是a中第i个可能的值</li><li>$S_{a_i}$ 是S的一个子集，包含a的值为$a_i$的所有项</li><li>$E(S)$是对a进行分割前数据集S的信息熵</li></ul><p>$$<br>E(S) = - \sum_{j=1}^n f_s(j) \times log_2 f_s(j)<br>$$</p><ul><li>n是S中不同属性值的个数</li><li>$f_s(j)$是集合S中j值出现的频率。</li></ul><ol start="2"><li>基于线性逻辑回归的分类器<br>本文也使用线性逻辑回归(LLR)与决策树C4.5进行比较。LLR建立了一个线性模型。回归变量是表v中列出的特性。LLR保留了属性的物理意义，并为每个特性赋予了权重。在WEKA软件中，LLR被称为SimpleLogistic算法，它基于[23]和[25]。它创建线性逻辑回归模型，以获得一个给定实例属于一个类的后验概率。线性logistic回归将一个线性模型与一个概率函数关联起来，并确保概率之和为1，且保持在[0,1]。根据[23]和[25]，逻辑线性回归模型定义如下:<br>$$<br>P_r(Ev = k|Features = a) = \frac{e^{F_k(a)}}{\sum_{l=1}^K e^{F_l(a)}}<br>$$<br>其中<br>$$<br>\sum_{l=1}^K F_l(a) = 0<br>$$<br>$$<br>F_k(a) = \beta_k^T \dot a = \sum_{i=1}^n \beta_{ik} a_i + \beta_{0k}<br>$$</li></ol><ul><li>$P_r(Ev = k|Features = a)$是在输入特征向量$a = (a1，……，an)$下，属于k类的后验概率</li><li>n是特征的数量</li><li>K是类的数目</li><li>$F_j(f_k)$ 为参数为$\beta_j$的线性模型</li></ul><p>简单的logistic算法基于LogistBoost算法[26]估计这些参数。该算法一直运行到收敛时才能找到线性logistic模型的最大似然值，该模型的线性函数采用线性最小二乘回归拟合。</p><p>B. Throttle = 0 事件的分类</p><p>数据库包含913个Throttle =0事件，被分为13个类，见第III-B节。除非另有说明，以下所有结果均使用C4.5算法使用10倍交叉验证方法[27]获得。数据集因此被划分为10个折叠，每个折叠有91或92个实例。两种算法分别使用9个分区对模型进行学习，用剩余的分区对学习后的模型进行测试。这样重复了10次，但每次都使用不同的学习和测试集。然而，对于13个类，两种学习算法在初始原始数据库上进行蛮力训练的结果是令人失望的49%的分类率。这个分类率已经远远好于随机分类。有些分类，如J: toll，分类很好，如表六所示，而其他分类率相当低，如F: curves。<br><img src= "/img/loading.gif" data-lazy-src="/image/600002/t006.png" alt=""></p><p>我们的目标是识别导致不同驾驶员行为的驾驶事件。因此，我们与内部专家驾驶员讨论这些结果，他们通过驱动器的利用分析了他们的驾驶方法。他们发现他们在一些驾驶事件上有相似的行为，这些驾驶事件被学习算法严重混淆。为了量化这种相似性并提高识别结果，我们根据式(6)测量每对类之间的Fisher线性判别法。</p><p>$$<br>J_{\frac{1}{2}} = \frac{1}{n} \times \sum{i=1}{n} \frac{(\mu_{1,n} - \mu{2,n})^2}{\sigma_{1,n}^2 + \sigma_{2,n}^2}<br>$$</p><ul><li>$J_{\frac{1}{2}}$是第一类和第二类之间的Fisher距离</li><li>n是特征的数量</li><li>$\mu_{C,n}$为特征n在C类中的平均值</li><li>$\sigma_{C,n}$为特征n在C类中的标准差值<br>这个判别器可以被解释为一个距离，并被用来量化类间相似性与类内差异<br>例如，图5给出了到B的Fisher距离: traffic，显示出这个类与许多其他类太接近，很难区分。<br><img src= "/img/loading.gif" data-lazy-src="/image/600002/g005.png" alt=""></li></ul><p>专家驾驶员的建议和Fisher判别器的分析导致我们修改关于驾驶员行为的初始数据库标签，并对一些类和实例进行分组和/或分离。下面的小节将介绍每一步。</p><p>C. 讨论关于Throttle = 0 事件的识别<br>仔细观察类E的速度图:大小约束，我们发现需要将5个实例切换到另一个类。然后，对于类F: curve，高速实例的速度形状与类H: speed imit相似。这些最初的改变将分级率提高到57%，特别是H级限速级speed limit class(+18%)和A级环形交叉口级roundabout class(+10%)。<br>进一步分析上述类的合并，我们还发现，从驾驶员行为的角度来看，以停止结束的出口匝道事件与通行费事件非常相似。因此，我们将类M:exit ramp带有stop的出口匝道的实例移动到类J: toll。</p><p>类似地，D类:红绿灯的停止事件与I类:停止标志的停止事件非常相似。这种混淆可以在表七所示的混淆矩阵中观察到。当这两个类合并后，分类率上升到63%。</p><p>然后我们研究了L: give-way类，它的分类率仍然很低。在检查分类结果时，我们发现带有stop的实例总是与类I: stop相混淆。因此，我们将这些实例移到类I: stop。此外，我们还合并类L的其余实例。A类的让道:roundabout是让道的一种变体。这些变化使我们能够减少类的数量并改进分类结果。</p><p>此时，分类结果和混淆矩阵表明，C:减速带，E:尺寸约束，F:曲线，经常被混淆。通过这些事件的速度曲线可以观察到它们的相似性。这三个类别都被归为一个更广泛的类别，称为E:城市减速。这个类别包括所有在城市地区的减速事件，除了那些由交通或环形路引起的减速事件。这使得分类数量减少到9个，而分类率增加了5%。</p><p>这种分类的缺点现在是B:分类率在20%左右的流量类。这是由于广泛的驾驶情况包括在这类。专家驾驶员强调，交通状况会根据道路类型而变化。因此，我们根据道路类型创建集群，将B: traffic类实例分为3类，分别为E:城市减速，H:限速，B: traffic。该解决方案确保了最终的全局分类率为74%。</p><p>最后，由于我们的标记修改，识别准确率从大约50%提高到74%，如表VIII所示。LLR甚至取得了略好一点的结果，识别率达76%。<br><img src= "/img/loading.gif" data-lazy-src="/image/600002/t008.png" alt=""><br>这个74%的最终结果是通过对我们数据库中913个事件进行10倍交叉验证得到的，假阳性率为6.7%。为了检验所提方法的泛化能力，我们进一步收集了一组新的使用同一车辆的驾驶数据。这个附加的数据集包含242个事件。<br>913事件的初始数据库用作训练集的算法,和242年的事件作为测试集。在这些相同的条件和选择属性,C4.5达到70%的识别率,这是非常接近74%的初始数据库上实现。LLR的识别率达到71%。考虑到类的数量，这个识别率相当不错。此外，驾驶员在面对不同事件时的行为是相似的，这是可以理解的，例如在环形交叉处的制动或卡车大小的约束事件。从我们专业的司机经验来看，这是意料之中的。这就是为什么在第二个数据库中验证242个事件的分类率对于我们的目标来说是可以接受的。最后，这项工作表明，基本特征，特别是最大和最小速度，是最重要的节流=0事件分类。关于Throttle = 0事件的初始工作允许我们构建并验证我们的方法，从而继续对Throttle&gt; O事件进行分类。</p><h2 id="DRIVING-EVENTS-RECOGNITION-IN-DIGITAL-ROADS"><a href="#DRIVING-EVENTS-RECOGNITION-IN-DIGITAL-ROADS" class="headerlink" title="DRIVING EVENTS RECOGNITION IN DIGITAL ROADS"></a>DRIVING EVENTS RECOGNITION IN DIGITAL ROADS</h2><h2 id="CONCLUSION"><a href="#CONCLUSION" class="headerlink" title="CONCLUSION"></a>CONCLUSION</h2>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> driving behavior proﬁling </tag>
            
            <tag> driving event </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>交通领域类期刊</title>
      <link href="/posts/600000/"/>
      <url>/posts/600000/</url>
      
        <content type="html"><![CDATA[<ol><li>交通领域类期刊谷歌学术排名，时间为2020年10月16日。</li></ol><p>| 序号  | 出版物 |  h5指数   |  h5中位数 |<br>| 1 |   IEEE Transactions on Intelligent Transportation Systems  | 92  |  125 |<br>| 2 |   Transportation Research Part C: Emerging Technologies   |  83  |  122 |<br>| 3 |   Transportation Research Part A: Policy and Practice  | 67   | 86 |<br>| 4 |   Transportation Research Part B: Methodological   | 65  |  86 |<br>| 5 |   Accident Analysis &amp; Prevention  |  61  |  75 |<br>| 6 |   Transportation Research Part D: Transport and Environment   |  59  |  82 |<br>| 7 |   Transportation Research Part E: Logistics and Transportation Review  | 57  |  75 |<br>| 8 |   Journal of Transport Geography  |  55  |  70 |<br>| 9 |   Transport Policy   |   51  |  68 |<br>| 10 |  Transport Reviews   |  47  |  66 |<br>| 11 |  Transportation Research Procedia    |  47  |  64 |<br>| 12 |  IEEE Intelligent Vehicles Symposium  | 46  |  61 |<br>| 13 |  Transportation Science   | 43 |   67 |<br>| 14 |  Transportation  |  43  |  65 |<br>| 15 |  Computer‐Aided Civil and Infrastructure Engineering |  43  |  63 |<br>| 16 |  Journal of Air Transport Management  | 43  |  63 |<br>| 17 |  Transportation Research Part F: Traffic Psychology and Behaviour  |    42  |  53 |<br>| 18 |  Transportation Research Record   | 39  |  56 |<br>| 19 |  IEEE Intelligent Transportation Systems Conference  |  37  |  60 |<br>| 20 |  IEEE Vehicular Technology Magazine  |  36  |  80 |</p><p>最新可见<a href="https://scholar.google.com/citations?view_op=top_venues&hl=zh-CN&vq=eng_transportation" target="_blank" rel="noopener">交通领域期刊谷歌学术排名</a></p><ol start="2"><li>fffff</li></ol><ol start="3"><li>其他的</li></ol><ul><li><a href="https://person.zju.edu.cn/xiqun/687353.html" target="_blank" rel="noopener">陈喜群博士</a></li><li><a href="http://blog.sciencenet.cn/blog-400447-1051516.html" target="_blank" rel="noopener">贺正冰博客中的国际交通期刊</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 交通领域 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 交通领域 </tag>
            
            <tag> 期刊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Driver Behavior Profiling Using Smartphones:  A Low-Cost Platform  for Driver Monitoring</title>
      <link href="/posts/56295/"/>
      <url>/posts/56295/</url>
      
        <content type="html"><![CDATA[<h2 id="Fuzzy-Logic-Based-Event-Detection"><a href="#Fuzzy-Logic-Based-Event-Detection" class="headerlink" title="Fuzzy Logic Based Event Detection"></a>Fuzzy Logic Based Event Detection</h2><p>现有的驾驶员程序分析机制通常基于多个输入数据和基于固定阈值的事件检测。例如，如果车辆的瞬时速度大于120公里/小时[6]，就会触发超速事件，这在高速公路的场景中是不现实的，在高速公路中，由于道路类型不同，速度限制可能会有所不同。像Greenroad[6]这样的商业应用也依赖GPS和智能手机传感器数据来检测事件。在这个应用程序中，分数被简单地计算为事件率，即应用程序计算的每单位距离上的事件数。在本例中，所有类型的事件都具有相同的评分相关性，并简单地合并到一个全局事件计数器中。</p><p>在SenseFleet中，本文同时考虑GPS和运动传感器的输入数据。</p><p><span class="inline-tag red">accelerometer</span></p><p>设备的加速度输出$a(t)=[a_x(t),a_y(t),a_z(t)]$,单位$m/s^2$,<br>则加速度矢量大小的描述如下：<br>$$<br>|a(t)| = \quart(a_x(t)^2+a_y(t)^2+a_z(t)^2) \tag{1}<br>$$<br>本文试图通过独立考虑每个加速度轴来推断汽车的纵向和横向运动。为此，我们将加速度矢量平移到地球坐标系，以便与飞行器的轨迹相一致。然而，即使这些信号被过滤(使用卡尔曼滤波器)，它也不可能清楚地从这个输出分解车辆的纵向和横向运动。然后我们决定计算加速度矢量的大小来减轻这个问题。需要注意的是，这个数值对于坐标系统(例如，设备，地球)来说是不变的，当司机在使用应用程序而车辆停止时，允许设备旋转或操作。</p><p>最终 jerk(j)被计算为加速度大小的时间导数<br>$$<br>j(t) = \fran{d|a(t)|}{dt} \tag{2}<br>$$</p><p>在计算了jerk后，为了测量车辆的方向变化(偏航率)direction variation (yaw rate) ，本文使用该设备的磁性和重力传感器来计算方向矢量。这个矢量包括偏航(yaw)、俯仰(pitch)和横摇(roll)，它们描述了绕不同轴的旋转。</p><p>本文考虑yaw rate,y测量车辆绕垂直于地球表面的轴旋转时的转向。<br><span class="inline-tag red"></span></p><p>运动传感器(如加速度计、磁传感器、重力传感器)的一个主要限制是噪声暴露量大，主要是电磁干扰和设备振动[19]。因此，在提出的机制中，我们融合运动传感器数据和GPS数据，以准确检测驾驶事件。特别是基于gps的度量，我们考虑了速度变化($\delta S$)和方位变化($\delta B$)GPS数据以1 Hz的采样率获得，其速度变化以两个连续采样的差值计算</p><p>$$<br>\delta S = S(t) - S(t-1) \tag{3}<br>$$</p><p>同样，方位变化量(即与北方的相对角度变化量，以。/s测量)的计算方法如公式4所示。</p><p>$$<br>\delta B = B(t) - B(t-1) \tag{4}<br>$$</p><p>不同的输入变量在不同的采样率下得到。在运动传感器的情况下，采样率在20到50赫兹之间变化，这取决于设备硬件和操作系统版本。为了缓解这些影响，我们实现了一个传感器融合层来同步运动传感器和GPS样本，以执行基于不同时间序列的事件检测。假设GPS样本以1hz的固定频率接收，我们在最后一秒内存储j和y样本。然后，对每个GPS定位固定的，我们计算$\delta S$和$\delta B$,平均yaw rate($\mu (y)$),和jerk标准差($\theta (j)$)。为了进一步减轻驾驶过程中设备振动的影响，我们考虑了jerk标准差而不是原始jerk或加速度。经过几次试验，我们观察到在加速、刹车或转向事件中，jerk标准差比总加速度或平均jerk表现出更明显的变化。</p><p>为了检测驱动事件，我们建立了一个模糊系统[20]，该系统由输入数据的一个模糊化阶段($[\theta (j),\mu (y),\delta S,\delta B]$)和一套模糊规则的应用组成。每个规则评估输入变量的不同可能的模糊值的组合和某一类型事件的输出(例如，硬加速，硬刹车，粗暴的转向，超速)</p><p>针对不同类型的机动，通过分析受控场景的输入变量值，手动推导出规则。对于输入变量的模糊化过程，我们考虑梯形隶属函数。对于输出变量，我们为每种不同类型的事件考虑一个单独的crisp值，从而允许重心去模糊化过程单独检测事件。各变量的模糊集如表1所示。对于模糊系统实现，我们使用jFuzzyLogic[21]，这是一种用于Java的开源模糊逻辑实现。然而，如下所述，这些集合的极限是在校准过程后动态建立的。模糊规则指示要触发的事件的特定条件。</p><p>举例来说，为了检测硬加速，系统考虑以下规则:</p><p>(未完)…</p>]]></content>
      
      
      
        <tags>
            
            <tag> paper </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> driving behavior proﬁling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>驾驶行为特征选取</title>
      <link href="/posts/600403/"/>
      <url>/posts/600403/</url>
      
        <content type="html"><![CDATA[<p>横向加速或侧向运动代表行驶事件，如左右转弯和变道，而纵向加速则代表车辆制动和加速。</p><p>根据论文[1]中可以看出总共有13个驾驶事件，见下图</p><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/DrivingEvents.png" alt="驾驶事件"></p><p>一个简单的线性插值用来处理这个不完整数据集。</p><p>论文[1]中根据实验内容提出了下面的13种驾驶事件</p><ul><li><span class="inline-tag blue"><strong>brake (B)</strong></span></li><li><span class="inline-tag blue"><strong>sudden brake (SB)</strong></span></li><li><span class="inline-tag blue"><strong>accelerate (A)</strong></span></li><li><span class="inline-tag blue"><strong>sudden accelerate (SA)</strong></span></li><li><span class="inline-tag blue"><strong>u-turn (U)</strong></span></li><li><span class="inline-tag blue"><strong>left turn (L)</strong></span></li><li><span class="inline-tag blue"><strong>sudden left turn (SL)</strong></span></li><li><span class="inline-tag blue"><strong>right turn (R)</strong></span></li><li><span class="inline-tag blue"><strong>sudden right turn (SR)</strong></span></li><li><span class="inline-tag blue"><strong>lane change left (CL)</strong></span></li><li><span class="inline-tag blue"><strong>sudden lane change left (SCL)</strong></span></li><li><span class="inline-tag blue"><strong>lane change right(CR)</strong></span></li><li><span class="inline-tag blue"><strong>sudden lane change right(SCR)</strong></span></li></ul><p>论文[2]中包含着5种攻击性驾驶事件</p><ul><li><span class="inline-tag green"><strong>over speed limit (OS)</strong></span></li><li><span class="inline-tag green"><strong>sudden brake (SB)</strong></span></li><li><span class="inline-tag green"><strong>sudden acceleration (SA)</strong></span></li><li><span class="inline-tag green"><strong>sudden turn (ST)</strong></span></li><li><span class="inline-tag green"><strong>sudden lane change (SC)</strong></span></li></ul><p>检测到的每一个突然的驾驶事件都有一个与之关联的地理位置，以指示这些事件发生的位置。将指定半径内的事件分组在一起，生成突发驾驶事件的热图。一个聚集了大量事故的区域将表明一个潜在的事故高发点。司机在将要经过这些点的时候提前预警，这将有效地减少交通事故的风险。局部的危险驾驶事件可以聚集在一起，形成一个潜在的事故高发点，可以部署为接近的车辆作为一个基于位置的先进警告服务。</p><p>论文[3]中驾驶事件类型选择了下面7种。</p><ul><li><span class="inline-tag red"><strong>Aggressive breaking</strong></span></li><li><span class="inline-tag red"><strong>Aggressive acceleration</strong></span></li><li><span class="inline-tag red"><strong>Aggressive left turn</strong></span></li><li><span class="inline-tag red"><strong>Aggressive right turn</strong></span></li><li><span class="inline-tag red"><strong>Aggressive left lane change</strong></span></li><li><span class="inline-tag red"><strong>Aggressive right lane change</strong></span></li><li><span class="inline-tag red"><strong>Non-aggressive event</strong></span></li></ul><p>参考论文：</p><ol><li><p>Saiprasert, Chalermpol &amp; Pholprasit, T. &amp; Pattara-atikom, Wasan. (2013). Detecting Driving Events Using Smartphone. 20th ITS World Congress. </p></li><li><p>Saiprasert, Chalermpol &amp; Thajchayapong, Suttipong &amp; Pholprasit, T. &amp; Tanprasert, Chularat. (2014). Driver behaviour profiling using smartphone sensory data in a V2I environment. 10.1109/ICCVE.2014.7297609. </p></li><li><p>Ferreira J Júnior, Carvalho E, Ferreira BV, de Souza C, Suhara Y, et al. (2017) Driver behavior profiling: An investigation with different smartphone sensors and machine learning. PLOS ONE 12(4): e0174959. <a href="https://doi.org/10.1371/journal.pone.0174959" target="_blank" rel="noopener">https://doi.org/10.1371/journal.pone.0174959</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> summary </tag>
            
            <tag> driving behavior proﬁling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Driver behavior profiling: An investigation with different smartphone sensors and machine learning</title>
      <link href="/posts/600001/"/>
      <url>/posts/600001/</url>
      
        <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p><strong>背景</strong><br>驾驶员的行为影响交通安全、燃料/能源消耗和气体排放。司机行为分析试图理解和积极影响司机的行为。通常驾驶员分析人物包括驾驶数据的自动收集和能生成一个分类来描绘出驾驶员攻击性的属性的计算机模型应用。不同的传感器和方法被应用在任务中，然而低成本解决方案和高性能仍然是研究的目标。</p><p><strong>本文</strong><br>本文对不同的Android智能手机传感器和分类算法进行了研究，以评估哪一种传感器/方法的组装能够使分类具有更高的性能。结果表明，特定的传感器组合和智能方法可以提高分类性能。</p><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>驾驶员监控和分析(Drivermonitoring andanalysis)或驾驶员行为分析(driverbehaviorprofiling)是自动收集驾驶数据(如速度、加速、刹车、转向、位置)并应用计算模型来为驾驶员生成安全评分的过程。</p><p>驾驶数据收集可以通过多种类型的传感器来实现，从智能手机中的通用设备，到监控摄像头、远程信息盒和车载诊断适配器等专用设备。</p><p><strong>本文的贡献</strong></p><p>本文的主要贡献是在现实世界的实验背景下，评估机器学习算法和Android智能手机传感器的多种组合在检测攻击性驾驶事件的任务中的性能。本文执行数据收集阶段，驾驶一辆车，并从几个不同的传感器收集数据，同时执行不同的调动。本文将介绍机器学习方法如何在任务中使用，并评估传感器和技术组合的准确性，旨在找到传感器/技术与每一类行为的最佳匹配。</p><h1 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h1><p><span class="inline-tag blue">Nericell由Mohan等人[17]</span>提出，是一款Windows Mobile智能手机应用程序，用于监测道路和交通状况。它使用智能手机的加速计来检测路面坑洼和刹车事件。它还使用麦克风来检测喇叭声，以及GPS/全球移动通信系统(GSM)来获得车辆的定位和速度。通过比较一组经验预先设定的阈值与加速度计数据的突然变化或与N秒滑动窗口内的平均值来检测刹车、颠簸和坑洞。没有使用MLA来探测这类事件。一些事件检测结果的False Positives(FPs)和Fale Negatives(FNs)包括:4.4%的FN, 22.2%的FP的刹车事件;低速(&lt;25公里/小时)检测颠簸/坑槽，FN为23%，FP为5%;FN和FP用于检测暴露车辆(如摩托车)的喇叭声。</p><p><span class="inline-tag blue">Dai and colleagues[9]</span>提出了一种Android应用程序，旨在实时检测和警报危险驾驶事件，通常与酒后驾驶有关。该应用程序使用智能手机的加速计和方向(偏航、俯仰和滚转角)传感器来检测异常的曲线运动(ACM)和保持速度的问题(PMS)，这是醉酒驾驶相关行为的两大类。用一系列方程来确定横向和纵向加速度矢量。如果在5秒时间窗内横向加速度的最大值和最小值之间的差异超过经验阈值，则ACM事件被检测。在任何给定时间，如果纵向加速度超过正或负的固定经验值，则检测PMS。与[17]类似，不使用MLA进行事件检测。实验结果:异常曲线运动FN为0%，FP为0.49%;速度控制问题的FN为0%，FP为2.90%。</p><p><span class="inline-tag blue">Johnson和Trivedi[10]</span>创建了一个名为MIROAD的iPhone应用程序。MIROAD使用基于智能手机的磁强计、加速计、陀螺仪和GPS传感器融合来检测主动驾驶事件，并据此将驾驶员的驾驶风格分为主动驾驶和非主动驾驶。基于DTW算法，采用单一分类器检测攻击事件。所有的处理都在智能手机上实时执行。实验分析表明，97%的攻击事件被正确检测到。</p><p><span class="inline-tag blue">WreckWatch由White等人[18]</span>提出，是一款基于Android智能手机的客户端/服务器应用程序，用于检测车祸。客户端应用程序检测事故，记录相关数据，并将其发送给服务器应用程序，服务器应用程序可以通知相关部门。失事监视客户端使用来自加速度计、GPS和麦克风的数据，并使用基于阈值的过滤来检测事故。事故预测框架由电话状态的11元组模型和评估模型是否表示事故的函数组成。模型变量包括在任何方向上经历的最大加速度，以及是否有响亮的声音发生的指示。一个触发事故检测的场景是当加速度、声音和车辆速度都高于经验阈值。与[9,17]相似，事故检测不采用MLA。实验结果表明，手机掉落不太可能导致FPs，智能手机可能检测不到一些事故，声学数据不足以检测出事故。</p><p><span class="inline-tag blue">Araujo and colleagues[15]</span>展示了一款智能手机应用程序，可以将油耗效率作为驾驶员行为的一个功能来评估。该应用程序还提供了实时驾驶提示，如shift gear earlier和accelerating too high。该应用程序使用OBD蓝牙适配器和Torque Pro智能手机应用程序从车辆传感器收集数据(例如，速度、加速度、转速)，而不是从智能手机传感器收集数据。数据采集完成后，app提取一系列特征，并对其应用三个分类器(一个线性判别器和两个模糊系统)。所有的处理都在智能手机上实时执行。</p><p><span class="inline-tag blue">Eren等人[11]</span>提出了一种iPhone应用程序，可以根据危险驾驶事件将驾驶员行为分类为安全或危险。该应用程序可以检测突然转弯、车道偏离、刹车和加速事件。用于事件检测的传感器有智能手机的加速度计、陀螺仪和磁强计。应用程序使用端点检测算法来划分事件的开始和结束时间。通过DTW算法将标定后的事件与模板事件数据进行比较。最后，贝叶斯分类器根据一段时间内发生的事件数量，将驾驶员的行为标记为安全或危险。实验结果表明，15个驾驶员中有14个(93.3%)被正确分类。值得注意的是，本文仅提供了驱动分类结果。因此，不提供事件分类性能结果。</p><p><span class="inline-tag blue">Fazeen和同事[19]</span>使用Android智能手机加速计和GPS来识别车辆状况(速度和换位)、驾驶模式(加速/减速和变道)和道路状况(光滑、不平整、粗糙、有颠簸或坑洼)。检测事件的主要方法是计算连续加速度计读数在某些轴上的持续时间、差异和斜率，并将它们与经验的固定/动态阈值进行比较。例如，工作状态，安全加减速在y轴上的重力不超过0.3g。与[9,17,18]类似，不使用MLA进行事件检测。实验结果表明，路面异常分类准确率为:凹凸路面异常分类准确率为81.5%，坑洼路面异常分类准确率为72.2%，高低路面异常分类准确率为75%，平坦路面异常分类准确率为91.5%，高低路面异常分类准确率为89.4%。</p><p><span class="inline-tag blue">Castignani等人[12]</span>提出了一个基于模糊逻辑的驾驶员行为分析移动工具，该工具利用了Android智能手机中的加速度计、磁强计和重力传感器。这个工具将驾驶员的行为分为Normal, Moderate, 和Aggressive，对应的驾驶分数在0(最好)和100(最差)之间。由于传感器数据是在byUBI-Meter移动应用程序中采集并存储在本地智能手机上，因此分类和评分不进行实时处理。随后，这些数据被发送到Internet上的远程服务器进行处理。Saiprasert和他的合作者[13]使用了GPS、加速度计和智能手机磁强计传感器来描述司机的行为:Very Safe,Safe,Aggressive, 和Very Aggressive。该剖面是通过检测相关驾驶事件实时计算的。事件检测是由DTW算法[20]的变化来执行的。</p><p><span class="inline-tag blue">SenseFleet由Castignani等人[14]</span>提出，是一个针对Android智能手机的驾驶员行为分析平台，能够独立于移动设备和车辆检测危险驾驶事件。该手机应用程序从加速度计、磁强计、重力传感器和GPS智能手机传感器收集数据，并利用模糊系统检测旅途中可能发生的危险事件，如超速、转弯、加速和刹车。应用程序计算每次行车的驾驶分数在0(最差)到100(最好)之间，作为检测到的危险事件的函数。所有的处理都是在智能手机上实时完成的。作者做了几个实验。在其中一个实验中，应用程序正确识别了90%以上的风险事件。</p><p><span class="inline-tag blue">Wahlstrom, Skog和Handel[21]</span>提出了一个基于轮胎滑移(滑移)和车辆翻车的理论可能性的检测危险车辆转弯事件的框架。全球导航卫星系统(GNSS)是Android智能手机GPS的总称，是唯一用于事件检测的传感器。利用经典力学方程，定义了滑移和翻车的理论阈值。滑动阈值根据切向和旋转速度、切向加速度以及车辆轮胎与道路之间的摩擦系数来定义。车辆翻车阈值是根据车辆的重心、轨道宽度和高度定义的。与[9,17 19]类似，事件检测中没有使用MLA。从三款不同的Android智能手机收集的GNSS数据的实验分析显示，FP和FN的平均比率为13%是最佳结果。</p><p>本文根据研究发现了一些值得注意的地方</p><ol><li>除了[21]之外，所有的论文都使用 <span class="inline-tag red"><strong>传感器融合数据</strong></span>作为事件检测算法的输入</li><li>采用的MLAs可归结为 <span class="inline-tag red"><strong>模糊逻辑</strong></span>[12,14,15]或 <span class="inline-tag red"><strong>DTW的变体</strong></span>[10,11,13]</li><li>最近的论文使用 <span class="inline-tag red"><strong>智能手机传感器</strong></span>，而不是车辆或远程信息箱传感器</li><li>一些论文[9,17-19,21]没有使用MLAs来检测驱动事件，而是使用 <span class="inline-tag red"><strong>物理方程或固定/动态阈值或两者的结合</strong></span>来达到这一目的。</li></ol><p>本文是在本地运行机器学习算法，但是作者表示未来的版本可能会运行在基于云的方式上，如[22,23]所示。其他与驾驶员行为相关的方法也与社会机制有关，如Song和Smith[24]所做的评价，即在高占有率的收费车道上研究驾驶员行为。在Sun和同事[25]的工作中，我们可以看到一个关于智能和互联社区的更全面的方法。关于基于智能手机的汽车传感系统的更全面的比较，请参考Engelbrecht和他的同事[26]的研究。</p><h1 id="Machine-learning-algorithms"><a href="#Machine-learning-algorithms" class="headerlink" title="Machine learning algorithms"></a>Machine learning algorithms</h1><p>本文比较了四种MLA的性能:人工神经网络(ANN)、支持向量机(SVM)、随机森林(RF)和贝叶斯网络(BN)。之所以选择这些MLAs是因为它们在分类问题的文献中大量出现，而且它们代表了不同的机器学习“tribes”，保证了机器学习算法的多样性。本节主要解释上述MLAs的基本概念。</p><ul><li><strong>Artificial neural networks(ANN)</strong></li><li><strong>Support vector machines(SVM)</strong></li><li><strong>Random forrest(RF)</strong></li><li><strong>Bayesian networks(BN)</strong></li></ul><h1 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h1><p>将工作建模为一个 <span class="inline-tag red"><strong>多标签监督学习分类问题</strong></span>，其中标签是 <span class="inline-tag red"><strong>驾驶事件类型</strong></span>(driving events types)。<br>本文工作的目标是识别运动传感器(及其轴)、学习算法(及其参数)和滑动窗口(nf)中的帧数的最佳组合，以检测单个驾驶事件类型。</p><p>在表单中定义了一个计算程序集</p><span class="inline-tag grey">$EA = \lbrace 1:sensor, \ 2:sensor \ axis(es),\  3:MLA,\  4:MLA \ configuration,\  5:nf \ \rbrace$</span>  <p>通过训练、测试和评估由指定的MLA(element #3)及其配置参数(element #4)生成的分类器的性能，对传感器(element #1)、它的轴(es)(element #2)和滑动窗口中的帧数(element #5)进行识别。通过更改该程序集中元素的值，本文实现了不同的驾驶员事件检测性能。因此，该评估将对元素值的几个组合进行评估，以揭示对每种驾驶事件类型表现最好的组合。</p><h2 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h2><p>图1显示了评估管道的高级视图。</p><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/g001.png" alt="图1. 评估管道的高层视图，显示了从原始传感器数据采样到培训、测试和评估MLAs的处理步骤"></p><p><span class="inline-tag red"><strong>第一步</strong></span></p><p> 对智能手机传感器原始数据进行采样，并将其从 <span class="inline-tag red"><strong>设备坐标系转换为地球坐标系</strong></span>(图2)。为了实现车辆内部的设备位置独立，这种平移是必要的。转换后的传感器数据存储在智能手机文件系统中。</p><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/g002.png" alt="图2. 转换坐标系"></p><p><span class="inline-tag red"><strong>第二步</strong></span></p><p> 从智能手机中检索转换后的传感器数据文件，并将其作为输入来生成<span class="inline-tag red"><strong>属性向量数据集</strong></span>。</p><p><span class="inline-tag red"><strong>第三步</strong></span></p><p>利用属性向量数据集对MLAs的性能进行训练、测试和评估。</p><p>如图1所示，第一个管道步骤在智能手机上执行，而第二个和最后一个步骤在普通计算机上执行。</p><p><span class="inline-tag red"><strong>评估性能</strong></span></p><p>本文用来评估每种驾驶事件类型组合性能的度量是<span class="inline-tag red"><strong>ROC曲线(AUC)下的面积</strong></span>[33,34]。分类器的AUC范围为0.0(最差)到1.0(最好)，但现实分类器的AUC不应该小于0.5，这相当于随机猜测。因此，计算程序集AUC越接近1.0，它在检测特定驾驶事件类型方面就越好。</p><h2 id="Evaluation-assembly"><a href="#Evaluation-assembly" class="headerlink" title="Evaluation assembly"></a>Evaluation assembly</h2><h3 id="sensor"><a href="#sensor" class="headerlink" title="sensor"></a>sensor</h3><p><span class="inline-tag red"><strong>传感器(sensor)</strong></span>是评估组装的第一个元素，它代表了以下Android智能手机运动传感器之一: <span class="inline-tag red"><strong>加速度计(Acc)，线性加速度(LinAcc)，磁强计(Mag)和陀螺仪(Gyr)</strong></span>。</p><ol><li><span class="inline-tag blue"><strong>加速度计</strong></span>测量施加在设备上的加速度，单位是米每秒的平方(m/s2)，包括重力。</li><li><span class="inline-tag blue"><strong>线性加速度传感器</strong></span>类似于加速度计，但不包括重力。</li><li><span class="inline-tag blue"><strong>磁强计</strong></span>测量应用于设备的微特斯拉(μT)磁场的力，其工作原理类似于磁铁。</li><li><span class="inline-tag blue"><strong>陀螺仪</strong></span>以每秒弧度(rad/s)来测量设备轴的旋转速率。</li></ol><p>这些传感器提供了一个在标准传感器坐标系中具有纳秒精度的三维(x,yez)时间序列(相对于设备)</p><h3 id="sensor-axis-es"><a href="#sensor-axis-es" class="headerlink" title="sensor axis(es)"></a>sensor axis(es)</h3><p><span class="inline-tag red"><strong>传感器轴(sensor axis(es))</strong></span>是评估组装的第二个元素</p><p>该元素的可用值为</p><ul><li><span class="inline-tag blue"><strong>all 3 axes</strong></span></li><li><span class="inline-tag blue"><strong>x axis alone</strong></span></li><li><span class="inline-tag blue"><strong>y axis alone</strong></span></li><li><span class="inline-tag blue"><strong>z axis alone</strong></span></li></ul><p>比如：加速度计产生了以下数据集</p><span class="inline-tag grey">    $accelerometer(with \ data\ from\ all\ three\ axes),\ accelerometer_x,\ accelerometer_y,\ accelerometer_z$</span><p>其中唯一的例外是 <span class="inline-tag red"><strong>磁强计</strong></span>,x轴的值在转换到地球的坐标系统后总是0或接近。因此<span class="inline-tag red"><strong>没有omagnetometer_x 数据集</strong></span>。</p><p>本文评估数据来自产生4个数据集的3个传感器,和产生3个数据集的1个传感器,总共有<span class="inline-tag red"><strong>3*4+3=15</strong></span>个数据集。本文在不同的数据集中分离传感器轴，以观察是否有任何一个轴可以作为检测特定驾驶员事件类型的最佳轴。</p><h3 id="MLA"><a href="#MLA" class="headerlink" title="MLA"></a>MLA</h3><p><span class="inline-tag red"><strong>MLA</strong></span>是评估组装的第三个元素<br>评估了MLP、SVM、RF和BN MLAs的分类性能。本文将这些算法的WEKA(版本3.8.0)实现与LIBSVM[35]库(版本3.17)结合使用。本文使用10倍交叉验证来训练和测试这些分类器，以最小化过拟合。</p><h3 id="Algorithm-configuration"><a href="#Algorithm-configuration" class="headerlink" title="Algorithm configuration"></a>Algorithm configuration</h3><p><span class="inline-tag red"><strong>算法配置(Algorithm configuration)</strong></span>是评估组装的第四个元素<br>本文执行了参数网格搜索，以评估表1中参数值的每一种可能组合。本文对大多数参数值进行实验设置，并遵循[36]中支持向量机的指导原则。对于表1中没有列出的参数，本文使用了WEKA默认值。</p><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/t001.png" alt="表1. MLAs configurations"></p><h3 id="sliding-window"><a href="#sliding-window" class="headerlink" title="sliding window"></a>sliding window</h3><p>原始传感器数据基本上由三轴值和一个纳秒时间戳组成，表示采样的瞬间/时间。但是，我们不向分类器发送原始传感器数据。取而代之的是，我们将传感器的时间序列样本以一秒的帧进行分组，组成一个滑动时间窗口，然后总结出一个属性向量。随着时间的推移，窗口在时间序列上以1帧的增量滑动，如图3所示。我们认为$f_0$是当前秒的帧，$f_{−1}$是前一秒的帧，以此类推，$f_{−(nf−1)}$，其中$nf$(评价集的第五个元素)是构成滑动时间窗口的帧数。本文在这个评估中使用了以下<span class="inline-tag red"><strong>$nf$值:4、5、6、7和8</strong></span>。<br>这些值是通过实验定义的，因此滑动窗口可以容纳收集的驾驶事件的长度，根据事件的aggressive，收集的时间范围为 2 到 7 秒。</p><p>本文评估了几个评估程序集的性能，以找出最适合检测每种驾驶事件类型的程序集。装配数是15个数据集、5个不同的nf值、4个BN算法配置(表1)、5个MLP配置、6个RF配置和36个SVM配置的所有组合的结果。</p><p>这个结果总共<span class="inline-tag red">$15 \ast 5 \ast 4 + 15\ast5\ast5 + 15\ast5\ast6 + 15\ast5\ast36 = 3825$</span>个评估程序集</p><h2 id="Attribute-vector"><a href="#Attribute-vector" class="headerlink" title="Attribute vector"></a>Attribute vector</h2><p>属性向量是滑动窗口的汇总。每个包含驾驶员事件的时间窗口都会生成属性向量的一个实例。相应地，如果在特定时间窗口中没有驾驶员事件，则不会生成属性向量实例。</p><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/g003.png" alt="图3. 滑动窗口"></p><p>本文通过计算构成时间窗口的帧内传感器数据样本上的均值(M)、中值(MD)、标准差(SD)和增加/减少趋势(T)来创建属性向量的实例。向量中属性的数量依赖于滑动窗口(nf)中的帧数量。有$nf$个平均值，中值，标准偏差属性和$nf - 1$个趋势属性。</p><p>图4描述了传感器数据的单个轴的属性向量的结构。当数据集由多个轴组成时，每个轴的属性向量被简单地连接起来，并且只保留最后一个向量的class label属性，因为它们都具有相同的值。</p><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/g004.png" alt="图4. 汇总一个nf帧的滑动时间窗口的属性向量,i=nf-1"></p><p>向量的属性计算如下:<br>$$M_0 = M(f_0) \quad  … \quad  M_i = M(f_{-i},f_0) \tag{1}$$</p><p>$$MD_0 = MD(f_0) \quad  … \quad  MD_i = MD(f_{-i},f_0) \tag{2}$$</p><p>$$SD_0 = SD(f_0) \quad  … \quad  SD_i = SD(f_{-i},f_0) \tag{3}$$</p><p>$$T_1 = \frac {M(f_{-1})}{M(f_0)} \quad … \qquad T_i = \frac {M(f_{-i})}{M(f_0)} \tag{4}$$</p><p>其中</p><ul><li>$i = [0..(nf-1)]$</li><li>$SF(f_i)$ 是应用于第$j^{th}$帧样本的一个汇总函数$\text{(mean, median, or standard deviation)}$</li><li>$SF(f_j,f_k)$ 是应用于样本从第$j$个帧到第$k$个帧的汇总函数($j&lt;k$)</li></ul><p>向量的class label attribute 来自于 驾驶事件的ground-truth<br>类标签class label是driving event，它的开始时间戳在帧$f_{-(nf-1)}$中样本的第一个时间戳和帧$f_0$中样本的最后一个时间戳之间。需要指出的是，由于不同驾驶出行产生的属性向量是时间独立的，因此可以将它们分组在同一个数据集中。</p><p>单个驾驶事件原始传感器数据示例生成$nf$个属性向量实例。这是因为包含事件的开始时间戳的窗口框架在其滑动时改变了其在窗口中的位置。因此，如果有一个特定驾驶员事件类型的$s$个样本，那么该事件类型将有$s*nf$个属性向量实例。此行为允许多个窗口捕获同一事件的不同部分或签名。</p><h2 id="Data-collection-in-a-real-world-experiment"><a href="#Data-collection-in-a-real-world-experiment" class="headerlink" title="Data collection in a real-world experiment"></a>Data collection in a real-world experiment</h2><p>为了收集驾驶事件的传感器数据，本文进行了一个真实的实验。在这个实验中，一个Android应用程序在驾驶员执行特定驾驶事件时记录智能手机传感器数据。本文还记录了驾驶员事件的开始和结束时间戳，以生成实验的ground-truth。<br>本文进行了4次汽车出行的实验，每次平均约13分钟。<br>实验条件如下：</p><ol><li>the vehicle was a 2011 Honda Civic(这辆车是2011年的本田思域(Honda Civic))</li><li>the smartphone was a Motorola XT1058 with Android version 5.1(这款智能手机是摩托罗拉XT1058, Android版本为5.1)</li><li>the smartphone was fixed on the car’s windshield by means of a car mount, and was neither moved nor operated while collecting sensor data(智能手机通过汽车支架固定在汽车的挡风玻璃上，在收集传感器数据时既不移动也不操作)</li><li>the motion sensors sampling rate varied between 50 and 100 Hz, depending on the sensor(运动传感器的采样率根据传感器的不同在50到100赫兹之间变化)</li><li>two drivers with more than 15 years of driving experience executed the driving events(由两名具有15年以上驾驶经验的驾驶员执行驾驶事件)</li><li>the weather was sunny and the roads were dry and paved with asphalt. (天气晴朗，道路干燥，铺着柏油)</li></ol><p>本实验收集的驾驶事件类型以[13]中的事件为基础。<br>本文的目的是建立一组驾驶事件，这些事件代表了现实世界中常见的事件，如刹车、加速、转弯和变道。表2显示了本工作中使用的<span class="inline-tag red"><strong>7种驾驶事件类型</strong></span>及其收集的样本数量。</p><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/t002.png" alt="表2. 驾驶员事件类型和样本数量"></p><p>图5显示了评估中使用的四个传感器捕获的攻击性左变道事件的传感器数据。</p><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/g005.png" alt="图5. 评估中使用的四个传感器获取主动变道事件数据"></p><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>本文使用5个不同的nf值，在之前描述的15个数据集上执行表1中描述的4个MLAs及其配置的所有组合。本文用15种不同的随机种子训练、测试和评估每个评估集合。最后，我们计算了这些执行的AUC平均值，并根据驾驶员事件类型对它们进行分组，并在图6所示的箱线图中对5个性能最好的组件进行排序。该图左边显示了驾驶事件，右边显示了每个事件的5个最佳评估程序集，最优的程序集在底部。<br>图6中的组合文本标识按此顺序进行编码：</p><ol><li><span class="inline-tag blue"><strong>the nf value</strong></span></li><li><span class="inline-tag blue"><strong>the sensor and its axis( if there is no axis indication, then all sensor axes are used)</strong></span></li><li><span class="inline-tag blue"><strong>the MLA and its configuration identifier</strong></span> </li></ol><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/g006.png" alt="图6. 前5个最好的AUC程序集，根据驾驶员事件类型分组，作为使用不同的随机种子的15次MLA训练/测试执行的结果"></p><!-- <center><img src= "/img/loading.gif" data-lazy-src="/image/pages/g006.png" width="70%" height="70%" />Figure 6. 前5个最好的AUC程序集，根据驱动事件类型分组，作为15次MLA训练/测试执行的结果，使用不同的随机种子</center> --><p><strong>结果结论</strong></p><ol><li>MLAs在 <strong>较高的nf值</strong>(即较大的滑动窗口大小)下表现更好。在35个性能最好的组件中，有23个 nf = 8,有6个nf = 7,有5个nf = 6，只有1个nf = 4。</li><li><strong>陀螺仪和加速度计</strong>是本实验中最适合检测驾驶事件的传感器。另一方面，磁强计单独不适合事件检测，因为没有35个最好的组件使用该传感器。此外，使用所有的传感器轴比使用单个轴在一般情况下性能更好。唯一的例外是陀螺仪的z轴，它最好的检测侵略性的左转弯。</li><li><strong>RF</strong>是目前为止表现最好的MLA, 35个最好的组装中有28个。第二好的是MLP，有7个最好的结果。RF在非侵略性事件、侵略性转弯、断裂和加速中占据了前5名。然而，MLP更擅长侵略性的变道。BN和SVM在35个表现最好的assemblies中没有排名。</li><li>MLP配置#1的性能最好。在此配置中，隐含层神经元的数量被定义为(#attr. + #classes)/ 2。这也是缺省的WEKA配置。对于RF，配置#6(# of iterations = 200; # of attributes to randomly investigate= 15)和#5(# of iterations = 200; # of attributes to randomly investigate = 10)给出了最好的结果。</li><li>本文在排名前35位的评估程序集中发现了令人满意的和同等的性能。这是因为最坏的AUC平均值(aggressive breaking event为0.980)和最好的AUC平均值( aggressive right lane change event为0.999)之间的差只有0.018。这一差异在本实验背景下不显著。</li></ol><h1 id="Conclusions-and-future-work"><a href="#Conclusions-and-future-work" class="headerlink" title="Conclusions and future work"></a>Conclusions and future work</h1><p>在这项工作中，本文使用从4个Android智能手机传感器(加速度计、线性加速度、磁强计和陀螺仪)收集的数据，对4个MLAs (BN、MLP、RF和SVM)在检测7种驾驶事件类型中不同配置的性能进行了定量评估。本文在一个有2名驾驶员的真实世界实验中收集了69个此类事件类型的样本。<br>记录这些事件的开始和结束时间作为实验的ground-truth。本文还比较了应用不同滑动时间窗大小时的性能。</p><span class="inline-tag grey">$EA = \lbrace 1:sensor, \  2:sensor \ axis(es), \  3:MLA,\  4:MLA \ configuration,\  5:number \ of \ frames \ in \ sliding \ window\  \rbrace$</span><p>使用表单的3865个评估程序集的不同随机种子执行了15次执行</p><p>因此找到了每个驱动事件类型的前5个执行程序集。在实验中，这些结果表明</p><ol><li>滑动窗口尺寸越大表现越好</li><li>陀螺仪和加速计是探测驾驶事件的最佳传感器</li><li>一般来说，使用所有传感器轴比使用单个传感器轴表现更好，除了侵略性的左转事件</li><li>到目前为止，RF是表现最好的MLA，其次是MLP</li><li>前35种组合的性能均令人满意且相当，AUC均值在0.980 ~ 0.999之间变化。</li></ol><p>在未来的工作中，本文希望通过不同的车辆、Android智能手机型号、道路状况、天气和温度来收集更多的驾驶事件样本。本文还希望在评估中增加更多的MLAs，包括基于模糊逻辑和DTW的MLAs。最后，本文打算使用本研究中观察到的最佳评估程序集来开发一个Android智能手机应用程序，该应用程序可以实时检测驾驶事件并计算驾驶员行为概况。</p>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> driving behavior proﬁling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Various Approaches for Driver and Driving Behavior Monitoring:A Review</title>
      <link href="/posts/100001/"/>
      <url>/posts/100001/</url>
      
        <content type="html"><![CDATA[<p><strong>论文题目</strong>：驾驶员和驾驶行为监控的各种方法:综述</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>近年来，驾驶员困倦和注意力分散已经成为大量事故中的重要因素，因为它们降低了驾驶员的感知水平和决策能力，对车辆的控制能力产生了负面影响。减少这类事故的一种方法是通过监测驾驶员和驾驶行为，并在驾驶员昏昏欲睡或分心的状态下提醒驾驶员。此外，如果能够提前预测不安全的驾驶行为，这也将有助于安全驾驶。</p><p>本文将讨论对驾驶员和驾驶行为的各种监测方法以及对不安全驾驶行为的预测。</p><h3 id="驾驶员困倦的监测"><a href="#驾驶员困倦的监测" class="headerlink" title="驾驶员困倦的监测"></a>驾驶员困倦的监测</h3><p>讨论了驾驶员行为的视觉特征和非视觉特征，以及与车辆特征相关的驾驶行为。<br>视觉特征方面： 详细讨论了眼睛的相关测量、哈欠检测、面部表情等视觉特征测量<br>非视觉特征： 探索各种生理信号和利用这些信号可能的睡意检测方法。<br>基于车辆的特征： 描述了方向盘运动和横向位置的标准偏差。</p><h3 id="驾驶员的注意力分散的监测"><a href="#驾驶员的注意力分散的监测" class="headerlink" title="驾驶员的注意力分散的监测"></a>驾驶员的注意力分散的监测</h3><p>描述了头部姿势和注视方向的方法。</p><h3 id="不安全的驾驶行为的预测"><a href="#不安全的驾驶行为的预测" class="headerlink" title="不安全的驾驶行为的预测"></a>不安全的驾驶行为的预测</h3><p>解释了基于面部表情和汽车动力学的预测方法</p><h3 id="主动驾驶安全系统需要解决的几个问题"><a href="#主动驾驶安全系统需要解决的几个问题" class="headerlink" title="主动驾驶安全系统需要解决的几个问题"></a>主动驾驶安全系统需要解决的几个问题</h3><ol><li>检测睡意的混合方法</li><li>安全驾驶的驾驶上下文感知</li><li>需要公共数据集的模拟和真实驾驶条件。</li></ol><h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><p><strong>原因</strong><br>司机困倦(<strong>drowsiness</strong>)和注意力分散(<strong>distraction</strong>)是交通事故的主要原因</p><p><strong>解决方法</strong><br>设计一个框架包含两个阶段：<strong>监控和预测驾驶员和驾驶行为</strong></p><h3 id="监测困倦状态"><a href="#监测困倦状态" class="headerlink" title="监测困倦状态"></a>监测困倦状态</h3><p><strong>视觉特征</strong></p><p>驾驶员疲劳或者困倦可能与眼运动(eye movement)、面部表情(facial expression)、心跳和呼吸频率(heart and breathing rate)以及大脑活动(brain activity)的症状相关。<br>为了检测驾驶员的困倦，视觉特征(visual features)比如眼运动和面部表情是非常重要的;<br>打哈欠的测量(yawning measurement)</p><p><strong>非视觉特征</strong></p><p>非视觉特征(non-visual features),间接评估驾驶员的等级可以考虑下面几个方面：</p><ul><li>心率变异率 heart rate variability (HRV)</li><li>皮肤电反应 galvanic skin response (GSR)</li><li>传导性 conductivity</li><li>方向盘控制压力 steering-wheel grip pressure</li><li>体温 body temperature</li><li>Electroencephalogram 脑电图(EEG)和 Electro-oculogram 眼电图(EoG)提供关于困倦或情绪反应[13]的额外的心理生理学信息。</li></ul><p><strong>行为信息</strong></p><p>驾驶行为信息如方向盘运动(steering wheel movement)、车道保持(lane keeping)、加速踏板运动(acceleration pedal movement)、制动(braking)等也应被考虑来检测驾驶员是否困倦。</p><h3 id="监测注意力分散"><a href="#监测注意力分散" class="headerlink" title="监测注意力分散"></a>监测注意力分散</h3><p>视觉分心指的是“眼离路”的状态，而认知分心被描述为“脑离路”的状态。<br>为了检测驾驶员的注意力分散，需要有效地提取驾驶员 <strong>头部姿态或注视信息</strong> 。</p><p>通过仔细监测驾驶员和驾驶表现行为，可以预测 <strong>小事故和大事故(minor and major accidents)</strong></p><ul><li>Jabon et al.[15]在事故发生前的不同时间段里识别了一套驾驶员的关键面部特征，并用它们来预测小事故和大事故。这种方法对于主动驾驶安全系统的设计是非常重要的，以防止事故。</li><li>瑞士EPFL的信号处理实验室[16]通过分析面部表情和肌肉运动来检测驾驶员的分心以及情绪，这些情绪可能表明驾驶员不能胜任手头的工作。</li></ul><p>本文的 <strong>目的</strong>：探讨监控驾驶员的状态以及预测不安全驾驶行为。</p><p>开发一个包括两个阶段的框架的几个问题:监测和预测驾驶员和驾驶行为。</p><h2 id="Driver-drowsiness-measurement"><a href="#Driver-drowsiness-measurement" class="headerlink" title="Driver drowsiness measurement"></a>Driver drowsiness measurement</h2><div class="note primary no-icon"><p>这一节主要是司机困倦状态的监测，从视觉特征和非视觉特征两个角度来讨论驾驶员的行为特征，并且讨论了基于车辆的特征相关的驾驶行为。视觉特征方面，这节详细讨论了与眼睛相关的测量方法，如PERCLOS、呵欠检测以及目前在视觉特征测量中的一些局限性。非视觉特征方面，这节探索用于检测睡意的生理信号。基于车辆的特征，本节描述了方向盘运动和横向位置的标准偏差。 </p></div><p><strong>监测困倦状态的原因</strong></p><p>微睡眠(micro-sleep)的持续时间可以在几秒到30秒甚至更多之间。因此，驾驶员的困倦状态，即从清醒到睡眠的过渡状态，应该被监测。</p><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/002.png" alt="Summary of various features for detecting and predicting driver drowsiness"></p><p>上图是监测司机困倦状态的特征总结。</p><p>驾驶员行为信息包括 <strong>视觉特征</strong>和 <strong>非视觉特征</strong>。</p><p><strong>视觉特征</strong></p><ul><li>闭眼(eye closure)</li><li>眨眼(eye blinking)</li><li>打哈欠(yawning)</li><li>头部姿势(head pose)</li><li>面部表情(facial expression)<br>可以增加的：</li><li>眨眼频率(frequency of eye blinking)：衡量疲劳程度的良好指标</li><li>眼睑张开程度(degree of eyelid opening)：衡量疲劳程度的良好指标</li></ul><p><strong>非视觉特征</strong></p><ul><li>心率(heart rate)</li><li>脉搏率(pulse rate)</li><li>大脑活动(brain activity)。<br>可以增加一些生理信号:</li><li>心电图(electrocardiogram ECG)</li><li>肌电图(electromyogram EMG)</li><li>眼电图(electro-oculogram EoG)</li><li>脑电图(electroencephalogram EEG))</li></ul><p><strong>驾驶行为信息</strong></p><ul><li>车道位置偏差(deviations from lane position)</li><li>车速(vehicle speed)</li><li>转向运动(steering movement)</li><li>加速踏板压力(pressure on the acceleration pedal)等</li></ul><h3 id="Visual-features"><a href="#Visual-features" class="headerlink" title="Visual features"></a>Visual features</h3><div class="note primary no-icon"><p>这一节主要是从视觉特征角度来讨论驾驶员的行为特征，详细讨论了与眼睛相关的测量方法PERCLOS,并介绍了一下目前的打呵欠检测以及目前在视觉特征测量中的一些局限性。</p></div><p>从视觉方面主要是看监测面部动作，面部动作(facial movements)包括下面三个方面：</p><ul><li>眨眼(eye blinking)</li><li>频繁打哈欠(frequent yawning)</li><li>点头或者摆动头(nodding or swinging head)</li></ul><div class="note info no-icon"><p>对于眨眼可以采用PERCLOS方法。</p></div><p><strong>PERCLOS (Percent Eye Closure)</strong></p><ul><li>对司机警觉性水平的可靠和有效的测定</li><li>PERCLOS是驾驶员眼睑闭合时间占瞳孔总时间的80%(或以上)，也反映了眼睑闭合缓慢。？？</li><li>当PERCLOS超过预定的阈值，提出的系统产生瞌睡警告。</li><li>其缺点是，有些情况监测不到：有时试图保持清醒的司机可能睁着眼睛睡着了。</li></ul><p>计算PERCLOS需要提取包括瞳孔面积在内的眼睛区域<br>在提取这些视觉特征方面存在一些限制：合适的照明(proper lighting)</p><p>监测司机是否困倦应该考虑真实情况：</p><ul><li><strong>白天和晚上</strong>：一个简单的CCD或网络相机在白天使用，而红外相机在晚上使用</li><li>司机 <strong>是否戴眼镜</strong>：<ol><li>需要找到合适的近红外(Near IR  NIR)光照波长【一个可能的候选波长是850nm。在真实的汽车环境中，反射的阳光也产生在眼镜的外表面。为了减少反射效果，Jo等[34]使用了带窄带通滤光片的近红外光源，将入射光的波长限制在850nm。这是因为高功率LED照明灯比车内的阳光更强大。】</li><li>用于商业产品如视觉机器[35]的faceLAB。利用被动式摄像机对视频图像进行实时处理，确定各特征的三维位置。该系统能够确定一个精确的3D头部姿态和计算眼睛注视方向。它的优点包括能够很好地应对光线不足的情况，以及司机戴着太阳镜时头部的运动。</li></ol></li></ul><div class="note info no-icon"><p>打哈欠可以通过测量驾驶员口腔轮廓的变化速率和变化量来检测[7,11]。</p></div><div class="note info no-icon"><p>头部姿态估计和头部运动检测(如点头)在监控驾驶员机警性方面也很重要[36,37]</p></div><div class="note info no-icon"><p>此外，司机的面部皱纹出现在眉毛、嘴部和鼻唇沟，这些都是很好的身体信号，表明困倦被抑制了，因此困倦就出现了。</p></div><h3 id="Non-visual-features"><a href="#Non-visual-features" class="headerlink" title="Non-visual features"></a>Non-visual features</h3><div class="note primary no-icon"><p>这一节主要是从非视觉特征角度来讨论驾驶员的行为特征，主要是通过生理信号的检测来识别，并考虑其侵入性特点的局限性。</p></div><p><strong>原因</strong></p><p>非视觉特征或生理信号，如心率和大脑活动，在预测困倦方面是有用的，与视觉特征相比，假阳性更少，因为只有在司机很好地在睡觉的路上，才能从视觉特征判断困倦状态。即基于这些生理信号的困倦预测使我们有可能及时地向困倦的司机发出警告。</p><p><strong>生理信号</strong></p><ul><li>心电图(ECG)<ul><li>从心电图信号中可以提取出心率(HR);心率可以用来检测睡意，因为它在警觉性和睡意状态之间存在显著差异[19,39]。</li><li>心率变异性(HRV)测量每一拍心跳的变化，也可用来检测睡意。随着司机从警觉状态到昏昏欲睡状态，心电图信号中低频与高频的比率逐渐降低[20,40]。</li></ul></li><li>脑电图(EEG)</li><li>肌电图(EMG)</li><li>眼电图(EoG)</li><li>光容积描记(PPG)</li></ul><p><strong>过程</strong></p><ol><li>处理生理信号的一个 <strong>关键问题</strong>是消除真实环境中不可避免的噪声和人为因素。</li><li>在有效滤波之后，采用了快速傅立叶变换(FFT)和离散小波变换(DWT)等特征提取技术。</li><li>然后，利用支持向量机(SVM)、人工神经网络(ANN)、线性判别分析(LDA)等方法对提取出来的特征进行分类[40-43]</li></ol><p><strong>局限性</strong></p><p>优点：检测驾驶员睡意的可靠性和准确性比可见特征高<br>重要局限性：侵入性</p><p><strong>解决方案</strong></p><ul><li>使用无线技术，如Zigbee和Blutooth，通过将电极放置在方向盘或驾驶员座位上，以非侵入性的方式测量生理信号[44,45]。最后，信号由智能手机处理，确定司机瞌睡[46]。然而，由于电极接触不当，这种非侵入式系统与侵入式系统相比精度较低。</li><li>为了获得可靠的驾驶员嗜睡检测结果，人们尝试融合各种测量结果[20,47]。将PERCLOS、ECC和EEG混合用于检测驾驶员嗜睡，其成功率高于单独测量[20]。Cheng等人[47]使用PERCLOS融合、眨眼频率、最大闭合时间和非转向百分比等方法检测睡意。</li></ul><h3 id="Driving-behavior-features"><a href="#Driving-behavior-features" class="headerlink" title="Driving behavior features"></a>Driving behavior features</h3><div class="note primary no-icon"><p>这一节主要是从车辆特征的驾驶行为特征角度，描述了方向盘运动和横向位置的标准偏差</p></div><p>驾驶行为特征或驾驶性能指标包括方向盘运动、车道保持、加速踏板运动和制动等[48-50]。这些特征与车辆类型以及驾驶员在驾驶习惯、驾驶技能和驾驶经验方面的差异性有关。检测驾驶员睡意水平最常用的两种驾驶行为指标是 <strong>方向盘运动和横向位置的标准偏差</strong>。</p><h2 id="Driver-distraction-detection"><a href="#Driver-distraction-detection" class="headerlink" title="Driver distraction detection"></a>Driver distraction detection</h2><div class="note primary no-icon"><p>这一节 描述了一些与驾驶员分心测量相关的问题，特别是头部姿势和注视方向方法。</p></div><p>暂无</p><h2 id="Predicting-unsafe-driving-behavior"><a href="#Predicting-unsafe-driving-behavior" class="headerlink" title="Predicting unsafe driving behavior"></a>Predicting unsafe driving behavior</h2><div class="note primary no-icon"><p>这一节介绍了不安全驾驶行为的预测方法并解释了基于面部表情和汽车动力学的预测方法。</p></div><p>对驾驶员状态、驾驶行为性能和车辆状态的监测对于提高驾驶员主动安全系统的性能是非常重要的。</p><p>驾驶员的状态通过测量困倦、疲劳或压力水平来监测[61-64]。通过分析驾驶速度、方向盘角度、制动和加速等信息，还可以监测驾驶行为、性能和车辆状态[48- 50,65,66]。在检测到困倦或注意力分散后，就会向司机发送警报。</p><p>主动驾驶员安全系统的另一个 <strong>重要问题</strong>是开发一种机制来 <strong>提前预测小事故和大事故</strong>。</p><p><strong>基于面部表情和汽车动力学的预测方法</strong><br>Jabon等人[15]使用面部特征来帮助预测驾驶员事故。他们结合了车辆动力学和驾驶员面部分析来预测事故。</p><p><strong>过程</strong></p><ol><li>首先，对22个原始面部特征进行综合分析。</li><li>然后从一组时域和频域的数值中提取出对事故预测最有价值的统计量，从而实现对重大和轻微事故的预测。</li></ol><p><strong>结果</strong><br>虽然Jabon et al.[15]的实验结果并不是基于真实的道路情况，但已经发现面部特征在事故发生前4秒表现出最准确的预测能力，并且在预测小事故时比预测大事故更有帮助。这是因为对重大事故的预测精度主要来自车辆的特征，而不是面部特征。</p><p><strong>新技术</strong><br>EPFL and PSA Peugeot Citroen[16]正在开发一种技术，以检测司机的分心以及情绪，表明司机不能胜任手头的任务。即，面部表情和肌肉运动在分析司机是否太过分散注意力、太累或甚至太过愤怒而无法安全控制车辆时非常重要。</p><p><strong>局限性</strong><br>虽然面部特征已被证明有助于预测小事故，但其预测效果还有待提高。</p><p>为了更准确地预测事故，有必要捕捉来自驾驶员或驾驶员环境系统其他部分的其他生理信号。在此基础上，可以提出一种新的交通事故预测模型。特别是在构建更广泛、更通用的事故预测模型时，需要考虑不同的参与者群体、交通文化和驾驶环境。</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><div class="note primary no-icon"><p>这一节讨论了主动驾驶员安全系统的一些问题：<br>1)检测睡意的混合方法<br>2)安全驾驶的驾驶上下文感知<br>3)需要用来模拟的公共数据集以及真实驾驶条件</p></div><h3 id="Hybrid-measures-for-drowsiness-detection"><a href="#Hybrid-measures-for-drowsiness-detection" class="headerlink" title="Hybrid measures for drowsiness detection"></a>Hybrid measures for drowsiness detection</h3><div class="note primary no-icon"><p>检测睡意的混合方法</p></div><p><strong>目前监测司机睡意的存在的问题</strong></p><ol><li>在驾驶行为和驾驶员行为特征中，驾驶行为有时不能可靠地检测驾驶员的嗜睡。</li><li>驾驶员行为特征优于驾驶行为特征，但视觉特征有时会受到光照条件和驾驶员姿态[34]的限制。</li><li>非视觉特征，如生理特征是可靠和准确的，但其本质是侵入性的。在它们能够在真实的车辆环境中使用之前，应该解决侵入性这个问题。虽然已经开发了一种侵入性较小的ECG测量方法[45]，但EEG和EoG仍然需要以侵入性的方式将电极放置在头皮或眼睛区域。然而，非侵入性的生理信号测量可能在不久的将来发展。</li></ol><p><strong>混合方法</strong></p><p>融合 <strong>视觉</strong>、<strong>生理</strong>和 <strong>驾驶行为特征</strong>的混合测量</p><p>即使在某些传感器失效的情况下，融合方法也能很好地检测出睡意。图4显示了一个用于驱动状态检测的混合测量示例。其中一个问题是在特征级或决策级[20]上开发一种可靠的数据融合方法。</p><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/003.png" alt="用于检测驱动程序状态的混合测量"></p><h3 id="Driving-context-awareness-for-safe-driving"><a href="#Driving-context-awareness-for-safe-driving" class="headerlink" title="Driving context-awareness for safe driving"></a>Driving context-awareness for safe driving</h3><div class="note primary no-icon"><p>安全驾驶的驾驶上下文感知</p></div><p>为了安全驾驶，驾驶上下文感知是必要的，需要有效地探索与驾驶条件和环境相关的各种信息。</p><p>将驾驶上下文划分为全局和局部。</p><ol><li><strong>全局</strong>驾驶上下文是指车辆类型、道路类型、驾驶时间、驾驶环境、路况等等</li><li><strong>局部</strong>(local)驱动上下文指的是驾驶员状态(driver status)。即局部语境与驾驶员的视觉和认知知觉以及驾驶员因分心、嗜睡和/或情绪而导致的视觉和认知知觉的恶化有关。</li></ol><p>发展方向：结合 <strong>驾驶环境</strong>和 <strong>驾驶员状态</strong>的基于驾驶上下文的计算模型。使用这样的模型，注意力分散和嗜睡的检出率将会增加，这将有助于预测不安全驾驶行为。</p><h3 id="Necessity-for-public-data-sets-for-simulation-and-real-driving-conditions"><a href="#Necessity-for-public-data-sets-for-simulation-and-real-driving-conditions" class="headerlink" title="Necessity for public data sets for simulation and real driving conditions"></a>Necessity for public data sets for simulation and real driving conditions</h3><div class="note primary no-icon"><p>需要用来模拟的公共数据集以及真实驾驶条件</p></div><p>在真实的驾驶环境中测试困倦有危险。</p><p><strong>真实驾驶条件的必要性</strong><br>Philp等人[67]发现，由于体验的单调性，在模拟环境中，来自自我评价的反应时间和嗜睡程度比在真实驾驶环境中要高。<br>Engstorm等[68]指出，真实驾驶条件下的生理负荷和转向活性均高于模拟环境。在真实的驾驶条件下，包括灯光和噪音的变化在内的各种因素也会影响驾驶员的注意力。</p><p><strong>没有可用的基准数据集</strong></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这篇文章回顾了各种可用的方法来确定困倦和分心的司机的状态。通过对驾驶员视觉特征、非视觉特征和驾驶行为行为等驾驶员行为的研究来检测驾驶员困倦。</p><ol><li>PERCLOS、闭眼时间(ECD)、闭眼频率(FEC)是一种基于视觉特征的检测驾驶员睡意的系统。<br>其中，PERCLOS在检测睡意方面表现良好，但存在光照条件等局限性。为了克服这个问题，使用了850nm的IR光源。</li><li>生理信号如ECG、EEG、EoG和PPG信号作为非视觉特征检测驾驶员困倦。尽管生理信号比视觉特征表现出更好的表现，但它们也有一些局限性，尤其是它们的侵入性。为了克服这个问题，应该开发侵入性较小的传感器。目前，ECG信号可以用一种较少干扰的方式捕获。驾驶性能行为如方向盘运动和侧位标准偏差也被用来检测困倦。</li><li>驾驶员注意力分散是通过头部姿势和注视方向来检测的。驾驶员的分心可能会导致更大的车道变化，更慢的对障碍的反应，以及更突然的转向控制。因此，为了开发一个更安全的驾驶员监控系统，我们应该监控驾驶员的分心。</li><li>对于主动驾驶安全系统而言，预测不安全驾驶行为是一种可取的方法。本文解释了基于面部表情和汽车动力学的预测方法。通过面部表情检测驾驶员的情绪，有助于预测驾驶员的驾驶行为。</li><li>最后本文讨论了主动驾驶安全系统未来发展中需要解决的几个问题。它们是a)检测睡意的混合措施，b)安全驾驶的驾驶环境感知，c)模拟和真实驾驶条件的公共数据集的可用性。</li></ol>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> driving behavior proﬁling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dataset</title>
      <link href="/posts/60504/"/>
      <url>/posts/60504/</url>
      
        <content type="html"><![CDATA[<h2 id="公开的数据集"><a href="#公开的数据集" class="headerlink" title="公开的数据集"></a>公开的数据集</h2><h3 id="SHRP2"><a href="#SHRP2" class="headerlink" title="SHRP2"></a>SHRP2</h3><h4 id="数据集获取方式"><a href="#数据集获取方式" class="headerlink" title="数据集获取方式"></a>数据集获取方式</h4><p><a href="https://insight.shrp2nds.us/" target="_blank" rel="noopener">https://insight.shrp2nds.us/</a></p><h3 id="Security-dataset"><a href="#Security-dataset" class="headerlink" title="Security dataset"></a>Security dataset</h3><h4 id="数据集获取方式-1"><a href="#数据集获取方式-1" class="headerlink" title="数据集获取方式"></a>数据集获取方式</h4><p><a href="https://ocslab.hksecurity.net/Datasets/driving-dataset" target="_blank" rel="noopener">https://ocslab.hksecurity.net/Datasets/driving-dataset</a></p><h4 id="数据集的描述"><a href="#数据集的描述" class="headerlink" title="数据集的描述"></a>数据集的描述</h4><ul><li><p>Driving time : about 23 hours</p></li><li><p>Driving length : about 46km (round-trip)</p></li><li><p>Driving path : between Korea University and SANGAM World Cup Stadium</p></li><li><p>#of Driver : 10 (A to J classes in the dataset and our paper.) </p></li><li><p>数据收集是在韩国进行的，使用的是起亚汽车公司的一款最新车型。</p></li><li><p>实验设置由城市道路、高速公路和停车位三种类型的四条道路组成，总长度为23公里。</p></li><li><p>城市道路有信号灯和人行横道，而高速公路没有。在停车位上，司机开车要慢，要小心。</p></li><li><p>试验于2015年7月28日开始。时间因素是通过在类似的时区(工作日晚上8点到11点)进行实验来控制的。</p></li><li><p>司机完成了两个来回为一个可靠的分类。每个司机的驾驶数据从A到j。每秒总共捕获94,401条记录，从而生成16.7 MB的数据集。</p></li><li><p>数据通过车载诊断2 (OBD-II)和CarbigsP (OBD-II扫描仪）从车辆的CAN总线上收集。使用的车辆有许多测量传感器和控制传感器，由电子控制单元(ECU)管理。</p></li></ul><h3 id="UAH‑DriveSet"><a href="#UAH‑DriveSet" class="headerlink" title="UAH‑DriveSet"></a>UAH‑DriveSet</h3><h4 id="数据集获取方式-2"><a href="#数据集获取方式-2" class="headerlink" title="数据集获取方式"></a>数据集获取方式</h4><p><a href="http://www.robesafe.uah.es/personal/eduardo.romera/uah-driveset/" target="_blank" rel="noopener">http://www.robesafe.uah.es/personal/eduardo.romera/uah-driveset/</a></p><h4 id="数据集可视化"><a href="#数据集可视化" class="headerlink" title="数据集可视化"></a>数据集可视化</h4><p><a href="https://github.com/Eromera/uah_driveset_reader" target="_blank" rel="noopener">https://github.com/Eromera/uah_driveset_reader</a></p><h4 id="数据集描述"><a href="#数据集描述" class="headerlink" title="数据集描述"></a>数据集描述</h4><ul><li>UAH-DriveSet是由驾驶监控应用DriveSafe在不同环境下的不同测试人员捕获的数据的公共集合。</li><li>数据集由6名不同年龄和车辆的驾驶员收集，其中包括一辆全电动汽车。(具体见下图)</li><li>3个行为(正常,昏昏欲睡和侵略性)进行了在两个不同的路线(高速公路和二级公路)<ul><li>25公里(往返)高速公路的道路通常3车道在每个方向和最大允许速度120公里/小时,</li><li>16公里左右的二级公路,通常一个车道在每个方向和最大允许速度约为90公里/小时。</li></ul></li><li>超过500分钟的自然驾驶相关的原始数据和额外的语义信息,一起旅行的录像。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/image/pages/001.png" alt="司机和车的列表信息"></p><h4 id="数据集的引用"><a href="#数据集的引用" class="headerlink" title="数据集的引用"></a>数据集的引用</h4><blockquote><p>E. Romera, L.M. Bergasa and R. Arroyo, “Need Data for Driving Behavior Analysis? Presenting the Public UAH-DriveSet”, IEEE International Conference on Intelligent Transportation Systems (ITSC), pp. 387-392, Rio de Janeiro (Brazil), November 2016. <a href="http://www.robesafe.uah.es/personal/eduardo.romera/pdfs/Romera16itsc.pdf" target="_blank" rel="noopener">pdf</a></p></blockquote><h3 id="HciLab-dataset"><a href="#HciLab-dataset" class="headerlink" title="HciLab dataset"></a>HciLab dataset</h3><h4 id="数据集获取方式-3"><a href="#数据集获取方式-3" class="headerlink" title="数据集获取方式"></a>数据集获取方式</h4><p><a href="https://www.hcilab.org/research/hcilab-driving-dataset/." target="_blank" rel="noopener">https://www.hcilab.org/research/hcilab-driving-dataset/.</a></p><h4 id="数据集描述-1"><a href="#数据集描述-1" class="headerlink" title="数据集描述"></a>数据集描述</h4><ul><li>来源自德国斯图加特大学可视化和交互系统研究所hciLab集团</li><li>数据中包含了10名参与者，压缩包数据共37.9MB的大小</li></ul><h4 id="数据集的引用-1"><a href="#数据集的引用-1" class="headerlink" title="数据集的引用"></a>数据集的引用</h4><blockquote><p>Stefan Schneegass, Bastian Pfleging, Nora Broy, Albrecht Schmidt, and Frederik Heinrich. 2013. A data set of real world driving to assess driver workload. In Proceedings of the 5th International Conference on Automotive User Interfaces and Interactive Vehicular Applications (AutomotiveUI ’13). ACM, New York, NY, USA, 150-157. DOI=10.1145/2516540.2516561 <a href="http://doi.acm.org/10.1145/2516540.2516561" target="_blank" rel="noopener">http://doi.acm.org/10.1145/2516540.2516561</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> dataset </category>
          
      </categories>
      
      
        <tags>
            
            <tag> driving behavior proﬁling </tag>
            
            <tag> dataset </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文调研</title>
      <link href="/posts/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/"/>
      <url>/posts/%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/</url>
      
        <content type="html"><![CDATA[<h1 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h1><p>总共9篇论文</p><ol><li><a href="#data-driven-robust-scoring-approach-for-driver-proﬁling-applications">Data-driven Robust Scoring Approach for Driver Proﬁling Applications</a></li><li><a href="">Driver Behavior Detection Techniques: A survey</a></li><li><a href="">Driver behavior profiling: An investigation with different smartphone sensors and machine learning</a></li><li><a href="">Driver Behavior Profiling Using Smartphones:  A Low-Cost Platform  for Driver Monitoring</a></li><li><a href="#driver-behaviour-proﬁles-for-road-safety-analysis">Driver behaviour proﬁles for road safety analysis</a></li><li>[Investigations on Driver Unique Identification from Smartphone’s GPS Data Alone](#Investigations on Driver Unique Identification from Smartphone’s GPS Data Alone)</li><li><a href="">Know Your Master: Driver Proﬁling-based Anti-theft Method</a></li><li><a href="">Various Approaches for Driver and Driving Behavior Monitoring: A Review</a></li><li><a href="">Who is behind the wheel? Driver identification and fingerprinting</a></li></ol><p>其中涉及的所有公开数据集的下载地址以及详细描述点击<a href="/posts/60504/" title="dataset">dataset</a><br>其中2、3(大概算是)、5、8是综述类文章；6、7、9涉及驾驶员识别</p><h1 id="Data-driven-Robust-Scoring-Approach-for-Driver-Proﬁling-Applications"><a href="#Data-driven-Robust-Scoring-Approach-for-Driver-Proﬁling-Applications" class="headerlink" title="Data-driven Robust Scoring Approach for Driver Proﬁling Applications"></a>Data-driven Robust Scoring Approach for Driver Proﬁling Applications</h1><p>驾驶分析行为：driving behavior proﬁling<br>驾驶员分析过程由两个子过程组成：</p><ol><li>第一种是通过从车载设备(如智能手机和OBDII装置)获取数据来检测某些驾驶行为</li><li>第二种是通过对检测到的行为进行评分的过程来衡量实际的驾驶风险。</li></ol><p><strong>本文的解决方法</strong></p><ul><li>本文提出了一种数据驱动的方法来计算司机的风险分数</li><li>利用SHRP2自然驾驶数据集，这是迄今为止最大的此类数据集。</li><li>训练了两种机器获取算法，即支持向量回归(SVR)和决策树回归(DTR)来反映驾驶员的分数。</li><li>驾驶员的分数是根据预测风险概率的加法逆元来量化的。</li><li>经过数据滤波和预处理，使用代表12种独特驾驶行为和每个驾驶员总驾驶时间的13个预测因子对模型进行训练。验证结果表明，该模型可以准确地预测风险概率。</li></ul><p><del>通常会根据收集到的数据计算每次出行的不同的系数Figures of Merit(FOMs)。</del><br><del>保险公司用来评估风险评分用四种驾驶行为FOMs:制动，超速，加速和转弯行为。</del><br><del>为每个FOM分配不同的权重</del></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>SHRP2自然驾驶研究(NDS)数据集提供了近9000个记录的撞车和接近撞车事件和超过20000个平衡基线事件(即，正常驾驶事件与每个司机的驾驶总数成比例)的大量驾驶上下文数据[5]。</p><p>收集到的数据可以研究危险事件中行为因素的发生率，还包括他们在正常驾驶期间的发生率</p><h2 id="本文的贡献："><a href="#本文的贡献：" class="headerlink" title="本文的贡献："></a>本文的贡献：</h2><ol><li>它提供了一个可靠的数据驱动框架，利用基线、碰撞和接近碰撞事件期间的行为背景信息来预测驾驶员的风险概率。为了实现这一点，12个行为风险预测因子被识别，特征矩阵被制定。驾驶分数用预测风险概率的加性逆表示。</li><li>采用支持向量回归(SVR)和决策树回归(DTR)两种机器学习算法来反映驾驶员的预测风险概率。通过不同的测试样本，比较了两种算法的平均性能和性能一致性。该算法的训练和测试使用了前所未有的数据量，超过2000名司机。</li><li>一个重要的发现是，在适当的采样时间内，只需捕捉少量的事件，就可以准确地预测驾驶风险(平衡的基线事件)。因此，不需要连续采集驾驶数据来确定某一驾驶员的相关风险。这有助于最小化将驾驶数据分流到云服务器所消耗的能量，并最小化预测驾驶风险的计算成本。</li></ol><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><ol><li>本文提出了一种基于数据驱动的健壮的用于驾驶员分析应用程序的风险评分的计算框架。<br>采用预测风险概率表示驾驶员的风险评分</li><li>利用来自SHRP2数据集的2000多名驾驶员的行为驾驶上下文信息和总曝光时间，设计并比较了两种风险预测模型。</li><li>采用一般的分裂法和10倍交叉验证法两种模型训练方法，结果表明，这些模型能够准确地预测风险概率。</li><li>一个重要的发现是，只需在适当的采样时间内捕获少数事件，就可以准确地预测某个司机的驾驶风险。</li><li>SVR模型似乎在所有性能度量上都优于DTR；对于不同的训练/测试样本，DTR的一致性优于SVR</li></ol><p>驾驶员行为分析的两大流派：</p><ol><li>驾驶员行为检测和分类。这包括检测某些事件，如:攻击性加速，攻击性变道等等。</li><li>开发一个评分函数，准确地反映已知的行为[3]，[10]的风险率。</li></ol><p>1多2少，因为评分函数的选择往往是比较主观的。缺乏大规模和可靠的数据集。</p><h2 id="数据集："><a href="#数据集：" class="headerlink" title="数据集："></a>数据集：</h2><p><a href="https://insight.shrp2nds.us/" target="_blank" rel="noopener">SHRP2 NDS Dataset</a></p><p>naturalistic driving (ND) data:自然驾驶数据</p><p>三个重要的优点：</p><ol><li>关于司机在撞车或接近撞车事件前行为的详细信息。</li><li>曝光信息，提供了在正常驾驶事件中不同驾驶行为发生频率的重要信息。</li><li>为进行统计上合理的研究铺平道路的收集数据的数量。</li></ol><h3 id="数据集描述："><a href="#数据集描述：" class="headerlink" title="数据集描述："></a>数据集描述：</h3><p>[SHRP2NDS]<a href="https://insight.shrp2nds.us/" target="_blank" rel="noopener">https://insight.shrp2nds.us/</a></p><p>在SHRP2NDS中，3542名驾驶员在美国6个不同的地点被招募，他们的车辆装备了不显眼的数据采集系统(DASs)，主要包括前向雷达传感器、摄像机、OBD单元来获取车辆的CAN总线信息和全球定位系统(GPS)。这是迄今为止有记录的最大数量的自然驾驶数据。数据还原人员随后能够提取出近9000个由崩溃和接近崩溃组成的风险事件。</p><p>此外，在超过20,000次的正常驾驶事件中随机捕获，为驾驶员提供暴露信息。<br>这些事件被称为平衡基线事件，因为它们的数量与驾驶员的总驾驶时间成正比。<br>VTTI数据分析人员在记录事件期间一共识别了59种驾驶上下文行为属性。<br>这些事件的每种类型的操作定义可以在[5]中找到，具体如下:</p><ol><li>Crash：sd与移动或固定物体(车辆、行人、骑自行车的人、动物等)的任何接触。还包括意外偏离道路。</li><li>Near crash: 任何需要规避动作以避免碰撞的驾驶冲突。</li><li>Balanced baseline events: 选择提供曝光信息的数据次数。它们的长度是21秒，它们的数量与每个司机的总驾驶时间成比例。</li></ol><p>VTTI数据分析人员利用收集到的数据提取并记录了事故发生前/接近事故发生前或基线事件期间的主要驾驶行为。</p><h1 id="Driver-Behavior-Detection-Techniques-A-survey"><a href="#Driver-Behavior-Detection-Techniques-A-survey" class="headerlink" title="Driver Behavior Detection Techniques: A survey"></a>Driver Behavior Detection Techniques: A survey</h1><div class="note danger"><p>综述类文章，后续需要详细看一下，下面还未总结</p></div><p>影响驾驶员行为的因素有疲劳、分心、经验、环境条件、车辆状况等。</p><p><strong>driving style：驾驶风格</strong></p><p>本文讨论了几种被提出的检测驾驶员行为的方法，并确定每种方法的优缺点。</p><p>本文将回顾近年来基于不同参数的驾驶员行为检测方法的研究，以确定适合的驾驶员行为检测方法。<br>用于驾驶员行为检测系统的不同技术，如高级驾驶员辅助系统(ADAS)、模拟器、远程安装摄像头等。</p><p><del>驾驶监控系统技术</del><br><del>驾驶员行为的分类是一个复杂的问题，因为它是一个多维的问题，并且受到驾驶员和交通状态[9]的几个特性的影响。交通状态由一组变量推导，如道路条件、车辆运动学和驾驶员行为[51]。</del><br><del>驾驶风格的评价和识别需要考虑不同的因素，如环境因素、道路状态和车辆[11]、事件分类和识别[12]以及生物生理状态[10]。</del></p><p>In-Vehicle Data Recording Systems 车载数据记录系统<br>Smartphone-based sensing in vehicles 基于智能手机的汽车传感系统<br>real time systems 实时系统</p><p>行为检测方法<br>在[40]中，他们在车上安装了CAN总线来采集数据。采用统计方法:隐马尔可夫模型(HMM)和高斯混合模型(GMM)进行检测。基于一个参数(速度)，他们使用Matlab分析数据，比较不同事件的速度，如变道、超速、停车和稳定驾驶，所有分心和自然驾驶。他们发现HMM比GMM得到的结果更准确。此外，他们还发现，在诸如超车等危险事件中分心驾驶会降低车速。然而，他们在所有事件中都有很高的误报概率。在[41]中，基于CAN-BUS，他们从真实驾驶旅行中收集了不同的数据，他们使用统计方法来检测驾驶行为，如在不可控条件下的均值、中位数和标准差。</p><p>神经网络的优缺点<br>缺点：</p><ol><li>很难分析，因为它们编码的信息不容易解释。</li><li>大多数神经网络不能处理数据点的时间序列，而只能一次计算一个数据向量的输出。在驾驶员行为建模领域，特别是对驾驶动作的预测，数据通常是由不同阶段的序列组成，包含这些时间信息是必不可少的。</li></ol><p>Fuzzy logic</p><p>有其自身的缺点，如信号丢失、需要大内存和长时间处理</p><p>在驾驶监控系统中，传感器通常是关键因素。为了检测驾驶员行为，我们需要自动收集驾驶数据，并应用计算机算法和模型来生成描述驾驶员性能配置文件的分类。在这篇综述中，我们将驾驶行为检测系统一般分为两类车载传感器系统和实时系统。几种技术已被用于检测和识别驾驶员的行为。每种技术都有其优点和缺点。非实时系统技术对驾驶员的训练和反馈非常重要，但不能很好地提高驾驶员的驾驶意识。另一方面，真实的司机监控系统需要多个硬件设备，处理时间长，存储容量大。然而，这些系统有其自身的缺点，如信号丢失、需要大内存和长时间处理。智能手机的发展，可用性和廉价的成本帮助增强和改进驾驶员行为监控系统，克服了以前系统面临的所有障碍。我们可以看到，基于分类问题提出了大量的研究来检测驾驶员的几种行为，并且统计方法给出了一个良好和准确的结果，它为检测问题提供了有价值的见解。此外，我们发现检测系统没有固定的参数，它基于每个系统的目标以及用于收集数据的工具的类型。</p><h1 id="Driver-behavior-profiling-An-investigation-with-different-smartphone-sensors-and-machine-learning"><a href="#Driver-behavior-profiling-An-investigation-with-different-smartphone-sensors-and-machine-learning" class="headerlink" title="Driver behavior profiling: An investigation with different smartphone sensors and machine learning"></a>Driver behavior profiling: An investigation with different smartphone sensors and machine learning</h1><p>驾驶员的行为影响交通安全、燃料/能源消耗和气体排放。司机行为分析试图理解和积极影响司机的行为。通常驾驶员行为分析任务包括驾驶数据的自动收集和计算机模型的应用，以产生一个分类，特征的驾驶员攻击性剖面。虽然采用了不同的传感器和分类方法，但低成本和高性能的解决方案仍是研究的目标。本文对不同的Android智能手机传感器和分类算法进行了研究，以评估哪一种传感器/方法的装配能够使分类具有更高的性能。结果表明，特定的传感器组合和智能方法可以提高分类性能。</p><p><strong>数据集描述</strong></p><ul><li>使用从4个Android智能手机传感器(加速度计、线性加速度、磁强计和陀螺仪)收集的数据，对4个MLAs (BN、MLP、RF和SVM)在检测7种驾驶事件类型中不同配置的性能进行了定量评估。</li><li>在一个有2名驾驶员的真实世界实验中收集了69个此类事件类型的样本。</li></ul><p><strong>实验结果</strong></p><ol><li>滑动窗口尺寸越大表现越好</li><li>陀螺仪和加速计是探测驾驶事件的最佳传感器</li><li>一般来说，使用所有传感器轴比使用单个传感器轴表现更好，除了侵略性的左转事件</li><li>到目前为止，RF是表现最好的MLA，其次是MLP</li><li>前35种组合的性能均令人满意且相当，AUC均值在0.980 ~ 0.999之间变化。</li></ol><p>在未来的工作中，本文希望通过不同的车辆、Android智能手机型号、道路状况、天气和温度来收集更多的驾驶事件样本。本文还希望在评估中增加更多的MLAs，包括基于模糊逻辑和DTW的MLAs。最后，本文打算使用本研究中观察到的最佳评估程序集来开发一个Android智能手机应用程序，该应用程序可以实时检测驾驶事件并计算驾驶员行为概况。</p><h1 id="Driver-Behavior-Profiling-Using-Smartphones-A-Low-Cost-Platform-for-Driver-Monitoring"><a href="#Driver-Behavior-Profiling-Using-Smartphones-A-Low-Cost-Platform-for-Driver-Monitoring" class="headerlink" title="Driver Behavior Profiling Using Smartphones:  A Low-Cost Platform  for Driver Monitoring"></a>Driver Behavior Profiling Using Smartphones:  A Low-Cost Platform  for Driver Monitoring</h1><p><strong>论文题目</strong>：使用智能手机的司机行为分析:一个低成本的司机监控平台</p><p>本文描述了 <strong>SenseFleet</strong>，一个新的移动设备和车辆独立的司机分析和评分应用程序。<br>一个模糊系统被用来计算不同司机的分数使用实时上下文信息，如路线拓扑或天气条件。</p><ul><li>SenseFleet通过融合运动传感器和GPS数据，能够检测加速、刹车、转向和超速事件。</li><li>为了对多个设备和车辆执行事件检测，本文使用了一个校准阶段，该阶段允许调整事件检测算法的模糊集限制。特别是对于超速事件，本文用web服务来获取道路上不同道路的速度限制。</li><li>与现有的解决方案相比，本文提出了一种计分算法，它不仅依赖于事件的数量，而且还考虑上下文信息，如当前的天气条件和时间。</li><li>为了验证平台，本文在不同的条件下使用了该应用程序(即，不同的驾驶员、设备、车辆)，本文使用单一的车辆和路径以及不同的驾驶员以平静和攻击性的方式驾驶进行了一项受控评估研究。</li><li>实验结果表明，SenseFleet能够准确地检测出危险驾驶事件，并能区分激进驾驶者和冷静驾驶者。评分结果与每个驾驶员为他们的实验提供的主观风险度量进行比较。结果显示，SenseFleet的分数在$\pm 1$邻近的司机集群。</li></ul><h1 id="Driver-behaviour-proﬁles-for-road-safety-analysis"><a href="#Driver-behaviour-proﬁles-for-road-safety-analysis" class="headerlink" title="Driver behaviour proﬁles for road safety analysis"></a>Driver behaviour proﬁles for road safety analysis</h1><p><strong>论文题目</strong>：用于道路安全分析的驾驶员行为概况</p><div class="note danger"><p>综述类文章，后续需要详细看一下，下面还未总结</p></div><p>超过90%的道路交通事故是由司机的行为引起的。因此，识别从事不安全驾驶行为的司机有很大的好处。驾驶员行为概况(DBPs)在这里被引入，作为一种评估驾驶员行为作为伤亡事故风险的函数的方法。他们使用全球定位系统(GPS)设备收集的数据，并辅以时空信息。这些配置文件由共同的风险分数组成，可以用来比较驾驶员之间的不同时间和空间。这篇论文详细介绍了这些dbp的发展，并演示了它们在建模影响驾驶员行为的因素时的使用。结果表明，即使控制了道路环境的影响，这些因素仍然是驾驶员行为最强的预测因子，表明不同的时空环境会引起驾驶员的各种心理反应。通过评估行为改变干预措施的影响，保险公司和政府将会对提高道路驾驶司机的风险评估方法和结果感兴趣。</p><h1 id="Investigations-on-Driver-Unique-Identification-from-Smartphone’s-GPS-Data-Alone"><a href="#Investigations-on-Driver-Unique-Identification-from-Smartphone’s-GPS-Data-Alone" class="headerlink" title="Investigations on Driver Unique Identification from Smartphone’s GPS Data Alone"></a>Investigations on Driver Unique Identification from Smartphone’s GPS Data Alone</h1><p><strong>论文题目</strong>：仅从智能手机的GPS数据调查驾驶员的唯一身份</p><p><strong>驾驶员身份识别</strong>一个新兴领域。</p><p>本文提出了一种仅使用智能手机GPS数据进行驾驶员识别的方法。</p><p><strong>数据</strong><br>本文的实验中，使用了两个月的时间收集了38个驾驶员的数据，总行程5万公里。<br>从每一次完成的行程中生成的数据中提取一组137个统计特征，从而量化驾驶员的自然风格。<br>为了进行驾驶员识别，本文将驾驶员分成4 - 5人的自然组，每组以路线邻近度作为分离的决定因素。</p><p><strong>结果</strong></p><ul><li>对于“驾驶员识别”问题，4-5名驾驶员组的平均准确率为82.3%</li><li>某些行为属性如高驾驶技能会影响识别的准确性。</li><li>随机森林分类器提供了最好的结果。</li><li>这些结果对各种利益相关者有很大的影响，因为所提出的方法可以根据驾驶员的自然驾驶风格来识别驾驶员，而这种自然驾驶风格是通过仅从GPS数据中提取的统计参数来量化的。</li></ul><p><strong>结论</strong></p><ul><li>即使只使用智能手机GPS，也有可能以相当高的精度识别司机。</li><li>为了识别出相似的司机需要进行更多的调查。</li><li>如果研究车辆电子控制单元中附加的感觉信息能与gps数据相混淆，那么准确率就会大大提高。</li></ul><h1 id="Know-Your-Master-Driver-Proﬁling-based-Anti-theft-Method"><a href="#Know-Your-Master-Driver-Proﬁling-based-Anti-theft-Method" class="headerlink" title="Know Your Master: Driver Proﬁling-based Anti-theft Method"></a>Know Your Master: Driver Proﬁling-based Anti-theft Method</h1><p><strong>论文题目</strong>：了解你的master:基于司机档案的防盗方法</p><p>本文提出了 <strong>驾驶员验证方法</strong>，该方法利用车辆传感器的测量数据分析驾驶模式。</p><p><strong>驾驶员识别</strong>方向</p><p><strong>数据集</strong></p><p><a href="http://ocslab.hksecurity.net/Datasets/driving-dataset" target="_blank" rel="noopener">CAN data</a></p><ol><li>本文分析司机的真实驾驶数据。且本文从机动车道、城市道和停车场三种道路类型收集数据。10名司机的驾驶数据被反复收集。</li><li>根据难以绕路的行为特征对驾驶员进行分类。为了提高精度，本文丰富了特征集，包括在之前的工作中广泛使用的与制动和加速相关的行为特征，以及从驾驶员行为推导出的机械特征。</li><li>通过特征选择，设计了考虑显著特征的模型。这减少了特征处理的时间开销，提高了检测性能。</li><li>通过导出平均值、中位数和标准差等统计特征来丰富特征集。这使得每个驾驶员的特征值波动的影响最小，最终得到一个可靠的模型。</li><li>处理滑动窗口检测时间点，当检测成为可靠的，并尽快通知车主盗窃事件。</li></ol><h1 id="Various-Approaches-for-Driver-and-Driving-Behavior-Monitoring-A-Review"><a href="#Various-Approaches-for-Driver-and-Driving-Behavior-Monitoring-A-Review" class="headerlink" title="Various Approaches for Driver and Driving Behavior Monitoring: A Review"></a>Various Approaches for Driver and Driving Behavior Monitoring: A Review</h1><p><strong>论文题目</strong>：驾驶员和驾驶行为监控的各种方法:综述 <a href="/posts/100001/" title="全文笔记">全文笔记</a></p><p>这篇文章回顾了各种可用的方法来确定困倦和分心的司机的状态。通过对驾驶员视觉特征、非视觉特征和驾驶行为行为等驾驶员行为的研究来检测驾驶员困倦。</p><ol><li>PERCLOS、闭眼时间(ECD)、闭眼频率(FEC)是一种基于视觉特征的检测驾驶员睡意的系统。<br>其中，PERCLOS在检测睡意方面表现良好，但存在光照条件等局限性。为了克服这个问题，使用了850nm的IR光源。</li><li>生理信号如ECG、EEG、EoG和PPG信号作为非视觉特征检测驾驶员困倦。尽管生理信号比视觉特征表现出更好的表现，但它们也有一些局限性，尤其是它们的侵入性。为了克服这个问题，应该开发侵入性较小的传感器。目前，ECG信号可以用一种较少干扰的方式捕获。驾驶性能行为如方向盘运动和侧位标准偏差也被用来检测困倦。</li><li>驾驶员注意力分散是通过头部姿势和注视方向来检测的。驾驶员的分心可能会导致更大的车道变化，更慢的对障碍的反应，以及更突然的转向控制。因此，为了开发一个更安全的驾驶员监控系统，我们应该监控驾驶员的分心。</li><li>对于主动驾驶安全系统而言，预测不安全驾驶行为是一种可取的方法。本文解释了基于面部表情和汽车动力学的预测方法。通过面部表情检测驾驶员的情绪，有助于预测驾驶员的驾驶行为。</li><li>最后本文讨论了主动驾驶安全系统未来发展中需要解决的几个问题。它们是a)检测睡意的混合措施，b)安全驾驶的驾驶环境感知，c)模拟和真实驾驶条件的公共数据集的可用性。</li></ol><h1 id="Who-is-behind-the-wheel-Driver-identification-and-fingerprinting"><a href="#Who-is-behind-the-wheel-Driver-identification-and-fingerprinting" class="headerlink" title="Who is behind the wheel? Driver identification and fingerprinting"></a>Who is behind the wheel? Driver identification and fingerprinting</h1><p><strong>论文题目</strong>：谁在开车?驾驶员身份和指纹识别</p><p><strong>驾驶员识别</strong></p><p><strong>研究问题</strong> </p><p>利用车载传感器数据描述驾驶员的驾驶风格</p><p><strong>本文方法</strong><br>本文利用车载传感器测量的真实驾驶数据集来解决驾驶员识别问题，提出了一种基于驾驶模式的时间优化的驾驶员指纹识别方法。</p><p>本文研究了达到预期识别性能所需的最小学习和分类时间。进一步进行特征选择，提取与驾驶员识别最相关的特征。最后，除了与驾驶模式相关的特征外，还显示了驾驶员相关的特征(比如，心率)，进一步提高了识别性能。</p><p><strong>数据集</strong></p><p>本文使用了三个数据集：</p><ol><li><a href="https://ocslab.hksecurity.net/Datasets/driving-dataset" target="_blank" rel="noopener">Security dataset</a></li><li><a href="http://www.robesafe.uah.es/personal/eduardo.romera/uah-driveset/" target="_blank" rel="noopener">UAH‑DriveSet</a></li><li><a href="https://www.hcilab.org/research/hcilab-driving-dataset/." target="_blank" rel="noopener">HciLab dataset</a></li></ol><p><strong>结果表明</strong></p><ul><li>车内网络数据如燃油平衡、制动踏板和方向盘数据对驾驶员的准确识别具有重要意义。</li><li>使用有限数量的传感器数据从受限但明智地选择的一组传感器收集，可以在驾驶的前3分钟内以非常高的准确性识别驾驶员。</li></ul>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
            <tag> driving behavior proﬁling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记 AM-GCN</title>
      <link href="/posts/25131/"/>
      <url>/posts/25131/</url>
      
        <content type="html"><![CDATA[<h1 id="AM-GCN"><a href="#AM-GCN" class="headerlink" title="AM-GCN"></a>AM-GCN</h1><p>《AM-GCN:Adaptive Multi-channel Graph Convolutional Networks》<br>论文题目：自适应多通道图形卷积网络</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>提出的问题</strong>：</p><ul><li>GCNs是否能将节点特征和拓扑结构最优地整合在一个信息丰富的复杂图形中。</li></ul><p><strong>调查的结果</strong>：</p><ul><li>最新的GCN在融合节点特征和拓扑结构方面的能力与最佳甚至令人满意的目标相去甚远。 该弱点可能会严重阻碍某些分类任务中GCN的功能，因为GCN可能无法自适应地学习拓扑结构和节点特征之间的某些深层相关信息。</li></ul><p><strong>面临的挑战</strong>：</p><ul><li>是否可以弥补这一弱点并设计出一种新型的GCN，这些GCN既可以保留最新的GCN的优势，又可以大大增强融合拓扑结构和节点特征的能力？</li></ul><p><strong>解决方法</strong>:</p><ul><li>本文提出了一种用于半监督分类的自适应多通道图卷积网络（AM-GCN）</li><li>AM-GCN中心思想是，从节点特征，拓扑结构及其组合中同时提取特定的嵌入和常见的嵌入，并使用注意力机制来学习嵌入的自适应重要性权重。 </li><li>本文在基准数据集上进行的广泛实验清楚地表明，AM-GCN从节点特征和拓扑结构中提取出最相关的信息，并以明显的余量提高了分类精度。</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>图卷积网络（GCN），一种整个过程部分受节点标签的监督。GCN的巨大成功部分归功于GCN提供了一种拓扑结构和节点特征的融合策略以学习节点嵌入，并且融合过程受到了端到端的监督 学习框架。设计用于学习图数据的神经网络，在解决图分析问题方面表现出了很高的知名度，例如节点分类[1,31]，图分类[7,37]，链接预测[13，36]和推荐[6,34]。<br>典型的GCN [14]及其变量[11,16,27,30,36]通常遵循消息传递方式。 关键步骤是特征聚合，即节点在每个卷积层中聚合来自其拓扑邻居的特征信息。 这样，特征信息通过网络拓扑传播到节点嵌入，然后将这样学习的节点嵌入用于分类任务。整个过程部分受节点标签的监督。GCN的巨大成功部分归功于GCN提供了一种拓扑结构和节点特征的融合策略以学习节点嵌入，并且融合过程受到了端到端的学习框架的监督。<br>但是，最近的一些研究揭示了最新的GCN在融合节点特征和拓扑结构方面的某些弱点。 例如，Li等。 [15]表明，GCN实际上对节点特征执行拉普拉斯平滑化，并使嵌入网络的节点逐渐收敛。Nt和Maehara [20]和Wuet等[30]证明拓扑结构起着低通的作用。 当特征信息通过网络拓扑结构传播时，对节点特征进行过滤。 高等。 [8]在GCN中设计一个条件随机字段（CRF）层，以明确保留节点之间的连接性。<br>GCNs从拓扑结构和节点特征中真正学习和融合了哪些信息?这是一个基本的问题。GCNs经常被用作端到端的学习框架。正确回答这个问题，有助于我们有原则地理解GCNs的能力和局限性。这立即激励了我们的学习。</p><p>本文的贡献：</p><ul><li>我们通过实验评估了GCNs融合拓扑结构和节点特征的能力，并指出了GCN的不足之处。我们进一步研究了如何大幅度提高GCN的融合能力进行分类的问题。</li><li>我们提出了一种新的自适应多通道GCN框架AM-GCN，它在拓扑空间和特征空间上执行图的卷积操作。结合注意机制，可以充分融合不同的信息。</li><li>我们在一系列基准数据集上进行的大量实验清楚地表明，AM-GCN的性能优于目前最先进的GCNs，并且能够很好地从节点特征和拓扑结构中提取出最多的相关信息，用于具有挑战性的分类任务。</li></ul><h2 id="GCNs的融合能力：实验调研"><a href="#GCNs的融合能力：实验调研" class="headerlink" title="GCNs的融合能力：实验调研"></a>GCNs的融合能力：实验调研</h2><p>在本节中，我们使用两个简单而直观的案例来检查最新的GCN是否可以从图中的节点特征和拓扑结构中自适应学习，并将它们充分融合以用于分类任务。<br>主要思想是，我们将清楚地分别建立具有网络拓扑的节点标情况1：随机拓扑和相关的节点特征<br>签和节点特征之间的高度相关性，然后我们将检查这两种简单情况下GCN的性能。 良好的GCN融合能力应在节点标签的监督下自适应地提取相关信息，从而获得良好的结果，但是，如果性能与基线相比急剧下降，则表明GCN无法从节点特征和拓扑中自适应地提取信息。 结构，即使节点特征或带有节点标签的拓扑结构之间也具有高度相关性。</p><h3 id="情况1：随机拓扑和相关的节点特征"><a href="#情况1：随机拓扑和相关的节点特征" class="headerlink" title="情况1：随机拓扑和相关的节点特征"></a>情况1：随机拓扑和相关的节点特征</h3><p>我们生成了一个由900个节点组成的随机网络，其中在任意两个节点之间构建边缘的概率为0.03。 每个节点都有一个50维的特征向量。 为了生成节点特征，我们为900个节点随机分配了3个标签，对于具有相同标签的节点，我们使用一种高斯分布来生成节点特征。 这三类节点的高斯分布具有相同的协方差矩阵，但三个彼此远离的不同中心。 在此数据集中，节点标签与节点特征高度相关，但与拓扑结构无关。</p><p>我们应用GCN [14]训练此网络。对于每个类，我们随机选择20个节点进行训练，另外选择200个节点进行测试。 我们仔细调整超参数以报告最佳性能，并避免过度平滑。 另外，我们仅将MLP [21]应用于节点特征。 GCN和MLP的分类准确度分别为75.2％和100％。<br>结果符合预期。 由于节点特征是与所述节点的标签高度相关的，MLP显示出优异的性能。 GCN从节点特征和拓扑结构中提取信息，但是无法自适应地融合它们以避免拓扑结构的干扰。 它无法比拟的MLP的高性能。</p><h3 id="情况2：相关的拓扑和随机节点特征"><a href="#情况2：相关的拓扑和随机节点特征" class="headerlink" title="情况2：相关的拓扑和随机节点特征"></a>情况2：相关的拓扑和随机节点特征</h3><p>我们生成另一个具有900个节点的网络。 这一次，将随机生成50个维中的每个维的节点特征。 对于拓扑结构，我们采用随机块模型（SBM）[12]将节点分为3个社区（分别为节点0-299、300-599、600-899）。 在每个社区内，建立边缘的概率设置为0.03，在不同社区的节点之间建立边缘的概率设置为0.0015。 在此数据集中，节点标签由社区确定，即，同一社区中的节点具有相同的标签。<br>同样，我们将GCN应用于此网络。 我们还将DeepWalk [22]应用于网络拓扑，也就是说，DeepWalk会忽略这些功能。 GCN和DeepWalk的分类准确度分别为87％和100％。<br>DeepWalk表现出色，因为它可以对网络拓扑结构进行全面建模。 GCN从节点特征和拓扑结构中提取信息，但是无法自适应地融合它们，以避免受到节点特征的干扰。 它无法比拟DeepWalk的高性能。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这些情况表明，目前的GCN融合机制[14]远未达到理想甚至令人满意的程度。 即使节点标签与网络拓扑或节点特征之间的相关性很高，当前的GCN也无法充分利用节点标签的监督来自适应地提取最相关的信息。 但是，实际情况更加复杂，因为很难知道拓扑或节点特征是否与最终任务更加相关，这促使我们重新考虑当前的GCN机制。</p><h2 id="AM-GCN-THE-PROPOSED-MODEL"><a href="#AM-GCN-THE-PROPOSED-MODEL" class="headerlink" title="AM-GCN: THE PROPOSED MODEL"></a>AM-GCN: THE PROPOSED MODEL</h2><h3 id="Problem-Settings"><a href="#Problem-Settings" class="headerlink" title="Problem Settings"></a>Problem Settings</h3><p>我们将重点放在属性图$G =（A，X）$上的半监督节点分类中，</p><ul><li>$A\in \mathbb R^{n×n}$是具有$n$个节点的对称邻接矩阵</li><li>$X\in \mathbb R^{n×d}$是节点特征矩阵，$d$是节点特征的维数。</li></ul><p>具体来说，$A_{ij} = 1$表示节点$i$和$j$之间存在一条边，否则，$A_{ij} = 0$。<br>我们假设每个节点都属于$C$中的一个类。</p><h3 id="AM-GCN-的整体框架"><a href="#AM-GCN-的整体框架" class="headerlink" title="AM-GCN 的整体框架"></a>AM-GCN 的整体框架</h3><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p><strong>关键思想</strong>：</p><ul><li>AM-GCN不仅允许节点特征在拓扑空间中传播，而且还可以在特征空间中传播，并且应该从这两个空间中提取与节点标签最相关的信息。</li><li>我们基于节点特征$X$构造了一个特征图。然后，通过两个特定的卷积模块，$X$能够在特征图和拓扑图上传播，以分别学习两个特定的嵌入$Z_F$和$Z_T$。</li><li>此外，考虑到这两个空间中的信息具有共同的特征，我们设计了一个具有参数共享策略的共同卷积模块，以学习共同的嵌入$Z_{CF}$和$Z_{CT}$，并且一致性约束$\mathcal L_c$用于增强$Z_{CF}$和$Z_{CT}$的“公共”属性。</li><li>此外，视差约束$\mathcal L_d$是为了确保$Z_F$和$Z_{CF}$之间以及$Z_T$和$Z_{CT}$之间的独立性。 考虑到节点标签可能与拓扑或特征或两者相关联，AM-GCN利用注意力机制将这些嵌入与学习的权重进行自适应融合，从而提取出最相关的信息$Z$进行最终分类任务。</li></ul><h3 id="Specific-Convolution-Module"><a href="#Specific-Convolution-Module" class="headerlink" title="Specific Convolution Module"></a>Specific Convolution Module</h3><p>首先，为了捕获特征空间中节点的底层结构，我们基于节点特征矩阵$X$构造了一个$k$最近邻（kNN）图$G_f =(A_f，X)$，其中$A_f$是$kNN$图的邻接矩阵。 具体来说，我们首先计算n个节点之间的相似度矩阵$S \in \mathbb R^{n×n}$。 </p><p>得到$S$的两种方法：【其中$x_i$和$x_j$是节点$i$和$j$的特征向量】</p><ol><li><p><strong>Cosine Similarity（余弦相似度）</strong><br>它使用两个向量之间的角度的余弦值来测量相似度：<br>$$<br>S_{ij} = \frac{x_i \cdot x_j}{|x_i||x_j|}<br>$$</p></li><li><p><strong>Heat Kernel（热核）</strong><br>$$<br>S_{ij} = e^{- \frac{\lVert x_i-x_j \rVert ^2}{t}}<br>$$</p><ul><li>其中t是热传导方程中的时间参数，取t=2.</li></ul></li></ol><p>本文统一选择<strong>余弦相似度</strong>来获得相似度矩阵$S$，然后为每个节点选择top $k$个相似节点对来设置边，最终得到邻接矩阵$A_f$。<br>然后使用特征空间中的输入图$（A_f，X）$，第$l$层输出$Z^{（l）}_f$可以表示为：<br>$$<br>Z^{(l)}_f = ReLU(\tilde D_f^{- \frac{1}{2}} \tilde A_f \tilde D_f^{- \frac{1}{2}} Z^{(l-1)}_f W^{(l)}_f)<br>$$</p><ul><li>$W^{(l)}_f$是GCN中第l层的权重矩阵（weight matrix）</li><li>ReLU是ReLU激活函数</li><li>初始化$Z^{(0)}_f = X$</li><li>$\tilde A_f = A_f + I_f$和$\tilde D_f$是$\tilde A_f$的对角度矩阵(diagonal degree matrix)</li><li>表示最后一层的输出嵌入作为$Z_F$</li></ul><p>用上面的方式，可以学习到能够在特征空间中捕获到特殊的信息$Z_F$的节点嵌入；<br>对于拓扑空间，原始的输入图$G_t = (A_t,X_t)$，其中$A_t = A$和$X_t = X$；<br>将基于拓扑图的嵌入$Z_T$的学习输出嵌入可以在特征空间中以相同的方式计算。因此，可以提取出拓扑空间中编码的特定信息。</p><h3 id="Common-Convolution-Module"><a href="#Common-Convolution-Module" class="headerlink" title="Common Convolution Module"></a>Common Convolution Module</h3><p><strong>存在的问题</strong>：</p><ul><li>事实上，特征空间和拓扑空间不是完全相关的；</li><li>节点分类任务基本上可能与特征空间、拓扑空间或两者都存在关联，这是很难预先知道的。因此，我们不仅需要提取这两个空间中嵌入的特定节点，还需要提取这两个空间共享的公共信息。这样，任务就可以更加灵活地确定哪一部分信息是最相关的。</li></ul><p><strong>本文的解决方法</strong>：</p><ul><li>设计了一个具有参数共享策略的公共gcn，使嵌入共享到两个空间中。</li></ul><p><strong>过程</strong>：</p><ol><li>首先，我们从拓扑图$(A_t,X)$中使用Common-GCN来提取节点嵌入$Z^{(l)}<em>{ct}$<br>$$<br>Z^{(l)}</em>{ct} = ReLU(\tilde D_t^{- \frac{1}{2}} \tilde A_t \tilde D_t^{- \frac{1}{2}} Z^{(l-1)}_{ct} W_c^{(l)})<br>$$<ul><li>$W_c^{(l)}$ 是Common-GCN的第l层的权重矩阵(weight matrix)</li><li>$Z^{(l-1)}<em>{ct}$ 是第l-1层的节点嵌入，且$Z^{(0)}</em>{ct} = X$</li></ul></li><li>利用Common-GCN从特征图(feature graph)$(A_f,X)$中学习节点嵌入,为了提取共享信息，本文对每一层Common-GCN共享相同的权值矩阵$W_c^{(l)}$<br>$$<br>Z^{(l-1)}_{ct} = ReLU(\tilde D_f^{- \frac{1}{2}} \tilde A_f \tilde D_f^{- \frac{1}{2}} Z^{(l-1)}_f W^{(l)}_c)<br>$$<ul><li>$Z^{(l-1)}_{cf}$ 是第l-1层的节点嵌入，且$Z^{(0)}_f = X$</li></ul></li><li>共享权矩阵(shared weight matrix)可以过滤出两个空间中的共享特征。根据不同的输入图形，我们可以得到两个输出嵌入$Z_CT$和$Z_CF$，两个空间的共同嵌入$Z_C$为:<br>$$<br>Z_C = (Z_{CT} + Z_{CF})/2<br>$$</li></ol><h3 id="Attention-Mechanism"><a href="#Attention-Mechanism" class="headerlink" title="Attention Mechanism"></a>Attention Mechanism</h3><p>现在我们有两个特定的嵌入$Z_T$和$Z_F$，和一个常见的嵌入$Z_C$。考虑到节点标签可以与其中一个甚至是它们的组合关联，我们使用注意机制$att(Z_T, Z_C, Z_F)$来了解它们对应的重要性$(\alpha_t,\alpha_c,\alpha_f)$:<br>$$<br>(\alpha_t,\alpha_c,\alpha_f) = att(Z_T,Z_C,Z_F)<br>$$</p><ul><li>$\alpha_t,\alpha_c,\alpha_f \in \mathbb R^{n \times 1}$表示嵌入$Z_T、Z_C、Z_F$的n个节点的注意值  </li></ul><p>这里我们关注节点$i$，它嵌入在$Z_T$中的是$z^i_T \in \mathbb R^{1 \times h}$(即$Z_T$的第$i$行).本文首先通过非线性变换对嵌入进行变换，然后使用一个共享的注意向量$q \in \mathbb R^{h’ \times 1}$去获得注意力值$\omega^i_T$<br>$$<br>\omega^i_T = q^T \cdot tanh(W \cdot (z^i_T)^T + b)<br>$$</p><ul><li>$W \in \mathbb R^{h’ \times h}$ 是weight matrix</li><li>$b \in \mathbb R^{h’ \times 1}$ 是bias vector（偏差向量）</li></ul><p>类似的，我们可以从嵌入矩阵$Z_C$和$Z_F$获取节点$i$的注意力值$\omega^i_C$和$\omega^i_F$。本文使用softmax函数去归一化注意力的值$\omega^i_T \omega^i_C, \omega^i_F$从而获得最终的权重。<br>$$<br>\alpha^i_T = softmax(\omega^i_T) = \frac{exp(\omega^i_T)}{exp(\omega^i_T)+exp(\omega^i_C)+exp(\omega^i_F)}<br>$$</p><ul><li>$\alpha^i_T$越大，对应的嵌入越重要。</li><li>$\alpha^i_C = softmax(\omega^i_C)$,$\alpha^i_F = softmax(\omega^i_F)$</li></ul><p>对于所有的n节点，我们学习的权重$\alpha_t=[\alpha^i_T],\alpha_c=[\alpha^i_C],\alpha_f=[\alpha^i_F] \in \mathbb R^{n \times 1}$,并且表示$\alpha_T = diag(\alpha_t)$,$\alpha_C = diag(\alpha_c)$和$\alpha_F = diag(\alpha_f)$，然后我们结合这三个嵌入来得到最终的嵌入$Z$:<br>$$<br>Z = \alpha_T \cdot Z_T + \alpha_C \cdot Z_C + \alpha_F \cdot Z_F<br>$$</p><h3 id="Objective-Function-目标函数"><a href="#Objective-Function-目标函数" class="headerlink" title="Objective Function(目标函数)"></a>Objective Function(目标函数)</h3><h4 id="Consistency-Constraint（一致性约束）"><a href="#Consistency-Constraint（一致性约束）" class="headerlink" title="Consistency Constraint（一致性约束）"></a>Consistency Constraint（一致性约束）</h4><p>对于两个输出嵌入Common-GCN的$Z_{CT}$和$Z_{CF}$，虽然Common-GCN的权值矩阵是共享的，但是这里我们设计了一个一致性约束来进一步增强它们的通用性。</p><ol><li>首先用l2归一化的方法将嵌入矩阵归一化为$Z_{CTnor}, Z_{CFnor}$。</li><li>然后，这两个归一化矩阵可以用来捕获n个节点的相似度，如$S_T$和$S_F$<br>$$<br>S_T = Z_{CTnor} \cdot Z_{CTnor}^T \<br>S_F = Z_{CFnor} \cdot Z_{CFnor}^T<br>$$<br>一致性意味着两个相似矩阵应该是相似的，这就产生了下面的约束:<br>$$<br>\mathcal L_c = \lVert S_T - S_F \rVert ^2_F<br>$$</li></ol><h4 id="Disparity-Constraint（视差约束）"><a href="#Disparity-Constraint（视差约束）" class="headerlink" title="Disparity Constraint（视差约束）"></a>Disparity Constraint（视差约束）</h4><p>这里因为嵌入$Z_T$和$Z_{CT}$是从相同图$G_t = (A_t,X_t)$中学习得到,为了确保他们能捕获不同的信息，本文应用<strong>Hilbert-Schmidt Independence Criterion (HSIC)</strong>【一个简单而有效的独立措施，以增强这两个嵌入的差距】</p><p>$Z_T$和$Z_{CT}$的HSIC约束<br>$$<br>HSIC(Z_T,Z_{CT}) = (n-1)^{-2} tr(RK_T RK_CT)<br>$$</p><ul><li>$K_T$ 是$k_{T,ij} = k_T(z^i_T,z^j_T)$的 Gram matrices</li><li>$K_CT$ 是$k_{CT,ij} = k_CT(z^i_CT,z^j_CT)$的 Gram matrices</li><li>$R = I - \frac{1}{n}ee^T$<ul><li>$I$ 是单位矩阵</li><li>$e$ 是一个全一的列向量<br>在本文的实现中，我们对$K_T$和$K_{CT}$使用内积核函数(inner product kernel function)<br>同样，考虑到ZF和ZCF的嵌入，也可以从同一个图(Af)中学习到,他们的差距也应该通过HSIC得到加强:<br>$$<br>HSIC(Z_F,Z_{CF}) = (n-1)^{-2} tr(RK_F RK_CF)<br>$$<br>视差约束$\mathcal L_d$<br>$$<br>\mathcal L_d = HSIC(Z_T,Z_{CT}) + HSIC(Z_F,Z_{CF})<br>$$</li></ul></li></ul><h4 id="Optimization-Objective-优化目标"><a href="#Optimization-Objective-优化目标" class="headerlink" title="Optimization Objective (优化目标)"></a>Optimization Objective (优化目标)</h4><p>n个节点的预测表示为$\hat Y =[\mathcal{\hat{y_{ic}}}] \in \mathbb R^{n \times C}$<br>其中$\mathcal{\hat{y_{ic}}}$ 是节点i属于类c的概率<br>$$<br>\hat Y = softmax(W \cdot Z + b)<br>$$</p><ul><li>$softmax(x) = \frac{exp(x)}{\sum^C_{c=1} exp(x_c)}$是跨所有类的normalizer</li></ul><p>假设训练集是L,对于每个$L∈L$真正的标签是$Y_l$和预测的标签是$\hat Y_L$,则所有训练节点的节点分类交叉熵损失表示为$\mathcal L_t$<br>$$<br>\mathcal L_t = -\sum_{l \in L} \sum^C_{i = 1} Y_{li} ln \hat Y_{li}<br>$$</p><p>结合节点分类任务和约束条件，得到如下总体目标函数<br>$$<br>\mathcal L = \mathcal L_t + \gamma \mathcal L_c + \beta \mathcal L_d<br>$$</p><ul><li>$\gamma$和$\beta$是一致性和视差约束条件的参数</li></ul><p>在标签数据的引导下，我们可以通过反向传播来优化所提出的模型，学习用于分类的节点嵌入</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><h4 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h4><p>本文提出的AM-GCN是在6个真实世界数据集上进行评估的，这些数据集总结在表1中。此外，为了重现性，我们在附录中提供了所有的数据网站</p><ul><li><strong>Citeseer</strong></li></ul><p>Citeseer是一个研究论文引文网络，节点为出版物，边缘为引文链接。节点属性是论文的单词包表示，所有节点被划分为6个区域。</p><ul><li><strong>UAI2010</strong></li></ul><p>本文使用的数据集有3067个节点和28311条边已在图卷积网络中测试，用于[28]中的社区检测</p><ul><li><strong>ACM</strong></li></ul><p>这个网络是从ACM数据集中提取的，其中节点代表论文，如果两篇论文的作者相同，那么它们之间就会有一条边。所有论文分为3个类(Database, Wireless Communication,<br>DataMining)。特征是论文关键词的词包表示</p><ul><li><strong>BlogCatalog</strong></li></ul><p>这是一个博客和他们的社会关系从博客目录网站的社交网络。节点属性由用户配置文件的关键字构造，标签代表作者提供的主题类别，所有节点被划分为6个类</p><ul><li><strong>Flickr</strong></li></ul><p>Flickr是一个图片和视频托管网站，在这里用户可以通过照片分享来相互交流。它是一个社交网络，节点代表用户，边代表用户之间的关系，所有节点根据用户的兴趣分组被分成9个类。</p><ul><li><strong>CoraFull</strong></li></ul><p>这是著名的引文网络Cora数据集的放大版，其中节点表示论文，边表示它们的引用，节点根据论文主题进行标记。</p><h4 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h4><p>本文比较了AM-GCN和两种最先进的方法，包括两种网络嵌入算法和六种基于图神经网络的方法。此外，我们为了重现性在补充中提供了所有的代码网站</p><ul><li><strong>DeepWalk</strong></li><li><strong>LINE</strong></li><li><strong>Chebyshev</strong></li><li><strong>GCN</strong></li><li><strong>kNN-GCN</strong></li><li><strong>GAT</strong></li><li><strong>DEMO-Net</strong></li><li><strong>MixHop</strong></li></ul><h4 id="参数设置"><a href="#参数设置" class="headerlink" title="参数设置"></a>参数设置</h4><p>更全面地评估我们的模型,我们选择三个标签利率训练集(例如,20、40、60标记节点每个类),选择1000个节点作为测试集。所有的基线都使用相同的参数初始化提出了他们的论文,我们也进一步仔细把参数来获得最佳的性能。</p><ul><li>对于我们的模型，我们同时训练三个具有相同隐含层维数($nhid1$)和相同输出维数($nhid2$)的两层GCNs，其中$nhid1 ∈ { 512, 768 }$ 和 $nhid2 ∈ { 32, 128, 256}$。</li><li>learning rate with Adam optimizer（Adam优化器的学习率）：$ 0.0001 ∼ 0.0005 $ </li><li>the dropout rate（丢包率）：$ 0.5 $</li><li>$weight decay（权重衰减） ∈ { 5e − 3, 5e − 4} $</li><li>$k ∈ { 2 … 10}$ for k-nearest neighbor graph（k近邻邻居图）.</li><li>The coefficient of consistency constraint and disparity constraints：${ 0.01, 0.001, 0.0001}$和${ 1e−10, 5e−9, 1e−9, 5e−8, 1e−8}$</li><li>对于所有方法，本文在同一个分区上运行5次，并报告平均结果。</li><li>本文使用Accuracy (ACC)和macro F1-score (F1)来评价模型的性能。</li></ul><h3 id="节点分类"><a href="#节点分类" class="headerlink" title="节点分类"></a>节点分类</h3><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>节点分类结果如上图所示，其中L/C表示每类标注节点的数量。本文有以下几点看法:</p><ul><li>与所有基线相比，本文提出的AM-GCN在所有标签率的数据集上通常都能获得最佳性能。特别对于ACC，AM-GCN获得了最大的相关提高，在BlogCatalog是8.59%，在Flickr是8.63%。结果表示AM-GCN的方法是有效的。</li><li>在所有数据集上，AM-GCN的表现都优于GCN和kNN-GCN，说明了AM-GCN自适应融合机制的有效性，因为它比分别执行GCN和kNNGCN可以提取更多有用的信息。</li><li>通过与GCN和kNN-GCN进行比较，可以发现拓扑图和特征图之间存在结构上的差异，在传统拓扑图上进行GCN并不总是比在特征图上进行GCN效果更好。例如，在BlogCatalog、Flickr和UAI2010中，特征图的性能优于拓扑。这进一步证实了在GCN中引入特征图的必要性</li><li>此外，与GCN相比，AMGCN在feature graph (kNN)更好的数据集(如UAI2010、BlogCatalog、Flickr)上的改进更为实质性。这说明AM-GCN引入了一个更好更适合标签的kNN图来监督特征传播和节点表示学习。</li></ul><h3 id="变体分析-Analysis-of-Variants"><a href="#变体分析-Analysis-of-Variants" class="headerlink" title="变体分析 Analysis of Variants"></a>变体分析 Analysis of Variants</h3><p>在本节中，我们将在所有数据集中比较AM-GCN与其三个变体，以验证约束的有效性。</p><ul><li><strong>AM-GCN-w/o</strong>: AM-GCN without constraints $\mathcal L_c$ and $\mathcal L_d$</li><li><strong>AM-GCN-c</strong>: AM-GCN with the consistency constraint $\mathcal L_c$ .</li><li><strong>AM-GCN-d</strong>: AM-GCN with the disparity constraint $\mathcal L_d$</li></ul><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>从上图中可以得出以下的结论：</p><ul><li>AM-GCN的结果始终优于其他三种变体，说明了这两种约束同时使用的有效性。</li><li>对于所有标签率的数据集，AM-GCN-c和AM-GCN-d的结果通常都优于AM-GCN-w/o，验证了这两个约束条件的有效性。</li><li>AM-GCNc在所有数据集上都优于AM-GCN-d，说明一致性约束在该框架中发挥了更重要的作用。</li><li>对比图2和表2的结果可以发现AM-GCN-w/o虽然没有任何限制，但仍然取得了成功</li></ul><h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><p>为了更直观地进行比较并进一步展示我们所提出模型的有效性，我们对BlogCatalog数据集进行了可视化。我们将输出嵌入在softmax之前的AM-GCN(或GCN, GAT)的最后一层上，用t-SNE[26]将学习到的测试集嵌入图绘制出来。图3中的BlogCatalog的结果由真实的标签着色。</p><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><h3 id="Analysis-of-Attention-Mechanism"><a href="#Analysis-of-Attention-Mechanism" class="headerlink" title="Analysis of Attention Mechanism"></a>Analysis of Attention Mechanism</h3><p>为了考察我们所提出的模型所学习到的注意值是否有意义，我们分别分析了注意力分布和注意力学习趋势。</p><h4 id="Analysis-of-attention-distributions"><a href="#Analysis-of-attention-distributions" class="headerlink" title="Analysis of attention distributions"></a>Analysis of attention distributions</h4><p>AM-GCN学习了两个特定的嵌入和一个常见的嵌入，每个嵌入都与注意值相关。我们对标签率为20的所有数据集进行了注意分布分析，结果显示为图4。</p><p>可以看出，对于Citeseer、ACM、CoraFull，拓扑空间中特定嵌入的注意值大于特征空间中的注意值，而公共嵌入的注意值介于它们之间。这意味着拓扑空间中的信息应该比特征空间中的信息更重要。为了验证这一点，我们可以看到，在表2中这些数据集上，GCN的结果优于kNN-GCN。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> taxi demand </tag>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记 ST-MGCN</title>
      <link href="/posts/43102/"/>
      <url>/posts/43102/</url>
      
        <content type="html"><![CDATA[<h1 id="Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting"><a href="#Spatiotemporal-Multi-Graph-Convolution-Network-for-Ride-hailing-Demand-Forecasting" class="headerlink" title="Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting"></a>Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting</h1><p>论文题目：基于时空多图卷积网络的网约⻋需求量预测</p><p>参考笔记：<a href="https://zhuanlan.zhihu.com/p/76978929" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76978929</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>ST-MGCN 是滴滴出行 AI Lab 发表于 AAAI 2019 的一种基于时空多图卷积网络的网约⻋需求量预测模型。区域级需求预测是网约车服务的关键技术。准确的网约车需求量预测可以指导车辆的调度，提高车辆的利用率，减少等待时间，以及缓解交通拥堵。区域之间所存在复杂的时空依赖关系使得这个问题具有很大挑战。已有方法主要关注于建模相邻区域在空间上的欧式相关性（Euclidean correlations），而作者发现距离较远区域之间的非欧相关性也对预测至关重要。本文提出了时空多图卷机网络 spatiotemporal multi-graph convolution network (ST-MGCN)，一个新的用于网约车需求预测的深度学习模型。作者首先将区域间的非欧相关性建模到多个图，然后用多图卷积（multi-graph convolution）来建模其相关性。用全局上下文信息（global contextual information）来建模时序信息，并进一步提出了上下文门控循环神经网络模型（contextual gated recurrent neural network），给历史数据分配权重。在两个数据集上表现强于已有算法的 10% 以上。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>时空预测是城市计算中的一项关键任务，它在自动驾驶汽车操作，能源和智能电网优化，物流和供应链管理等领域都有广泛的应用。 在本文中，我们研究了一项重要任务：区域级乘车需求预测，这是智能交通系统的重要组成部分之一。 区域级别的叫车需求预测的目的是根据历史观察结果预测城市中各个区域的未来需求。 准确的乘车需求预测可以帮助组织车队，提高车辆利用率，减少等待时间并减轻交通拥堵（Yao等人，2018b）。 主要由于复杂的时空相关性，该任务具有挑战性。 一方面，在不同区域之间观察到复杂的依赖性。比如说，一个区域的需求通常受到其空间相邻邻居的影响，同时又与具有相似语境的远距离区域相关联。另一方面，不同时间观测值之间也存在非线性相关性。 通常与一小时前，一天前甚至一周前的各种历史观察结果相关。</p><p>深度学习的最新进展为基于区域的时空预测中的复杂时空关系建模提供了令人鼓舞的结果。 使用卷积神经网络和递归神经网络，在（Shi等人2015; Yu等人2017; Shi等人2017; Zhang，Zheng和Qi 2017; Zhang等人2018a ； Ma等人，2017； Yao等人，2018b； 2018a）。 尽管取得了可喜的结果，但我们认为在建模时空相关性时，两个重要方面被大大忽略了。 首先，这些方法主要关注于建模不同区域之间的欧几里得相关性，但是，我们观察到非欧几里德成对相关性对于准确预测也至关重要。<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"><br>对于区域1，除了邻近区域2外，它还可能与共享相似功能的远距离区域3相关，即它们都在学校和医院附近。 此外，区域1也可能会受到区域4的影响，区域4通过高速公路直接连接到区域1。 其次，在这些方法中，当使用RNN对时间相关性进行建模时，每个区域都是独立处理或仅基于本地信息进行处理。 但是，我们认为全局和上下文信息也很重要。 例如，全球乘车需求的增加/减少通常表明某些事件的发生会影响未来的需求。</p><p>为了解决这些挑战，我们提出了一种新颖的深度学习模型，称为时空多图卷积网络（ST-MGCN）。在ST-MGCN中，我们建议将区域之间的非欧氏相关性编码为多个图形。与（Yao等人2018b）不同，后者使用图嵌入作为每个区域的额外恒定特征，我们利用图卷积显式地建模区域之间的成对关系。图卷积能够在执行预测时聚集邻域信息，而这是传统图嵌入难以实现的。此外，为了在建模时间相关性时合并全局上下文信息，我们提出了上下文门控递归神经网络（CGRNN）。它通过学习门控机制来增强RNN，该门控机制是根据汇总的全局信息计算得出的，以重新加权不同时间戳中的观测值。在两个现实世界的大规模乘车需求数据集上进行评估时，ST-MGCN始终以最先进的方式始终领先于最先进的基准。总而言之，本文做出了以下贡献：</p><ul><li>我们在乘车需求预测中确定区域之间的非欧几里得相关性，并建议使用多个图形对它们进行编码。 然后，我们进一步利用提出的多图卷积对这些相关性进行显式建模。</li><li>我们建议使用上下文门控RNN（CGRNN）在对时间依赖性进行建模时合并全局上下文信息。</li><li>我们在两个大规模的真实世界数据集上进行了广泛的实验，与用于叫车需求预测的最新基准方法相比，该方法可将相对误差减少10％以上。</li></ul><p>网约车需求量预测问题可以通过其数据建模方式来理解。以1小时为时间单位，1km*1km 的网格为空间单位，某城市某个小时订单量可以用如下所示的2d格点图片来表示，每个格点的数值是在该时间段内该区域所产生的滴滴打车的订单数的总和。那么所谓网约车需求量预测，就是已知过去几个小时每个格点的订单数，预测未来的订单数。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="城市计算的时空预测"><a href="#城市计算的时空预测" class="headerlink" title="城市计算的时空预测"></a>城市计算的时空预测</h3><h3 id="图卷积网络"><a href="#图卷积网络" class="headerlink" title="图卷积网络"></a>图卷积网络</h3><h3 id="Channel-wise-attention"><a href="#Channel-wise-attention" class="headerlink" title="Channel-wise attention"></a>Channel-wise attention</h3><p>Region-level ride-hailing demand forecasting </p><h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><p>本文要解决的问题是，如何能够更好的建模多个区域之间所存在的非欧且多模态的时间和空间相关性，以实现高准确率的网约车需求量预测。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>我们将时空乘车需求预测的学习问题形式化，并描述如何使用提出的时空多图卷积网络（ST-MGCN）对时空依赖进行建模。</p><h3 id="区域级网约车需求预测（Region-level-ride-hailing-demand-forecasting-）"><a href="#区域级网约车需求预测（Region-level-ride-hailing-demand-forecasting-）" class="headerlink" title="区域级网约车需求预测（Region-level ride-hailing demand forecasting ）"></a><strong>区域级网约车需求预测</strong>（Region-level ride-hailing demand forecasting ）</h3><ul><li>将城市划分为大小相等的网格，每个网格定义为区域v∈V，其中V表示城市中所有不相交区域的集合。</li><li>令X（t）表示在第t个间隔的所有区域中的订单数。</li><li>将区域级乘车需求预测问题公式化为给定具有固定时间长度的输入的单步时空预测，即，学习函数f：R | V |×T→R | V | 将所有区域的历史需求映射到下一个时间步的需求。<br>问题的数学表述如下，输入连续 T 个时刻的格点集合 X（格点的值为订单数），输出下一时刻的订单数，通过训练学习得到该映射函数 f。<br>$$<br>[X^{(t-T+1)},…,X^{(t)}] \xrightarrow{f(\cdot)} X^{(t+1)}<br>$$<br>这里的多模态可以理解为多重维度的关系。</li></ul><p>此外，订单数还与时间紧密相关，比如早晚高峰，节假日等，会对用车数产生比较大的影响，且会呈现某种周期性。所以，作者总结了这个问题所面临的两个挑战。空间上，需要学习区域间存在的多模态非欧相关性。时间上，需要学习复杂的多个时刻的时间依赖关系。</p><p>时空数据预测的相关工作可以分为两类：</p><ol><li>将数据建模成为 2d image 上的格点，使用 CNN 的方法进行预测；</li><li>将数据建模到 graph 的节点上，基于图的方法进行预测。建模称为 2d image 的方法无法处理非欧数据（CNN 所处理的规整的网格是欧式空间）。</li></ol><p>假如就这个问题而言，我们建模为 1km*1km 的网格，那么整个城市不同区域使用的分辨率都是相同的，如果我们希望在市中心用更高的分辨率，郊区用更低的分辨率，或者加入某些兴趣点，这种方法是无法实现的。而现有基于 graph 的方法，虽然在区域建模上具有很高灵活性，但是无法建模上述区域间多模态的相关性。</p><h3 id="框架概览"><a href="#框架概览" class="headerlink" title="框架概览"></a>框架概览</h3><p>ST-MGCN的系统架构：<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><ul><li><p>将多个区域之间的相关性的不同方面表示为多个图，其顶点表示区域，并且边缘编码区域之间的成对关系。</p></li><li><p>首先，我们使用提议的上下文门控递归神经网络（CGRNN）来考虑全球上下文信息来汇总不同时间的观察结果。</p></li><li><p>然后，使用多图卷积来捕获区域之间的不同类型的相关性。 </p></li><li><p>最后，使用完全连接的神经网络将特征转换为预测。</p></li></ul><p>主要分为三个部分：</p><ol><li>将每一帧的数据建模成为三张 graph。每张 graph 上的节点相同，即城市的网格化划分。连边通过如下图所示的规则进行构建。根据不同维度的相关性（邻域信息、功能相似度、交通连通性）确定节点之间的连边值。</li><li>时间维度的预测：将T个历史时间步的信息融合到一张图上。</li><li>空间维度的预测：将一个时刻的不同的相关性的图合成一个图。这就是论文题目中所说的多图卷积。<h3 id="Spatial-dependency-modeling"><a href="#Spatial-dependency-modeling" class="headerlink" title="Spatial dependency modeling"></a>Spatial dependency modeling</h3></li></ol><p>在本节中，我们将展示如何使用多个图来编码区域之间不同类型的相关性，以及如何使用建议的多图卷积来对这些关系进行建模。</p><p>我们使用图形对区域之间的三种相关类型进行建模，包括</p><ol><li>邻域图$\mathcal{G}_N =(V，A_N)$，它编码空间接近度，</li><li>函数相似度图$\mathcal{G}_F =(V，A_F)$，它编码区域周围兴趣点（POI）的相似度，</li><li>运输连通性图$\mathcal{G}_T =(V，A_T)$，它编码远处区域之间的连通性。<br>请注意，我们的方法可以很容易地扩展为通过构造相关图来建模新类型的相关性。</li></ol><p><strong>Neighborhood</strong>：</p><ul><li>区域的邻域是根据空间邻近性定义的。 </li><li>通过将区域连接到3×3网格中的8个相邻区域来构造图形。<br>$$<br>A_{N,ij} = \begin{cases}<br>1,  &amp;\text {$v_i$ and $v_j$ are adjacent}  \<br>0, &amp;\quad otherwise<br>\end{cases}<br>$$</li></ul><p><strong>Functional similarity</strong>：</p><ul><li>在对某个区域进行预测时，直观地参考在功能方面与此区域相似的其他区域。 </li><li>可以使用每个类别的周围POI来表征区域功能，</li><li>并且将两个顶点（区域）之间的边缘定义为POI相似性<br>$$<br>A_{S,i,j} = sim(P_{v_i},P_{v_j}) \in [0,1]<br>$$</li><li>$P_{v_i}$，$P_{v_j}$分别是区域$v_i$和$v_j$的POI向量</li><li>其维数等于POI类别的数量，每个条目代表该区域中特定POI类别的数量。</li></ul><p><strong>Transportation connectivity</strong>：<br>运输系统也是执行时空预测时的重要因素。<br>直观地，可以将那些地理上相距遥远但可方便到达的区域进行关联。 这些类型的连通性是由高速公路（如高速公路），公路或地铁（如地铁）引起的。<br>在这里，我们将通过这些道路直接连接的区域定义为“已连接”（connected），<br>相应的边定义为：<br>$$<br>A_{C,i,j} = max(0,conn(v_i,v_j)- A_{N,i,j} \in {0,1}<br>$$</p><ul><li>$conn(u,v)$是$v_i$和$v_j$之间的连通性的指标函数。</li><li>注意，将邻域边缘从连通性图中移除以避免冗余相关，并且还导致稀疏图。</li></ul><p><strong>Multi-graph convolution for spatial dependency modeling</strong>：</p><ul><li>用于空间依赖建模的多图卷积</li><li>通过构建这些图，我们提出了多图卷积，以对等式5中定义的空间相关性进行建模。<br>$$<br>X_{l+1} = \sigma \big ( \bigsqcup_{A \in \mathbb A} f(A;\theta_i) X_l W_l \big )<br>$$</li><li>$X_l \in \mathbb R ^{|V| \times P_l}$,$X_{l+1} \in \mathbb R ^{|V| \times P_{l+1}$ 是| V |的特征向量 l层和l + 1层中的两个区域。</li><li>σ表示激活函数，F表示聚合函数，例如求和，最大值，平均值等。</li><li>A表示一组图形</li><li>f（A;θi）∈R| V |×| V | 表示基于θi参数化的图A∈A的不同样本的聚合矩阵</li><li>W_l \in \mathbb R^{P_l \times P_{l+1}}表示特征变换矩阵，<br>例如</li><li>如果$f(A;\theta_i)$是拉普拉斯矩阵L的多项式函数，则它将在多张图中变为ChebNet（Defferrard，Bresson和Vandergheynst 2016）。</li><li>如果$f(A;\theta_i) = I$，即恒等矩阵，则它将退回到完全连接的网络。<br>在实现中，选择f（A;θi）作为图拉普拉斯算子L的K阶多项式函数，图3显示了通过图卷积层对集中区域进行值转换的示例。 邻接矩阵为0或1，项Lk ij 6 = 0表示vi能够以k跳达到vj。 在卷积运算方面，k定义了空间特征提取过程中接收场的大小。 使用图1中的道路连通图GC =（V，AC）进行说明。 在邻接矩阵AC中，我们有：</li></ul><h3 id="Temporal-correlation-modeling"><a href="#Temporal-correlation-modeling" class="headerlink" title="Temporal correlation modeling"></a>Temporal correlation modeling</h3><p>我们提出了上下文门控递归神经网络（CGRNN）来建模不同时间戳中的观察之间的相关性。 CGRNN通过使用上下文感知门控机制增强RNN来将上下文信息纳入时间建模中，该机制的体系结构如图4所示。假设，我们有T个时间观测值，而X（t）∈R | V |×P表示第t个观测值 ，其中P是要素尺寸，如果要素仅包含订单数量，则P将为1。 然后上下文选通机制的工作流程如下。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> taxi demand </tag>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记 L-CNN</title>
      <link href="/posts/57948/"/>
      <url>/posts/57948/</url>
      
        <content type="html"><![CDATA[<h1 id="《Real-Time-Taxi-Passenger-Prediction-With-L-CNN》"><a href="#《Real-Time-Taxi-Passenger-Prediction-With-L-CNN》" class="headerlink" title="《Real-Time Taxi-Passenger Prediction With L-CNN》"></a>《Real-Time Taxi-Passenger Prediction With L-CNN》</h1><p>论文题目：L-CNN的实时出租车乘客预测</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>本文提出了一种基于CNN和LSTM的新型神经网络L-CNN，并开发了一种有效的实时预测模型来预测出租车司机最有可能的乘客。</li><li>本文的模型可以轻松地扩展到其他实时交通预测问题，例如道路交通和流量预测。 </li><li>最后，我们根据成都出租车产生的GPS轨迹测试了我们的方法。 提出的方法可以在15分钟的间隔内提前1小时进行乘客预测，结果证明了我们的预测系统的效率。</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>局限性：</p><ul><li>在出租车推荐系统中，有大量的统计学习模型可用于乘客预测。 然而，由于复杂的特征工程，这些方法总是遭受不良性能的困扰并且不适用于实时系统。 另外，很难将这些模型推广到其他应用程序。</li><li>神经网络模型可以从数据中提取有意义的特征，但是深度学习中很少考虑交通应用中必不可少的空间结构。</li></ul><p>L-CNN采用高级架构-在线学习框架和基于卷积神经网络（CNN）[7]，长期短期记忆网络（LSTM）[1]，[2]和嵌入层[ 8]。<br>L-CNN的目标是直接通过类似于图像的输入来理解和完善乘客的机动性。<br>L-CNN利用来自图像处理算法的启发，为交通预测提供了一种新的解决方案。</p><p>本文的贡献：</p><ol><li>提出一种基于LSTM，CNN和嵌入层的本地L-CNN自然网络，以预测潜在的出租车司机的乘客，而无需经过广泛的工程设计和其他外部数据。同时，该算法可以捕获时空维度上的特征以用于出租车乘客预测，并且可以轻松地扩展到其他交通预测问题。</li><li>提出了用于实时预测的在线学习模型。 可以训练模型以根据可用的最新数据保持状态为最新。 因此，它适用于当前流行的基于流数据的预测系统。 它不需要保存任何历史数据，所有知识都记录在模型中。</li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>多项工作已经通过智能驾驶[9]，出租车服务的时空结构建模[10]-[12]，建立乘客发现策略[13]，[14]甚至不同的应用程序成功地探索了此类数据。 通过乘客角度预测出租车位置[15]（在方案2的市区）。</p><ol><li><p>统计模型<br>这些预测模型大多数使用统计学习模型。 在静力学上，出租车业务的这些模型[16]，[17]专注于从历史位置痕迹和集合点中提取节能运输模式。 在[18]中，时空因素被用于交通预测，因为一些研究人员认为轨迹数据具有两个特征：时间和空间，这大大提高了统计学习建模的复杂性。 Chen等。 [19]分析拼车服务。 Pamula等。 [20]提出了出租车共享服务，目的是显着减少每辆出租车的总行驶距离和每人的旅行成本。 但是，由于复杂的功能工程，这些方法总是性能较差，并且不适合实时系统。</p></li><li><p>机器学习模型<br>随着机器学习的发展，深度学习已经完成了一些工作，例如计算机视觉，交通相关研究[21]，[22]。Zhangetal。[23]提出了一种基于深度学习的模型来预测人群的流动性。模型中很少考虑空间结构。卷积RNN已用于处理空间和时间，但仅用于视频应用。在[24]-[26]中，实时预测对算法的及时响应能力有很高的要求，因此具有较低复杂度的贝叶斯算法适合将来的特征提取。尽管过去的短期交通流量预测方法已经存在了一段时间，但新兴的智能交通技术要求交通流量预测功能必须快速且可扩展到整个城市网络。在本文中，我们提出了一种具有时空相关性的神经网络，即基于CNN和LSTM的L-CNN，并开发了一种有效的实时预测模型来预测出租车司机最有可能的乘客。 L-CNN可以轻松扩展到其他实时交通预测问题，例如道路交通和流量预测。</p></li></ol><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="原始数据"><a href="#原始数据" class="headerlink" title="原始数据"></a>原始数据</h3><p>GPS轨迹数据是2014年从成都收集的。记录的信息如下：<br>{DriveID, Time, Flag, Latitude, Longitude}. </p><ul><li>DriveID: The taxi’s identiﬁer. 出租车的标识符。</li><li>Time: The timestamp of each GPS record. 每条GPS记录的时间戳。</li><li>Latitude: The latitude of GPS record. GPS记录的纬度。</li><li>Longitude: The longitude of GPS record. GPS记录的经度。</li><li>Flag: The mark of whether the taxi is carrying passengers.出租车是否载有乘客的标记。Flag= 0和1分别代表空闲和占用出租车。</li></ul><p>每个GPS数据记录都描述了特定时间和地点的出租车的行驶状况。</p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><h4 id="地图网格划分"><a href="#地图网格划分" class="headerlink" title="地图网格划分"></a>地图网格划分</h4><p>为减少处理大量轨迹数据的复杂性，将地图网格划分方法应用于过程地图和GPS位置以建立网格路网。与单独的经纬度坐标相比，网格图可以合并更多的信息。 网格的大小对于预测的准确性和数据处理中消耗的运行时间至关重要。<br>在本文中，地图网格是一个边长为500米的正方形。 至于出租车司机，接机位置与其正在制定的战略密切相关。 但是，相邻的提货通常是一个区域，在该区域中，驾驶员遇到潜在乘客的机会更高。</p><h4 id="位置离散化"><a href="#位置离散化" class="headerlink" title="位置离散化"></a>位置离散化</h4><p>在原始数据集中，每个轨迹分别记录坐标位置和乘客的携带状态。 为了找到该区域的客流模式，我们需要统计乘客在机上和机外的位置，即，载客状态的GPS坐标从0变为1或从1变为0。 因此，我们按时间戳对每个DriveID的所有GPS记录进行了排序，并选择了标志更改的GPS记录以指示上车或下车的地点。</p><h4 id="时间离散化"><a href="#时间离散化" class="headerlink" title="时间离散化"></a>时间离散化</h4><p>一个区域具有相同的模式，用于在同一时间段内正常地提取和下放客流。 由于时间是一个连续变量，我们需要对其进行离散以考虑一段时间内的模式。 我们在位置离散化之后执行时间离散操作，将24小时除以固定的时间间隔，并统计到该分割点的时间段内的数据。本文将间隔时间取为15分钟，并将该时间段的数据打包到一个文件中。 可以在数据预处理后从原始数据集中提取登机位置，注意pt = {lat，lng}，其中lat，lng和Tre分别代表经度，纬度和时间戳。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>在本节中，我们将重点放在实时出租车乘客预测问题上。 通过分析成都出租车的GPS轨迹数据发现了两个关键发现。</p><ul><li>首先，乘客行为在短期内具有一定的随机性，但从长期来看则具有规律性。 </li><li>其次，出租车客预测依赖于时空维度，这意味着要考虑时空维度的特征，同时要用传统的预测方法进行预测。<br>因此，提出了一种基于CNN，LSTM和嵌入技术的中性神经网络L-CNN。</li></ul><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>通过对连续多个矩的G（t）矩阵进行统计分析，我们发现G（t）具有时空相关性。 因此，为了提取空间特征，我们首先输入了G（t）矩阵到CNN [7]，然后将LSTM [2]应用于时间维预测，并最终通过逆网络生成了预测结果矩阵。 此外，我们使用嵌入层技术进行语义学习，因为交通模式显然与时间，天气等有关。</p><p>我们使用D（t）三倍来记录T时刻的星期和分钟。 请注意，由于我们将时间离散化了15分钟，因此min只能取四个值。 我们将min映射到集合{0，1，2，3}以进行嵌入层处理。 可以考虑其他描述性信息，例如温度，天气，季节，是否为法定假日等。在较短的时间跨度内，我们目前仅使用星期，小时，分钟这三个指标。</p><h3 id="L-CNN网络"><a href="#L-CNN网络" class="headerlink" title="L-CNN网络"></a>L-CNN网络</h3><p>逆网络用于通过h（t）重建预测结果。 在本文中，我们尝试使用两种方法来构建逆网络：1）逆卷积网络； 2）示例完全连接的网络并重塑转换。 我们通过实验对这两种方法进行了测试，令人惊讶的是，方法2的性能更好，而方法1却损失了很多细节。因此，实验部分的策略采用方法2作为逆变换。 我们模型的正向传播过程定义为公式4、5。</p><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>学习问题：我们的算法的目的是直接从观测序列中学习两个映射函数fw，b和Inversew，b，其潜在状态h（t）。 MSE损失函数用于形式化此学习问题，以便捕获序列的动态以及从该潜在空间到观测值的映射。 </p><p>我们的模型主要由卷积网络，LSTM和逆网络组成。卷积网络用于提取特征，LSTM用于产生时间维预测，逆网络用于重构结果矩阵。 本文根据这三个网络的特征来确定这三个网络所扮演的不同角色，并构建模型以集成这三个网络。</p><p>在本节中，我们描述使用L-CNN网络进行在线学习和预测的细节。 在线学习是指根据可用的最新数据对模型进行最新培训，从而避免了传统的机器学习模型无法利用最新数据的缺点。 对于新数据T，首先计算损失函数以进行反向传播以在线训练模型，然后将其用作预测T +1矩的输入。 在训练模型时，需要注意两个关键方面。 1）h（t）并非总是传入，在新的一天开始时，它将被重新初始化。 2）L-CNN是逐步模型，为了在+3范围内获得预测，我们使用G（t +1）预测作为G（t +2）的输入，然后使用G（t +2）作为输入 G（t +3）。 在+3视界预测之后，h（t + 3）将返回到h（t + 1）进行训练。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>对一系列时空出租车乘客矩阵G（t）进行实验。 我们考虑在+3的范围内进行预测，即在给定大小为T的训练序列的情况下，将在T +1到T + 3的时间步长内对模型的质量进行评估。注意，每个序列的缩放比例都在0和1之间。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>成都出租车数据集：包含来自成都省14000名出租车司机的智能手机的11天丰富数据。数据集结构显示在表I中，应注意，FLAG表示是否载有乘客，而0表示无乘客，正好相反。对于每辆出租车，每10秒钟收集一次数据，对于我们的发现板而言足够准确</p><h3 id="数据预处理-1"><a href="#数据预处理-1" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>成都数据集每天大约有2 GB数据，可以通过spark进行辅助数据处理。首先，将数据集按时间排序，以使用FLAG查找登机位置，然后根据公式2在时空维度上离散化位置数据，最后生成一个 ∗ n矩阵G（t）。 在本实验中，我们将距离500 m * 500 m的空间离散化，以生成50 * 50的矩阵G（t）。 可视化结果如图1所示。我们可以观察到统计结果倾向于在中心附近发散。 在完成数据预处理之后，我们使用Pytorch（这是一个用于深度学习的流行框架）来构建用于培训和测试的L-Unet网络。</p><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>数据预处理是在高效的分布式计算平台上执行的，Spark2.6.0由两个服务器组成，两个服务器使用Intel Xeon CPU E5-2640 v2 @ 2.00 GHz，64 GB内存，</p><h3 id="结果分析"><a href="#结果分析" class="headerlink" title="结果分析"></a>结果分析</h3><p>我们使用以下模型进行了实验：a）均值：一种简单的启发式方法，该方法通过使用一系列具有观测到的过去值的均值拟合的数据来预测训练步骤，从而预测未来值。 b）L-Unet：我们基于LSTM和CNN的模型，该模型是一种在线学习系统，可以在过去30分钟内从过去的历史记录和推测更改后的变化中进行学习，而无需进行大量的要素工程设计和其他外部数据集（例如Weather或POI）。 c）L-Unet-：我们的模型没有嵌入层。所有结果均为T + 1至T + 3预测的平均预测误差。我们使用均方根误差（RMSE）作为得分函数。如表II所示，我们的模型具有较低的RMSE。同时，我们不需要大量适用于在线学习和实时系统的功能工程和其他外部数据集。不断利用持久数据训练模型将在空间维度上保持良好的泛化性能。图3、4、5是预测结果的可视化。从这些图片中，我们可以看到模型的预测与边缘的地面真实情况高度一致。同时，也可以很好地预测客流量的突然增加，这对出租车资源的分配具有重要的指导作用。根据表II，到了晚上，预测结果会稍差一些，因为乘客人数比往常要多（测试数据使用的是最后一天的数据，即周日的数据。人们通常在周末晚上出门在成都放松一下），而我们的培训数据有限，并且缺乏周末样本。但是，四个预测值的相对值与原始数据基本一致，如图4所示。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文研究了实时出租车驾驶员预测问题。通过对成都出租车数据结构的分析发现了两个关键发现：首先，乘客行为在短期内具有一定的随机性，但在相对长期内具有较高的规律性。其次，滑行乘客预测在时间和空间维度上存在依赖性，这意味着在预测时应考虑空间和时间维度的特征。但是，传统的预测方法很难利用它们。因此，提出了一种基于CNN，LSTM和嵌入技术的本地中立网络L-CNN，用于滑行乘客预测。这里，CNN子网用于提取空间维特征，LSTM用于时间维，嵌入层用于描述。然后，我们引入了一种用于训练和预测的新的在线学习模型L-CNN，可以通过流数据在实时预测系统上轻松实现该模型。大量实验的结果表明，我们的模块具有较低的RSME和更好的可视化效果。值得注意的是，我们的模块可以在不进行大量功能设计的情况下运行，并且很容易将其归纳为其他实时流量预测或其他时空预测问题。在本文中，L-CNN源于图像和视频帧预测方法，这是该领域的新尝试。将来，我们将继续使用L-CNN进行其他实时流量预测。这将大大降低交通问题的预测成本。我们有充分的理由相信，这种实时交通预测将在智慧城市的出租车资源分配中发挥重要的指导作用。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> taxi demand </tag>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记</title>
      <link href="/posts/18527/"/>
      <url>/posts/18527/</url>
      
        <content type="html"><![CDATA[<h1 id="Survey-on-traffic-prediction-in-smart-cities"><a href="#Survey-on-traffic-prediction-in-smart-cities" class="headerlink" title="Survey on traffic prediction in smart cities"></a>Survey on traffic prediction in smart cities</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>机器学习的快速发展以及新数据源的出现使人们可以比以往更准确地检查和预测智慧城市中的交通状况。 这可以帮助优化未来自动化城市中运输服务的设计和管理。 在本文中，我们将详细介绍此类智能交通的预测方法，并概述现有的数据源和预测模型。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><ul><li>智能城市”一词是指使用信息和通信技术来感知，分析和集成运营城市核心系统中的关键信息。</li><li>以高精度预测交通的下一个可能状态是一项基本挑战，因为该信息有助于防止不幸的事件，例如交通拥堵或道路上的其他异常情况。</li><li>文献经常将交通称为流量，因为它具有与流体相似的特性。因此，当我们谈论交通流量预测时，我们希望基于历史和实时数据来预测交通流量的下一个状态（可以是流量，速度，密度或行为）。</li></ul><p>流量预测的两种动机：</p><ul><li><p>能够计算出更有效的路线并减少出行时间</p></li><li><p>市场价值</p></li><li><p>行人流量预测也是智慧城市中的重要挑战</p></li></ul><p>最终目标将是开发一个集成的管理系统，该系统将合并对车辆和其他城市交通流量（如行人或自行车）的预测。 新一代的智能城市管理系统可以揭示大城市中车辆，行人和自行车流量之间的高层关联。</p><h2 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h2><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"><br>传感器分为</p><ul><li>感应回路</li><li>磁传感器</li><li>视频图像处理器</li><li>微波雷达</li><li>激光雷达传感器</li><li>主动红外</li><li>被动红外</li><li>声音感应器</li></ul><p>检查两个数据源，即固定位置传感器和移动传感器，并使用以下标准进行比较：</p><ul><li>数据源的形式描述</li><li>优点和缺点</li><li>通常测量的数据类型</li><li>公开可用的数据集列表。</li></ul><h3 id="Data-from-fixed-position-sensors"><a href="#Data-from-fixed-position-sensors" class="headerlink" title="Data from fixed position sensors"></a>Data from fixed position sensors</h3><p>传统的固定位置传感器基于存在型探测器/传感器，它们部署在空间中的固定位置（表示为p）。 由于这种特性，这些传感器始终在道路的特定点进行测量。 （根据使用的传感器的功能，它们可能会测量一个或多个车道）。</p><p>测量类型取决于传感器的功能。 它们中的一些仅具有基本功能，因为它们仅能够测量流量计数（即流量），而另一些还可以测量流量的速度或密度。 更先进的传感器还能够检测车辆的等级，从而更好地了解交通流的特征。</p><p>与移动传感器相比，传统固定位置传感器</p><ul><li>最大优势在于它们是可靠的数据源，可以捕获所有经过的车辆。 另一方面，GPS传感器一次只能跟踪一辆车辆。 因此，取决于该区域中可用的移动传感器的数量，诸如车辆数量或流量密度之类的汇总统计信息只能近似达到一定的精度。</li><li>缺点是我们无法观察车辆的确切路径。 因此，很难找到不同路段之间的关系。 我们只能根据可用的传感器数据得出粗略的估算值（例如空间相关性分析）。 不幸的是，大型传感器网络的部署和维护成本可能过高。</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
            <tag> taxi predict </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记</title>
      <link href="/posts/51874/"/>
      <url>/posts/51874/</url>
      
        <content type="html"><![CDATA[<h1 id="《Multi-Range-Attentive-Bicomponent-Graph-Convolutional-Network-for-Traffic-Forecasting》"><a href="#《Multi-Range-Attentive-Bicomponent-Graph-Convolutional-Network-for-Traffic-Forecasting》" class="headerlink" title="《Multi-Range Attentive Bicomponent Graph Convolutional Network  for Traffic Forecasting》"></a>《Multi-Range Attentive Bicomponent Graph Convolutional Network  for Traffic Forecasting》</h1><p>论文题目：用于交通预测的多范围注意力双组分图卷积网络</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ol><li>存在的问题：路网复杂的时空依赖性和基本的不确定性</li><li>目前的研究集中解决通过在整个固定加权图中利用图卷积网络（GCN）来建模空间依赖性，但是存在问题：边缘，即成对节点之间的相关性，要复杂得多并且彼此相互作用。</li></ol><p>本文的解决方法：本文提出了多范围注意力双组分GCN（<strong>MRA-BGCN</strong>），这是一种用于交通预测的新型深度学习模型。</p><ul><li>首先根据路网距离构建节点图，并根据各种边缘交互模式构建边缘图</li><li>然后，我们使用双分量图卷积实现节点和边的交互。</li><li>引入了多范围注意力机制来聚合不同邻域范围内的信息，并自动了解不同范围的重要性。</li></ul><p>在两个现实世界的道路网络交通数据集<strong>METR-LA</strong>和<strong>PEMSBAY</strong>上进行的广泛实验表明，MRA-BGCN达到了最新水平。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>交通预测的<strong>任务</strong>：根据历史交通数据预测道路网络的未来交通。</p><p>存在的<strong>挑战</strong>：</p><ol><li>不规则的基础道路网络导致交通数据之间的复杂关联。（复杂的时空依赖性）</li><li>由于各种不可预测的交通状况，交通数据固有地不确定。（基本的不确定性）</li></ol><p>早期的交通预测：</p><ul><li>主要对单个观测节点或少数观测节点采用浅层机器学习，这受到捕获非线性的能力的限制。在交通数据中忽略或几乎没有利用空间依赖性。</li></ul><p>CNN、RNN<br>CNN将模型限制为处理网格结构（例如，图像和视频），并且不考虑由不规则道路网络主导的非欧几里得相关性。</p><p>为了解决这个问题，将图卷积网络（GCN）高效地处理非欧氏相关性，并与RNN（Li等人，2018）或CNN（Yu等人，2018）集成以嵌入道路的先验知识 网络并捕获成对节点之间的相关性。</p><p>引入GCN的结果不错，但是忽略了两个方面，即存在的两个<strong>问题</strong>：</p><ol><li>这些方法主要致力于通过在整个固定加权图中利用GCN来对空间依赖性进行建模。 然而，边缘，即成对节点之间的相关性，要复杂得多并且彼此相互作用。</li></ol><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>如图1（a）所示，传感器1和3以及传感器2和3通过道路链接相互关联。 显然，这些相关性随当前交通状况而变化，并且彼此交互。<br>如图1（b）所示，现有方法根据路网距离构建加权图，并使用GCN来实现节点的交互，而成对节点之间的相关性则由邻接矩阵中的固定标量表示， 忽略了边的复杂性和相互作用。</p><ol start="2"><li>这些方法通常使用在给定的邻域范围内（即，𝑘-hops）聚集的信息，而忽略多个范围信息。 但是，不同范围的信息显示出不同的流量属性。 较小的邻域范围表示本地依赖性，而较大的范围倾向于揭示相对较大区域中的总体流量模式。 此外，不同范围内的信息并非在所有情况下都具有同等作用。 例如，由于交通事故，一个节点主要受其最近邻居的影响，在该节点上，模型应引起更多关注，而不是平等地考虑𝑘-hops的所有邻居。</li></ol><p>为了解决上面提出的问题，本文提出了一种称为 <strong>Multi-Range Attentive Bicomponent GCN（MRA-BGCN）</strong> 的深度学习模型，该模型不仅考虑节点相关性，还将边缘视为彼此交互的实体，并利用多个范围信息。见图1（c）</p><p>本文的贡献：</p><ol><li>本文提出了MRA-BGCN，它引入了双成分图卷积来显式地建模节点和边的相关性。根据路网距离建立节点向图，同时考虑流连通性和竞争关系两种边界相互作用模式建立边向图。</li><li>本文提出了双成分图卷积的多范围注意机制，它可以聚合不同邻域范围内的信息，并了解不同范围的重要性。</li><li>本文在两个真实的交通数据集(metro - la和PEMS-BAY)上进行了广泛的实验，提出的模型获得了最先进的结果。</li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>早期流量预测方法,例如,基于线性回归的方法(Nikovski et al ., 2005),基于卡尔曼滤波的方法(简et al ., 2003),和基于自回归综合移动平均(ARIMA)的方法(里皮et al ., 2013),主要采用浅机器学习单一观测节点或几个节点,由捕捉非线性的能力有限的交通数据和忽视或仅利用空间依赖性。</p><p>深度学习的最新进展使得交通预测中复杂的时空依赖性建模成为可能。一些尝试(Ma et al.， 2017;赵等，2017;Zhang et al. 2018)将卷积神经网络(CNNs)和递归神经网络(RNNs)用于流量预测。在这些研究中，CNNs被引入用于处理规则网格结构(如图像和视频)来捕捉空间依赖性，而没有考虑以不规则道路网络为主的非欧氏相关性。</p><p>为了解决这个问题，研究人员已经应用图卷积来建模交通预测的非欧氏相关性。Li et al.(2018)提出了扩散卷积递归神经网络(Diffusion Convolutional Recurrent Neural Network, DCRNN)，用扩散卷积算子代替了门控递归单元(GRU)中的全连通层(Chung et al.， 2014)。扩散卷积对给定图及其反图进行图卷积，同时考虑流入和流出的关系。余等。(2018)提出了时空GCN (ST-GCN)，它结合了图卷积和ID卷积。在ST-GCN中，图卷积捕捉空间依赖性，ID卷积在时间轴上捕捉时间依赖性，计算效率比RNNs高得多。</p><p>上述基于gcn的方法将路网距离编码为表示空间依赖性的固定加权图。为了进一步建模交通预测中的复杂关联，Wu等人。(2019a)提出用自适应邻接矩阵捕获给定图中看不到的隐藏空间依赖关系。该自适应邻接矩阵是通过计算节点嵌入的相似度来实现的。但是，隐藏的空间依赖关系是通过数据的方式学习的，缺乏领域知识的指导，容易出现过拟合问题。此外，现有的交通预测方法不能很好地模拟边界的相互作用和利用多范围信息。</p><h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p>交通预测的<strong>任务</strong>是对路网中N个相关交通传感器的历史交通数据进行预测。</p><p>本文定义了$N$相关流量传感器作为加权有向图$G = (V, E，A)$</p><ul><li>$V$是一组$|V| = N$个节点的集合</li><li>$E$是一组边缘集合</li><li>$A \in \mathbb R^{N \times N}$是一个加权邻接矩阵,表示节点的接近性,例如,道路网络任何一对节点之间的距离。</li><li>$t$时刻在$G$上观测到的交通数据表示为图信号$X^{(t)} \in \mathbb R^{N \times P}$，其中$P$为每个节点的特征维数。</li></ul><p>交通预测问题的目标是学习一个函数$f$，在给定$T’$历史图形信号和图形$G$的情况下能够预测$T$个未来图形信号:</p><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><h3 id="图卷积"><a href="#图卷积" class="headerlink" title="图卷积"></a>图卷积</h3><p>GCNs是学习非欧几里得结构数据(即图)的构建块(Wu et al.， 2019b)。它们被广泛应用于节点分类(Kipf和Welling, 2017)、图分类(Ying et al.， 2018)、链路预测(Zhang and Chen, 2018)等领域。</p><p>GCN方法分为两类：基于光谱的和基于空间的</p><p>图卷积的定义为图$G = (V, E，A)$<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"><br>其中xe IRNXP为输入信号，8e RPXF为可学习参数矩阵，A = A + Iw为具有自连接的邻接矩阵，o为A的对角度矩阵，D-1A为归一化邻接矩阵，p为非线性激活函数。</p><p>一个图的卷积可以聚合1-hop邻居的信息。通过叠加多个图的卷积层，可以扩展接受邻域范围。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="模型概览（MRA-BGCN框架）"><a href="#模型概览（MRA-BGCN框架）" class="headerlink" title="模型概览（MRA-BGCN框架）"></a>模型概览（MRA-BGCN框架）</h3><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>这个框架包含两个部分：</p><ul><li>双组分图卷积模块（the bicomponent graph convolution module）<ul><li>包含了几个节点方面的图卷积层和边方向的图卷积层</li><li>可以显式地模拟节点和边的相互作用。多距离注意层。</li></ul></li><li>多范围注意层（the multi-range attention layer）<ul><li>聚合了不同邻域范围内的信息，并学习了不同范围内的重要性</li></ul></li><li>此外，本文将MRA-BGCN和RNN结合，建立交通预测的时间依赖性模型。</li></ul><h3 id="Bicomponent-Graph-Convolution"><a href="#Bicomponent-Graph-Convolution" class="headerlink" title="Bicomponent Graph Convolution"></a>Bicomponent Graph Convolution</h3><p>在给定图结构的情况下，图卷积是对节点间交互进行建模的一种有效操作。然而，在交通预测中，边缘，即成对节点之间的相关性，要复杂得多，并且相互作用。因此，我们提出了双成分图卷积，它可以显式地模拟节点和边的相互作用。</p><p>Chen et al.(2019)提出在模型边缘关联中引入边缘邻接线图。设$G= (V, E, A)$表示节点向图。$GL= (V_L, E_L, A_L)$为对应的线形图，则GL的节点V为E中的有序边，即$V_L= {(i→j);(i,j) \in E}$和$|V|=|E|$。$A_L$是一个非加权邻接矩阵，它编码节点图中的边邻接，定义为:$A_{L,(i-i),(j-k)} =1$，否则为0。</p><p>尽管有考虑边邻接的能力，但该线图是一个未加权图，如果目标节点与另一个源节点共享，该线图只考虑两条边的相关。然而，对交通预测中常见的各种边缘相互作用模式进行表征是无效的。如图3所示，我们定义了两种类型的边交互模式来构造沿边图$G_e= (V_e, E_e, A_e)$。注意，$V_e$的每个节点代表$E$的一条边。</p><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><h4 id="Stream-connectivity"><a href="#Stream-connectivity" class="headerlink" title="Stream connectivity"></a>Stream connectivity</h4><p>流连通性:在一个交通网络中，一个道路连接可能受到其上下游道路连接的影响。如图3(a)所示，(i→j)为(j→k)的上游边，两者之间存在相关性。<br>直观上，如果关节节点j有大量的邻居(即j的程度较大)，则(i→j)与(j→k)的相关性是不牢固的，因为它容易受到其他邻居的影响。</p><p>本文计算A中的流连通性的边权值。使用高斯核函数:</p><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><h4 id="Competitive-relationship"><a href="#Competitive-relationship" class="headerlink" title="Competitive relationship"></a>Competitive relationship</h4><p>竞争关系:共享同一源节点的道路链路可能会争夺交通资源，形成竞争关系。如图3(b)所示，共享目标节点k的两条边(i→k)和(j→k)由于竞争关系存在关联。与流连接类似，竞争关系的强度与源节点的出格程度有关。例如，如果一条边的源节点有多个输出边，则该边对于流量资源的竞争是健壮的。因此，我们将Ae中竞争关系的边权计算为:<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>通过构造的边向图$G_e$，如图2所示，双组分图卷积可以显式地模拟节点和边的相互作用。k-hop双组分图卷积公式为:</p><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>其中8c为参数8的图的卷积运算，Jlis串联操作,X (l - 1)是输入层的l node-wise图卷积,z (l - 1)是输入层edge-wise图卷积的l, M E RIVXEl的关联矩阵编码节点和边之间的联结,定义为:Mi(我)= M,(我)= 1和0。MZC)聚合与每个单条节点连接的边表示，MTx()聚合与每个单条节点连接的节点表示。W是一个可学习的投影矩阵，它将原始节点输入X(0)转换为原始边输入z(0)</p><h3 id="Multi-Range-Attention"><a href="#Multi-Range-Attention" class="headerlink" title="Multi-Range Attention"></a>Multi-Range Attention</h3><p>我们提出了双元图卷积的多范围注意机制，以自动学习不同邻域范围的重要性，能够聚合不同邻域的信息范围,而不是给定的范围(例如,邻居在𝑘-hops)。</p><p>双成分图卷积模块得到不同邻域范围内的节点表示，x ={x(1)，x(2)， (k)， x() E RIVIXF，其中k为最大跳数(即双成分图卷积模块的层数)，F为每个节点的表示维数。X(E RF表示节点i在l层中的表示，多距离注意层的目标是捕获多个邻域范围的综合表示。为此，首先，一个由我们IRFXF参数化的共享线性变换应用于每一层的每个节点。然后通过计算W与u的相似度来测量各层的注意力系数，其中u IR为嵌入的邻域范围上下文，初始化为随机向量，在训练过程中共同学习。最后，利用SoftMax函数对系数进行归一化处理。多范围注意机制的表述为:</p><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
            <tag> taxi predict </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记</title>
      <link href="/posts/29557/"/>
      <url>/posts/29557/</url>
      
        <content type="html"><![CDATA[<h1 id="Big-Data-Analytics-in-Intelligent-Transportation-Systems-A-Survey"><a href="#Big-Data-Analytics-in-Intelligent-Transportation-Systems-A-Survey" class="headerlink" title="Big Data Analytics in Intelligent Transportation Systems: A Survey"></a>Big Data Analytics in Intelligent Transportation Systems: A Survey</h1><p>论文题目：智能交通系统中的大数据分析：一项调查</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>本文首先回顾了大数据和智能交通系统的历史和特征。接下来将讨论在ITS中进行大数据分析的框架，其中总结了数据源和收集方法，数据分析方法和平台以及大数据分析应用程序类别。介绍了智能交通系统中大数据分析应用的几个案例研究，包括道路交通事故分析，道路交通流量预测，公共交通服务计划，个人出行路线计划，铁路运输管理与控制以及资产维护。最后，本文讨论了在ITS中使用大数据分析的一些开放挑战。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>几种</p><ol><li>大数据分析可以处理ITS中生成的大量多样而复杂的数据。 大数据分析解决了三个问题：数据存储，数据分析和数据管理。</li><li>大数据分析可以提高ITS的运营效率。 ITS中的许多子系统需要处理大量数据，以提供信息或提供决策来管理流量。 通过快速收集数据并分析当前和历史的大量交通数据，交通管理部门可以实时预测交通流量。 公共交通大数据分析可以帮助管理部门了解交通网络中的乘客出行方式，从而可以更好地规划公共交通服务。 运输APP开发人员的大数据分析可以帮助用户以最合适的路线并在最短的时间内到达目的地。</li><li>大数据分析可以提高ITS的安全水平。 使用先进的传感器和检测技术，可以获得大量的实时运输信息。 通过大数据分析，我们可以有效地预测交通事故的发生。 当发生事故或需要紧急救援时，基于大数据分析的系统中的实时响应功能可以大大提高紧急救援能力。 大数据分析还可以提供新的机会来识别资产问题，例如人行道老化，路面退化等。它可以帮助您在适当的时间做出维护决策，并防止车辆或基础设施处于故障状态。</li></ol><p>在第二部分中讨论了在ITS中进行大数据分析的体系结构。<br>第三节总结了数据来源和收集方法。<br>第四节讨论大数据分析方法。<br>第五部分详细介绍了ITS大数据分析应用程序的案例研究。<br>在第六节介绍大数据分析平台。<br>第七节讨论了在ITS中使用大数据分析的一些开放挑战。<br>第八部分总结了这篇论文。  </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> taxi demand </tag>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>出租车需求量预测小总结</title>
      <link href="/posts/38577/"/>
      <url>/posts/38577/</url>
      
        <content type="html"><![CDATA[<p>实际上，直接做出租车(网约车)需求量预测的文章并不多，但这个问题可以归结为<strong>交通流预测</strong>。后续的基线对比也可以和交通流预测方面的基线进行对比。<br>从特征角度、模型角度、技术角度来进行总结。</p><h2 id="特征角度"><a href="#特征角度" class="headerlink" title="特征角度"></a>特征角度</h2><p>当不考虑模型结构，而是采用最简单的高维特征来预测出租车的需求量的话，<br>出租车需求量预测所需要的特征如下:[1]</p><h3 id="基本特征"><a href="#基本特征" class="headerlink" title="基本特征"></a>基本特征</h3><h4 id="时间特征（Temporal-Features）"><a href="#时间特征（Temporal-Features）" class="headerlink" title="时间特征（Temporal Features）"></a>时间特征（Temporal Features）</h4><ul><li>Month</li><li>Day of month</li><li>Day of week</li><li>Hour</li><li>Holiday</li><li>Historical UOTD </li></ul><p>平日，周末和全天的标准化每小时出租车需求分布图，可以看出需求在工作日和周末之间具有不同的时间模式。（工作日有两个高峰，分别代表早晨高峰和傍晚高峰。 在周末，晚上只有一个高峰。）</p><h4 id="空间特征（Spatial-Features）"><a href="#空间特征（Spatial-Features）" class="headerlink" title="空间特征（Spatial Features）"></a>空间特征（Spatial Features）</h4><ul><li>District</li><li>POI name</li><li>POI category</li><li>Distance distribution</li></ul><h4 id="气象特征（Meteorological-Features）"><a href="#气象特征（Meteorological-Features）" class="headerlink" title="气象特征（Meteorological Features）"></a>气象特征（Meteorological Features）</h4><ul><li>Weather condition </li><li>Temperature </li><li>Wind</li><li>Humidity</li><li>Air quality </li></ul><h4 id="活动特征（Event-Features）【影响打车者的动机】"><a href="#活动特征（Event-Features）【影响打车者的动机】" class="headerlink" title="活动特征（Event Features）【影响打车者的动机】"></a>活动特征（Event Features）【影响打车者的动机】</h4><ul><li>Discount pricing strategy</li><li>Even-odd license plate plan</li><li>Version of the App </li></ul><h3 id="组合特征"><a href="#组合特征" class="headerlink" title="组合特征"></a>组合特征</h3><ul><li>线性模型无法描述输入特征之间的相关性</li><li>将多样化的组合特征输入模型有助于从多尺度和多方面表征不同因素之间的相互作用，这是提高模型的预测能力的关键。</li></ul><h4 id="时时组合特征（Temporal-Temporal-Combinational-Features）"><a href="#时时组合特征（Temporal-Temporal-Combinational-Features）" class="headerlink" title="时时组合特征（Temporal-Temporal Combinational Features）"></a>时时组合特征（Temporal-Temporal Combinational Features）</h4><ul><li>将一天中的小时和星期几作为一个组合功能</li></ul><h4 id="时空组合特征（Temporal-Spatial-Combinational-Features）"><a href="#时空组合特征（Temporal-Spatial-Combinational-Features）" class="headerlink" title="时空组合特征（Temporal-Spatial Combinational Features）"></a>时空组合特征（Temporal-Spatial Combinational Features）</h4><ul><li>POI类别和“小时”的组合将捕获出租车需求的这种时空依赖性</li></ul><p>居住类POI和基础设施类POI的平均每小时标准化出租车需求图</p><h4 id="气象空间组合特征（Meteorological-Spatial-Combinational-Features）"><a href="#气象空间组合特征（Meteorological-Spatial-Combinational-Features）" class="headerlink" title="气象空间组合特征（Meteorological-Spatial Combinational Features）"></a>气象空间组合特征（Meteorological-Spatial Combinational Features）</h4><ul><li>结合气象和空间特征的理由是，气象信息对出租车需求的影响因功能不同的POI而异。</li><li>雨天和非雨天娱乐场所和机场的平均每小时标准化出租车需求图【降雨对机场的影响不明显。娱乐场所的需求对降雨敏感】</li></ul><h4 id="其他组合功能（Other-Combinational-Features）"><a href="#其他组合功能（Other-Combinational-Features）" class="headerlink" title="其他组合功能（Other Combinational Features）"></a>其他组合功能（Other Combinational Features）</h4><ul><li>将POI，小时和天气组合为时空气象特征</li></ul><p>相关的特征大概有100多种功能，这些功能包括2亿个维度。</p><p>总的特征表如下:<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><h2 id="出租车需求预测"><a href="#出租车需求预测" class="headerlink" title="出租车需求预测"></a>出租车需求预测</h2><p>根据预测模型是否需要出租车轨迹，分为基于轨迹的预测和无轨迹的预测。</p><ul><li>基于轨迹的预测：</li></ul><p><strong>缺点</strong>：轨迹信息并不总是与UOTD信息相关联。</p><ul><li>基于无轨迹的预测：</li></ul><p>论文的总结：</p><p>论文[1]提出了<strong>LinUOTD</strong>，一种用于预测大型在线出租车平台的单位原始出租车需求（UOTD）的统一方法。LinUOTD是具有<strong>超过2亿维特征</strong>的<strong>线性回归模型</strong>。 并且设计了一个<strong>时空正则化</strong>方案，一个<strong>分布式学习框架</strong>和一个基于<strong>散列的令牌化</strong>方法，以在大规模数据集上实现有效，并行和可扩展的特征学习。</p><p>参考文献：</p><ol><li>The Simpler The Better: A Unified Approach to Predicting Original Taxi Demands based on Large-Scale Online Platforms</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> taxi demand </tag>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>出租车需求量预测论文集</title>
      <link href="/posts/60455/"/>
      <url>/posts/60455/</url>
      
        <content type="html"><![CDATA[<h2 id="demand-需求"><a href="#demand-需求" class="headerlink" title="demand-需求"></a>demand-需求</h2><h3 id="问题定义："><a href="#问题定义：" class="headerlink" title="问题定义："></a><strong>问题定义</strong>：</h3><ul><li>如何使用历史请求数据来预测将来时间戳中某个区域的请求数，其中开始/提起或结束/下车的次数被用作表示该区域在给定的时间的需求。</li><li>通常，交通预测的需求包括出租车和共享自行车的需求。</li></ul><h3 id="概念："><a href="#概念：" class="headerlink" title="概念："></a><strong>概念</strong>：</h3><ol><li><h3 id="交通需求预测的四阶段法："><a href="#交通需求预测的四阶段法：" class="headerlink" title="交通需求预测的四阶段法："></a>交通需求预测的四阶段法：</h3></li></ol><h4 id="交通的发生与吸引"><a href="#交通的发生与吸引" class="headerlink" title="交通的发生与吸引"></a>交通的发生与吸引</h4><ul><li>发生与吸引交通量的预测是交通需求预测四阶段预测中的第一个阶段，最基本的部分之一</li><li>任务：求出对象地区的交通需求总量，即生成交通量（Trip Production）。然后在此约束下，求出各个交通小区的发生与吸引交通量。</li></ul><h4 id="交通的分布"><a href="#交通的分布" class="headerlink" title="交通的分布"></a>交通的分布</h4><h4 id="交通方式划分"><a href="#交通方式划分" class="headerlink" title="交通方式划分"></a>交通方式划分</h4><h4 id="交通流分配"><a href="#交通流分配" class="headerlink" title="交通流分配"></a>交通流分配</h4><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>在同一预测任务下，不同数据集上的结果差异很大。例如，在需求预测任务中，在相同的时间间隔和预测时间下，NYC Taxi和TaxiBJ数据集的准确度分别为8.385和17.24。</p><h3 id="文献-中短期预测-："><a href="#文献-中短期预测-：" class="headerlink" title="文献(中短期预测)："></a>文献(中短期预测)：</h3><ul><li><p>X. Li, G. Pan, Z. Wu, G. Qi, S. Li, D. Zhang, W. Zhang, and Z. Wang, “Prediction of urban human mobility using large-scale taxi traces and its applications,” Frontiers of Computer Science, vol. 6, no. 1, pp. 111– 121, 2012. </p></li><li><p>L. Moreira-Matias, J. Gama, M. Ferreira, J. Mendes-Moreira, and L. Damas, “Predicting taxi–passenger demand using streaming data,”IEEE Transactions on Intelligent Transportation Systems,vol.14,no.3, pp. 1393–1402, 2013. </p></li><li><p>W. Li, J. Cao, J. Guan, S. Zhou, G. Liang, W. So, and M. Szczecinski, “A general framework for unmet demand prediction in on-demand transport services,” IEEE Transactions on Intelligent Transportation Systems, vol. 20, no. 8, pp. 2820–2830, 2018. </p></li><li><p>J. Guan, W. Wang, W. Li, and S. Zhou, “A uniﬁed framework for predicting kpis of on-demand transport services,” IEEE Access, vol. 6, pp. 32005–32014, 2018. </p></li><li><p>D. Salinas, M. Bohlke-Schneider, L. Callot, R. Medico, and J. Gasthaus, “High-dimensional multivariate forecasting with lowrank gaussian copula processes,” in Advances in Neural Information Processing Systems, 2019, pp. 6824–6834.</p></li><li><p>K. Ishibashi, S. Harada, and R. Kawahara, “Inferring latent trafﬁc demand offered to an overloaded link with modeling qos-degradation effect,” IEICE Transactions on Communications, 2018. </p></li><li><p>X. Geng, Y. Li, L. Wang, L. Zhang, Q. Yang, J. Ye, and Y. Liu, “Spatiotemporal multi-graph convolution network for ride-hailing demand forecasting,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 33, 2019, pp. 3656–3663. </p></li><li><p>Y. Li, Z. Zhu, D. Kong, M. Xu, and Y. Zhao, “Learning heterogeneous spatial-temporal representation for bike-sharing demand prediction,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 33, 2019, pp. 1004–1011. </p></li><li><p>Y. Li and J. Moura, “Forecaster: A graph transformer for forecasting spatial and time-dependent data,” arXiv preprint arXiv:1909.04019, 2019. </p></li><li><p>X. Geng, L. Zhang, S. Li, Y. Zhang, L. Zhang, L. Wang, Q. Yang, H. Zhu, and J. Ye, “{CGT}: Clustered graph transformer for urban spatio-temporal prediction,” in Proceedings of the International Conference on Learning Representations, 2020. [Online]. Available: <a href="https://openreview.net/forum?id=H1eJAANtvr" target="_blank" rel="noopener">https://openreview.net/forum?id=H1eJAANtvr</a></p></li><li><p>H. Yao, F. Wu, J. Ke, X. Tang, Y. Jia, S. Lu, P. Gong, J. Ye, and Z. Li, “Deep multi-view spatial-temporal network for taxi demand prediction,” in Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. </p></li><li><p>H. Yao, X. Tang, H. Wei, G. Zheng, and Z. Li, “Revisiting spatialtemporal similarity: A deep learning framework for trafﬁc prediction,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 33, 2019, pp. 5668–5675. </p></li><li><p>J. Ke, H. Zheng, H. Yang, and X. Chen, “Short-term forecasting of passenger demand under on-demand ride services: A spatio-temporal deep learning approach,” Transportation Research Part C: Emerging Technologies, vol. 85, pp. 591–608, 2017. </p></li><li><p>J. Ye, L. Sun, B. Du, Y. Fu, X. Tong, and H. Xiong, “Co-prediction of multiple transportation demands based on deep spatio-temporal neural network,” in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. ACM, 2019, pp. 305–313. </p></li><li><p>N. Davis, G. Raina, and K. Jagannathan, “Grids versus graphs: Partitioning space for improved taxi demand-supply forecasts,” arXiv preprint arXiv:1902.06515, 2019. </p></li><li><p>L. Liu, Z. Qiu, G. Li, Q. Wang, W. Ouyang, and L. Lin, “Contextualized spatial–temporal network for taxi origin-destination demand prediction,” IEEE Transactions on Intelligent Transportation Systems, vol. 20, no. 10, pp. 3875–3887, 2019. </p></li><li><p>D. Lee, S. Jung, Y. Cheon, D. Kim, and S. You, “Forecasting taxi demands with fully convolutional networks and temporal guided embedding,” in In Advances in Neural Information Processing Systems, 2018. </p></li><li><p>L. Bai, L. Yao, S. Kanhere, X. Wang, and Q. Sheng, “Stg2seq: spatialtemporal graph to sequence model for multi-step passenger demand forecasting,” in Proceedings of the 28th International Joint Conference on Artiﬁcial Intelligence. AAAI Press, 2019, pp. 1981–1987. </p></li><li><p>Y. Wang, H. Yin, H. Chen, T. Wo, J. Xu, and K. Zheng, “Origindestination matrix prediction via graph convolution: a new perspective of passenger demand modeling,” in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. ACM, 2019, pp. 1227–1235. </p></li><li><p>J. Xu, R. Rahmatizadeh, L. B¨ ol¨oni, and D. Turgut, “Real-time prediction of taxi demand using recurrent neural networks,” IEEE Transactions on Intelligent Transportation Systems, vol. 19, no. 8, pp. 2572– 2581, 2017. </p></li><li><p>Z. Pan, Z. Wang, W. Wang, Y. Yu, J. Zhang, and Y. Zheng, “Matrix factorization for spatio-temporal neural networks with applications to urban ﬂow prediction,” in Proceedings of the 28th ACM International Conference on Information and Knowledge Management, 2019, pp. 2683–2691. </p></li></ul><h2 id="交通预测存在的问题"><a href="#交通预测存在的问题" class="headerlink" title="交通预测存在的问题"></a>交通预测存在的问题</h2><ol><li>现有研究专注于<strong>密集型数据</strong>，对于稀疏的城市来说，如何进行研究？多数现有解决方案都是数据密集型的。但是，<strong>异常情况</strong>（极端天气，临时交通控制等）通常是非周期性的，因此很难获得数据，这使得训练样本的大小和学习情况比正常交通条件下的要困难得多。此外，由于不同城市的发展水平不平衡，许多城市还存在数据不足的问题。但是，足够的数据通常是深度学习方法的先决条件。解决此问题的一种可能方法是使用迁移学习技术在整个城市中执行深度时空预测任务。该技术旨在有效地将知识从数据丰富的源城市转移到数据稀缺的目标城市。尽管已经提出了新的方法（[51]，[71]，[75]），但是尚未对这些研究进行彻底的研究，例如如何设计高质量的数学模型以匹配两个区域，或者如何集成其他可用的辅助数据源等。仍然值得考虑和调查。</li><li><strong>知识图融合</strong>:知识图是知识集成的重要工具。 它是一个复杂的关系网络，由许多概念，实体，实体关系和属性组成。 运输领域的知识隐藏在多源和庞大的交通大数据中。 大规模运输知识图的构建，学习和深度知识搜索可以帮助挖掘更深的交通语义信息，提高预测性能。</li><li><strong>长期预测</strong>：现有的交通量预测方法主要基于<strong>短期到中期</strong>的预测，关于长期预测的研究很少。 由于更复杂的时空依存关系和更多不确定因素，长期预测更加困难。 对于长期预测，历史信息可能不会对短期预测方法产生太大影响，因此可能需要考虑其他补充信息。</li><li><strong>多源数据</strong>：如为了提高交通流量预测的性能，我们可以考虑诸如道路网络结构，交通量数据，兴趣点（POI）和城市人口等信息。 有效融合多个数据可以填补丢失的数据并提高预测的准确性。</li><li><strong>实时预测</strong>：实时交通预测的目的是在短时间内进行数据处理和交通状况评估。 但是，由于数据，模型大小和参数的增加，算法的运行时间过长，无法满足实时预测的要求。 因此，如何设计有效的轻量级神经网络以减少网络计算量并加速网络是一个巨大的挑战。</li><li>模型的<strong>可解释性</strong>：由于神经网络的结构复杂，参数量大，算法透明性低，因此验证其可靠性是众所周知的。 缺乏可解释性可能会给交通预测带来潜在的问题。 考虑到复杂的数据类型和交通数据的表示形式，设计可解释的深度学习模型比其他类型的数据（例如图像和文本）更具挑战性。</li><li><strong>基准流量预测</strong>：随着领域的增长，提出了越来越多的模型，并且这些模型通常以相似的方式呈现。 评估新的交通量预测方法的有效性并在缺乏标准化基准和连续实验设置和大型数据集的情况下对模型进行比较已变得越来越困难。 另外，模型的设计变得越来越复杂。 尽管消融研究已在大多数方法中进行，但仍不清楚每个组件如何改进算法。 因此，设计具有标准通用数据集的可再现基准框架非常重要。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> taxi demand </tag>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>船运到达时间比赛-数据集分析</title>
      <link href="/posts/9621/"/>
      <url>/posts/9621/</url>
      
        <content type="html"><![CDATA[<h2 id="船运到达时间预测比赛–赛题介绍"><a href="#船运到达时间预测比赛–赛题介绍" class="headerlink" title="船运到达时间预测比赛–赛题介绍"></a>船运到达时间预测比赛–赛题介绍</h2><h3 id="赛题介绍"><a href="#赛题介绍" class="headerlink" title="赛题介绍"></a>赛题介绍</h3><p>在企业全球化业务体系中，海运物流作为其最重要的一项支撑。其中，船运公司会和数据供应公司进行合作，对运输用的船通过GPS进行定位以监控船的位置；在运输管理的过程中，<strong>货物到达目的港的时间</strong>是非常重要的一项数据，那么<strong>需要通过船运的历史数据构建模型，对目的港到达时间进行预测</strong>，预测时间简称为<strong>ETA（estimated time of arrival）</strong>，目的港到达时间预测为ARRIVAL_ETA（<strong>ATA</strong>）。</p><p>本次大赛提供历史运单GPS数据、历史运单事件数据、港口坐标数据，预测货物运单的到达时间，对应“历史运单事件”数据中EVENT_CODE字段值为ARRIVAL AT PORT时EVENT_CONVOLUTION_DATE的时间值。</p><h3 id="比赛数据"><a href="#比赛数据" class="headerlink" title="比赛数据"></a>比赛数据</h3><p>大赛提供脱敏后的训练数据及测试数据，训练数据集包括：</p><ul><li><strong>历史运单GPS数据</strong></li><li><strong>历史运单事件数据</strong></li><li><strong>港口坐标数据</strong></li><li>测试运单数据为不同运单、运输过程中的不同位置所构成，供选手测试对应的ETA时间。</li><li>货物运单在船运过程中，会产生大量的GPS运单数据，记录为“历史运单GPS数据”；</li><li>货物运单在船运过程中离开起运港、到达中转港、到达目的港等关键事件，记录为“历史运单事件数据”；</li><li>“港口的坐标数据“为与运单船运相关的港口坐标信息。</li></ul><h3 id="数据表格："><a href="#数据表格：" class="headerlink" title="数据表格："></a>数据表格：</h3><table><thead><tr><th align="center">列名</th><th align="center">类型</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">loadingOrder</td><td align="center">VARCHAR2</td><td align="center">脱敏后的主运单，货物的运单编号，类似快递单号</td></tr><tr><td align="center">carrierName</td><td align="center">VARCHAR2</td><td align="center">脱敏后的承运商名称，类似快递公司名称</td></tr><tr><td align="center">timestamp</td><td align="center">DATE</td><td align="center">时间，格式为：yyyy-MM-dd’T’HH:mm:ss.SSSZ，如2019-09-05T16:33:17.000Z</td></tr></tbody></table><h2 id="船运到达时间预测比赛–论文调研"><a href="#船运到达时间预测比赛–论文调研" class="headerlink" title="船运到达时间预测比赛–论文调研"></a>船运到达时间预测比赛–论文调研</h2><h3 id="关注点"><a href="#关注点" class="headerlink" title="关注点"></a>关注点</h3><p><strong>阅读论文需要注意的问题</strong>：</p><ol><li>预测的对象是什么（轨迹，到达时间，目的地）？ </li><li>正在使用哪种方法来估计系统的未来状态？（集体学习、机器学习、深度学习）采用的是什么样的解决方案？是将问题怎么转化的？ </li><li>该方法可以预测多久？ （一个小时之内还是更久，按时间片预测还是？）</li><li>预测准确度是多少？ （预测的指标是什么）</li><li>是否考虑了数据可用性/质量以及如何考虑？（数据的来源以及数据预处理？）</li><li>这篇论文的创新点是什么？或者对自己有什么启发的地方？</li></ol><p>即</p><ul><li><strong>预测对象</strong>  </li><li><strong>创新点</strong>  </li><li><strong>解决方案</strong>   </li><li><strong>算法</strong>  </li><li><strong>预测时间</strong>  </li><li><strong>预测准确度</strong> </li><li><strong>指标</strong>   </li><li><strong>数据集</strong>  </li></ul><h3 id="论文概述"><a href="#论文概述" class="headerlink" title="论文概述"></a>论文概述</h3><p><strong>船运方面的论文</strong></p><h4 id="Forecast-of-Marine-Traffic-Volume-using-Time-Series-Model"><a href="#Forecast-of-Marine-Traffic-Volume-using-Time-Series-Model" class="headerlink" title="Forecast of Marine Traffic Volume using Time Series Model"></a>Forecast of Marine Traffic Volume using Time Series Model</h4><ul><li>使用流行的时间序列模型估计未来的海上交通量。一种定量分析[8] arima模型</li></ul><h4 id="Vessel-Destination-and-Arrival-Time-Prediction-with-Sequence-to-Sequence-Models-over-Spatial-Grid-3"><a href="#Vessel-Destination-and-Arrival-Time-Prediction-with-Sequence-to-Sequence-Models-over-Spatial-Grid-3" class="headerlink" title="Vessel Destination and Arrival Time Prediction with Sequence-to-Sequence Models over Spatial Grid.[3]"></a>Vessel Destination and Arrival Time Prediction with Sequence-to-Sequence Models over Spatial Grid.[3]</h4><ul><li><strong>预测对象</strong> ：预测船只的目的港和估计的到达时间</li><li><strong>创新点</strong> ：将海域划分为一个空间网格，然后将其最近的运动表示为一系列代码，从中提取出其趋势。这些信息和船只的当前位置使我们可以预测直到到达目的地的未来运动。主要是轨迹预测。</li><li><strong>解决方案</strong> ：一种基于序列到序列的方法,作为轨迹预测问题的扩展，该问题以一系列历史位置作为输入并返回一系列未来位置，用于确定到达港口和估计到达时间。本文的解决方案首先代表覆盖地中海的空间网格上的轨迹。然后训练序列到序列模型，以根据运动趋势和当前位置预测船只的未来运动。最后使用分布式体系结构模型和应用的负载平衡技术构建了解决方案，以实现高性能和可扩展性。</li><li><strong>算法</strong> :使用seq2seq模型预测将来的位置，系统将根据这些位置确定目标并估计到达时间。</li><li><strong>预测时间</strong>  </li><li><strong>预测准确度</strong> </li><li><strong>指标</strong>   </li><li><strong>数据集</strong>  </li></ul><h4 id="Towards-an-approach-for-long-term-AIS-based-prediction-of-vessel-arrival-times"><a href="#Towards-an-approach-for-long-term-AIS-based-prediction-of-vessel-arrival-times" class="headerlink" title="Towards an approach for long term AIS-based prediction of vessel arrival times"></a>Towards an approach for long term AIS-based prediction of vessel arrival times</h4><p>这是一篇讲述解决方案的论文，无实验。</p><ul><li><p>分为<strong>短期预测</strong>（一个小时之内）、<strong>长期预测</strong>（预测提前超过一个小时）</p><ul><li>通过分析<strong>船舶运动模式</strong>进行<strong>短期预测</strong>–主要侧重于避免碰撞，因此重点是在不久的将来估算船舶位置，以支持转向决策，而实际目的地无关紧要</li><li>使用<strong>路由提取</strong>进行<strong>长期预测</strong>–估计轨迹或位置的时间间隔超过1小时需要隔离海道，将其与已知的船只轨迹进行比较，以确定最接近的匹配或异常。</li><li><strong>组合方法</strong>的<strong>长期预测</strong>–船舶运动模式和海道提取相结合</li></ul></li><li><p>所有长期预测都依赖于海道提取，然后再尝试估算船舶的目的地。</p></li><li><p>四个<strong>需要解决的不同目标</strong>：数据质量问题，数据量和分布式数据挖掘，行为模式的发现和包含以及天气数据的融合。  </p><ul><li>数据的质量问题</li><li>结合气象数据</li><li>大数据集的航路点/路线提取</li><li>通过发现和包含行为模式来提高准确性。</li></ul></li><li><p><strong>存在的问题</strong></p><ul><li><p>数据不完全可靠：误以为噪声是错误的。</p><ul><li>解决方案：进行3次的异常检查、数据预处理</li><li>第一个方法是立即丢弃超出范围的接收数据，例如纬度低于-90°且高于90°且经度低于-180°且高于180°。</li><li>第二个应该删除范围内的数据，但要指出不可行的轨迹。</li><li>最后一项检查是为数据源分配可靠性，并在可靠性高于或低于所需阈值时决定是否存储或拒绝数据。</li></ul></li><li><p>历史数据量比较庞大（这个还有待商议，DBSCAN是发现航路点/“热点区域”的首选算法。随着数据量超过存储容量，必须特别考虑与分布式数据挖掘和群集有关的替代方法。）</p></li><li><p>查看船只的历史数据以发现行为模式，以及它们对偏离通常航道的可能性的影响。</p></li><li><p>每个模式都是一组航行属性，例如该区域的平均速度，转弯速率和平均行程持续时间。通过将船只与发现的模式之一相关联，可以构造一条新的预测路线，并将该路线的最终位置用于到达时间预测算法。</p></li></ul></li><li><p><strong>思路总结</strong>：首先处理获取到的数据的质量问题，然后然后将好的数据和气象数据相融合，接着对现有数据集执行行为模式分析，以发现每个船只在选择使用哪条路线时的偏好。</p></li></ul><h4 id="Real-time-Destination-and-ETA-Prediction-for-Maritime-Traffic"><a href="#Real-time-Destination-and-ETA-Prediction-for-Maritime-Traffic" class="headerlink" title="Real-time Destination and ETA Prediction for Maritime Traffic"></a>Real-time Destination and ETA Prediction for Maritime Traffic</h4><ul><li><p><strong>预测对象</strong>：预测船只的目的港（分类）和估计的到达时间（回归）</p></li><li><p><strong>创新点</strong>  </p></li><li><p><strong>解决方案</strong>   </p></li><li><p><strong>算法</strong> ：前馈神经网络预测到达时间（回归问题）</p></li><li><p><strong>预测时间</strong>  </p></li><li><p><strong>预测准确度</strong> </p></li><li><p><strong>指标</strong>   </p></li><li><p><strong>数据集</strong></p></li><li><p><strong>特征选取</strong>：坐标，时间戳，航向，航向，速度和出发和到达端口的编码值、目标端口名称及其经度和纬度坐标</p></li></ul><p>【轨迹，速度，天气条件和其他外部因素通常会对到达最终目的地所需的实际轨迹和时间产生重大影响。<br>使用了三个附加功能，例如目标端口名称及其经度和纬度坐标。为了预测剩余的行程时间，我们利用了前馈神经网络。】</p><ul><li>对于给定的用例，输入层从数据集中接收初始数字特征，例如坐标，时间戳，航向，航向，速度和出发和到达端口的编码值，以及我们发现的对如上所述的回归任务有用的特征。</li><li>提取每个时间戳记的持续时间，可以实时预测</li><li><strong>可以思考的点</strong>：<ul><li>建议可以使用LSTM进行离线预测。 还可以针对每种特定的船舶类型训练许多单独的LSTM模型，以捕获其独特的行为。 所有这些模型的输出都可以馈送到完全连接的层，以预测将来的n时间戳。</li><li>可以使用聚类方法来捕获以前看不见的端口并提高准确性。受序列到序列模型的启发，我们还计划通过预测船舶的下一个N时间戳来进一步提高ETA预测，以不仅获得对ETA的了解，而且还可以了解这些N个下一步骤的轨迹行为。</li></ul></li></ul><h4 id="Predicting-Destinations-by-Nearest-Neighbor-Search-on-Training-Vessel-Routes"><a href="#Predicting-Destinations-by-Nearest-Neighbor-Search-on-Training-Vessel-Routes" class="headerlink" title="Predicting Destinations by Nearest Neighbor Search on Training Vessel Routes"></a>Predicting Destinations by Nearest Neighbor Search on Training Vessel Routes</h4><ul><li><strong>预测对象</strong>：预测目的地港口和船只行程的到达时间</li><li><strong>解决方案</strong>：使用最近邻居搜索来查找更接近查询AIS点的训练路线。 还包括特别的改进，例如一种避免在一个查询路由内频繁更改预测端口并通过使用遗传算法自动进行参数调整的方法。</li></ul><ol start="7"><li>【7】</li></ol><p><strong>预测对象</strong>：估计船只到达港口区域的时间<br><strong>方法</strong>：前馈神经网络预测到达时间<br><strong>预测时间</strong>：<br><strong>预测准确度</strong>：<br><strong>数据</strong>：</p><p><strong>陆运方面ETA论文</strong></p><h4 id="When-Will-You-Arrive-Estimating-Travel-Time-Based-on-Deep-Neural-Networks"><a href="#When-Will-You-Arrive-Estimating-Travel-Time-Based-on-Deep-Neural-Networks" class="headerlink" title="When Will You Arrive? Estimating Travel Time Based on Deep Neural Networks"></a>When Will You Arrive? Estimating Travel Time Based on Deep Neural Networks</h4><ul><li><strong>模型框架</strong>：<!-- ![image.png](attachment:image.png) --></li></ul><h4 id="Learning-to-Estimate-the-Travel-Time"><a href="#Learning-to-Estimate-the-Travel-Time" class="headerlink" title="Learning to Estimate the Travel Time"></a>Learning to Estimate the Travel Time</h4><ul><li>用规则模型计算 ETA 是此前地图行业通用做法之一。即分别计算各段路的行驶时间，全部加起来再根据红绿灯时间做一个偏移修正。<!-- ![image.png](attachment:image.png) -->考虑到路段的通行状态每时每刻都在动态变化，一个比较实际的做法是利用最新的历史数据（比如，刚刚过去的 5 分钟）来估计路段的实时通行时间，而把历史平均通行时间作为默认值来填充信息缺失的路段（若一个路段在最近没有滴滴车辆经过，此时它的通行状态是未知的）。<br>从类型上看，ETA 是一个很典型的回归问题<br>将 <strong>MAPE</strong>（mean absolute percentage error）选择为目标函数<br>在业界比较流行的模型，Tree Based model 和 Factorization Machine<br>将深度学习应用到 ETA 上。这一 ETA 模型的核心思路是 global model + recurrent model。其中 global model 的作用类似于上一代模型，针对行程的全局信息进行学习；而 recurrent model 则专注于对 link 序列等局部细节的学习。<br>WD 模型的大体结构：  <!-- ![image.png](attachment:image.png) -->其 Wide 分支其实和 FM 是源出一脉的，对特征进行二阶交叉，对历史数据拥有一定的记忆功能。而它的 Deep 分支就是传统的多层感知机结构，有较好的泛化能力。<br>Recurrent model 的选择则比较丰富，不仅仅限于 RNN（包括变种 GRU、LSTM、SRU 等），还可以是一维卷积 CNN，或者是纯粹的 Attention model。  <!-- ![image.png](attachment:image.png) -->这一模型总共有<strong>三类特征</strong>：  </li><li>Dense feature：行程级别的实数特征，比如起终点球面距离、起终点 GPS 坐标等。  </li><li>Sparse feature：行程级别的离散特征，比如时间片编号、星期几、天气类型等。  </li><li>Sequential feature：link 级别的特征，实数特征直接输入模型，而离散特征先做 embedding 再输入模型。注意，这里不再是每个行程一个特征向量，而是行程中每条 link 都有一个特征向量。比如，link 的长度、车道数、功能等级、实时通行速度等。<br>其中，Wide 和 Deep 模块对行程的整体信息进行建模，而 Recurrent 模块对行程的轨迹进行细致的建模，可以捕捉到每条 link、每个路口的信息。在最终汇总时，Wide 模块通过仿射变换把输出变到合适维度，Deep 模块直接把顶层 hidden state 作为输出，而 Recurrent 模块将 LSTM 的最后一个 hidden state 作为输出。三个模块的输出向量被拼接起来，进入最终的 Regressor 进行预测，得到 ETA 值。全部参数都基于 MAPE loss 做梯度下降来训练。<br>整个系统的架构  <!-- ![image.png](attachment:image.png) -->最底层为数据源，分别是地图信息、GPS 轨迹、订单记录和其它必要的附加信息。然后，原始数据经过特定的处理，变为模型可用的格式，用于训练模型。注意这里有一个小分支，表示在训练完成之后还单独取出了一小批 up-to-date 数据进行 finetune，使得模型更倾向于最新收集的数据。  </li></ul><h4 id="论文总结表格："><a href="#论文总结表格：" class="headerlink" title="论文总结表格："></a>论文总结表格：</h4><table><thead><tr><th align="center">论文</th><th align="center">算法</th></tr></thead><tbody><tr><td align="center">单元格</td><td align="center">单元格</td></tr><tr><td align="center">单元格</td><td align="center">单元格</td></tr></tbody></table><h3 id="存在的挑战"><a href="#存在的挑战" class="headerlink" title="存在的挑战"></a>存在的挑战</h3><ol><li>存储和处理大数据，以及处理糟糕的数据质量和丢失的值[5]</li><li>怎么估计一条路径的旅行时间？一种是整条路径进行总体估计，一种是分段进行估计，然后最后对他们求和来获得总的旅行时间</li><li>多种复杂因素：交通受到空间的影响相关性，时间依赖性和外部因素。</li></ol><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li>Jeong, Ranhee, and R. Rilett. “Bus arrival time prediction using artificial neural network model.” Proceedings. The 7th International IEEE Conference on Intelligent Transportation Systems (IEEE Cat. No. 04TH8749). IEEE, 2004.</li><li>网址：<a href="https://cloud.tencent.com/developer/news/321755" target="_blank" rel="noopener">https://cloud.tencent.com/developer/news/321755</a></li><li>Duc-Duy Nguyen, Chan Le Van, and Muhammad Intizar Ali. 2018. Vessel Destination and Arrival Time Prediction with Sequence-to-Sequence Models over Spatial Grid. In Proceedings of the 12th ACM International Conference on Distributed and Event-based Systems (DEBS ’18). Association for Computing Machinery, New York, NY, USA, 217–220. DOI:<a href="https://doi.org/10.1145/3210284.3220507" target="_blank" rel="noopener">https://doi.org/10.1145/3210284.3220507</a></li><li>Nguyen, D. D., Le Van, C., &amp; Ali, M. I. (2018, June). Vessel destination and arrival time prediction with sequence-to-sequence models over spatial grid. In Proceedings of the 12th ACM International Conference on Distributed and Event-based Systems (pp. 217-220).</li><li>Dobrkovic, A., Iacob, M. E., van Hillegersberg, J., Mes, M. R., &amp; Glandrup, M. (2016). Towards an approach for long term AIS-based prediction of vessel arrival times. In Logistics and Supply Chain Innovation (pp. 281-294). Springer, Cham.</li><li>Bodunov, O., Schmidt, F., Martin, A., Brito, A., &amp; Fetzer, C. (2018, June). Real-time Destination and ETA Prediction for Maritime Traffic. In Proceedings of the 12th ACM International Conference on Distributed and Event-based Systems (pp. 198-201).</li><li>Roşca, V., Onica, E., Diac, P., &amp; Amariei, C. (2018, June). Predicting Destinations by Nearest Neighbor Search on Training Vessel Routes. In Proceedings of the 12th ACM International Conference on Distributed and Event-based Systems (pp. 224-225).</li><li>Forecast of Marine Traffic Volume using Time Series Model</li><li>Travel Time Estimation of a Path using Sparse Trajectories </li></ol><h2 id="船运到达时间预测比赛–数据集思考"><a href="#船运到达时间预测比赛–数据集思考" class="headerlink" title="船运到达时间预测比赛–数据集思考"></a>船运到达时间预测比赛–数据集思考</h2><h3 id="比赛中数据存在的问题"><a href="#比赛中数据存在的问题" class="headerlink" title="比赛中数据存在的问题"></a>比赛中数据存在的问题</h3><ol><li><p>历史运单GPS数据中可能会有异常的GPS：</p><ul><li>GPS坐标在陆地，或者有些港口是内陆的港口。</li><li>GPS漂移：两点距离过大，超过船的行驶能力。</li><li>GPS在部分地区的比较稀疏（比如南半球、敏感海域）。</li><li>最后的GPS点可能和港口的距离较远（比如塞港时，或者临近目的港时已无GPS数据）。</li><li>speed字段之后数据可能会有少量缺失（如GPS设备短暂异常）。</li></ul></li></ol><ol start="2"><li><p>历史运单事件数据</p><ul><li>数据大多手工录入，大多不准确，可以考虑作为辅助数据</li></ul></li></ol><ol start="3"><li><p>港口坐标数据</p><ul><li>一个港口由于客观原因可能会有多个NAME表示，且不按五位编码表示</li><li>重点为NAME和经纬度坐标</li><li>0.831116，-0.386592为不完整数据，如要使用，需要自己补充</li></ul></li></ol><h3 id="清洗数据需要思考的问题"><a href="#清洗数据需要思考的问题" class="headerlink" title="清洗数据需要思考的问题"></a>清洗数据需要思考的问题</h3><ol><li>出发港与目的港是否正确，到港时间用什么规则？哪类数据可以丢弃？</li><li>训练数据中有很多缺失值和异常值，而且港口名格式不统一，到港时间不稳定，船速与经纬度的变化有可能不一致，因此需要根据一些合理可靠的信息来判断真正的到港时间。</li><li>在原始数据集中，数据缺失较多还含有一些不合理的值，所以我们可以考虑是否利用这些数据集（坏样本对模型影响很大），若利用则应该怎么进行填充，以及用多少。</li><li>删除掉信息缺失较多的数据，以及去掉一切不合理的数据，或者做一些填充</li><li>数据清洗，比如说速度在走经纬度不走、经纬度在走速度为0、一个港口有多个名字、实际航线和路由不符合等等</li><li>比赛提供的数据量非常大，但其中夹杂着重复数据以及缺失值、异常值。仅使用了历史运单GPS数据集进行训练，且只保留了运单号、时间戳、经纬度字段的数据</li><li>对最终船的状态需要做一些具体判断，比如：实际上有部分船速度为0，但仍有记录，具体构建方法可以自己再斟酌一下。</li><li>清洗数据方面可以从根据运单聚合后的航行平均速度、数据空值等方面进行考虑</li><li>比如说经纬度变化而速度为0、方向异常，训练集与测试集也可以尽量保留相同清洗处理，同时要保证测试集运单号没有减少。</li></ol><p><strong>总结</strong>：</p><h3 id="数据标签需要考虑的问题"><a href="#数据标签需要考虑的问题" class="headerlink" title="数据标签需要考虑的问题"></a>数据标签需要考虑的问题</h3><ol><li>需要考虑用哪些数据来打标，还可以尝试不同的打标方式，以及分析数据集中可能存在的情况，比如：测试集给的路由是A到B，但实际上可能有中间港口C，而测试集实际离港时间给的C，打标时需考虑这种问题。</li><li>目前最优模型使用的就是baseline提供的打标方式；我们也尝试过使用运单最后一条的时间减去当前GPS样本的时间作为标签。最后预测的时候就用当前GPS时间加上预测的值，作为最终结果。但是这种打标方式对于groupBy后的统计特征就很不友好，后边还会提取其它特征试试。</li><li>标签的方法可以直接参考baseline，但是为了方便计算得分，可以转成以小时为单位的label。</li></ol><h3 id="基线选取"><a href="#基线选取" class="headerlink" title="基线选取"></a>基线选取</h3><p>baseline：</p><ol><li>lgb<ul><li>一个快速的，分布式的，高性能的基于决策树算法的梯度提升框架。可用于排序，分类，回归以及很多其他的机器学习任务中。梯度增强决策树(Gradient Boosting Decision Tree, GBDT)的一种框架，使用GOSS和EFB将新的GBDT实现称为LightGBM。</li><li>初赛Baseline<ul><li>Baseline只取前1000000行数据作为训练</li><li>由于采用聚合运算，因此最后测试结果相同订单的预测时间相同</li><li>选取经纬度、速度、方向做简单特征</li><li>模型采用lightgbm+kf作为模型训练</li><li>本Baseline旨在完成一个完整的流程，具体特征、label的选择需要其本人自己改进</li><li>数据放在’data/‘文件夹下即可</li><li>经测试得分176495.2554，误差很大</li></ul></li></ul></li><li>ann</li><li>深度学习</li></ol><h3 id="特征选取需要思考的问题"><a href="#特征选取需要思考的问题" class="headerlink" title="特征选取需要思考的问题"></a>特征选取需要思考的问题</h3><ol><li>平均速度、距离等都是可以考虑的重要特征。</li><li>尝试构建一些距离特征，平均速度，平均加速度等特征。</li><li>以距离、速度、时间等统计量为主，也需要进行特征选择来调优</li><li>特征工程部分只是用到了一些基本的特征，比如经纬度、速度之类的特征。这部分比较玄学，有时候增加特征，线下MSE降低，线上反而增加了。建议找一个稳定的验证集，来测试模型性能。</li><li>主要用的还是原始数据中的特征以及根据经纬度计算位移。</li><li>构造训练集时数据量不必很大，我们只选择了与测试集到达相同港口的那些订单数据作为训练集。特征选取也不必很多，平均速度、距离等都是可以考虑的重要特征。</li></ol><p><strong>总结</strong>：<br>特征选取  </p><ul><li>适当的把所有有用的特征先选出来，然后可以可以通过实验来进行挑选合适的特征，或者加上权重划分？  </li><li>数据中有的特征：距离、速度、时间、经纬度等等   </li><li>需要额外计算的特征：平均速度、平均加速度等等  </li></ul><h3 id="解决方案的思考"><a href="#解决方案的思考" class="headerlink" title="解决方案的思考"></a>解决方案的思考</h3><ol><li>对测试集中每条需要预测的运单，找出训练集中与之相似的轨迹的集合，然后在相似轨迹集上训练模型预测结果。寻找相似轨迹算法的好坏对训练结果有很大的影响。</li></ol><h3 id="比赛的难点"><a href="#比赛的难点" class="headerlink" title="比赛的难点"></a>比赛的难点</h3><ol><li>数据问题，数据虽然很大，但是有效数据并不多。数据存在大连数据不准或者缺失、错误等问题。</li><li>线上线下不一致：这个问题一直困扰了我们，现在都没有找到可靠验证方式，只能继续使用5折交叉。原因可能是测试集的分布序列比较特殊。因为错过比赛，所以无法进行线上验证</li></ol><p>注： 如果推理代码中没有写def _inference(self, data) 可以试试尝试重新写这类推理代码，其中建议使用logger.info进行信息查询，这样可以一步步排除问题，花费时间也最多只需要半天时间。原因是推理代码出现问题我遇到过，重写def _inference(self, data)就好了，这类问题还是建议手动排查为好。<br>可能是推理代码中的测试数据的列顺序和baseline代码中输入的训练数据的列的顺序不一致导致的吧。比如推理代码中的是[‘weekday’,’timeindex’],而baseline中训练时输入是[‘timeindex’,’weekday’]<br>试试看手动指定一下列的顺序， 可能是输入的特征顺序乱了<br>baseline中训练集和测试集是随机划分的，应该是这个原因吧。</p><h3 id="评估标准"><a href="#评估标准" class="headerlink" title="评估标准"></a>评估标准</h3><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>ETA_NUM取的是是测试集的行数34712，预测的ETA数量<br>ETA为选手评估的时间值<br>对于未提交的运单ETA，后台统一取timestamp时间计算。</p><p>主要是根据运单号、GPS时间戳即timestamp匹配ETA值，与真实的ATA做MSE</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> taxi demand </tag>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记-出租车需求预测5</title>
      <link href="/posts/45610/"/>
      <url>/posts/45610/</url>
      
        <content type="html"><![CDATA[<h1 id="Combining-time-series-and-textual-data-for-taxi-demand-prediction-in-event-areas-a-deep-learning-approach"><a href="#Combining-time-series-and-textual-data-for-taxi-demand-prediction-in-event-areas-a-deep-learning-approach" class="headerlink" title="Combining time-series and textual data for taxi demand prediction in event areas: a deep learning approach"></a>Combining time-series and textual data for taxi demand prediction in event areas: a deep learning approach</h1><p>论文题目：结合时间序列和文本数据以预测活动区域中的出租车需求：深度学习方法</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>翻译</strong>：<br>准确的时间序列预测对于交通，能源，金融，经济等众多应用领域至关重要。但是，尽管现代技术能够探索大量的时间数据以建立预测模型，但它们通常会忽略有价值的信息通常以非结构化文本的形式提供。尽管此数据的格式完全不同，但它通常包含在时间数据中观察到的许多模式的上下文解释。在本文中，我们提出了两种深度学习架构，这些架构利用单词嵌入，卷积层和注意力机制将文本信息与时间序列数据相结合。我们将这些方法应用于活动区域的出租车需求预测问题。通过使用纽约公开提供的出租车数据，我们经验表明，通过融合这两个互补的交叉模式信息源，所提出的模型能够显着减少预测中的误差。</p><p><strong>总结</strong>：</p><ul><li>提出了两种深度学习架构，这些架构利用单词嵌入，卷积层和注意力机制将文本信息与时间序列数据相结合。</li><li>方法应用于活动区域的出租车需求预测问题</li><li>使用纽约公开提供的出租车数据</li><li>融合这两个互补的交叉模式信息源，所提出的模型能够显着减少预测中的误差。</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><ul><li><p>典型的方法仅集中于捕获与习惯/例行行为有关的复发性移动趋势[1]，以及利用与近期观察模式的短期相关性[2，3]。</p><ul><li>这种方法对于长期规划应用或在诸如居民区这样的非重要区域，在容易发生多个特殊事件（例如音乐演唱会）的活跃且高度动态的区域中对需求进行建模可以成功</li><li>体育比赛，节日，游行和抗议，这些方法无法准确地模拟出行需求[4]。</li><li>这种情况涉及需求激增，这强调了良好的预期能力的需求。</li></ul></li><li><p>大多数事件信息通常采用非结构化自然语言文本的形式。</p></li><li><p>跨域数据融合挑战问题</p></li><li><p>在时序数据中观察到的来自Web的文本数据可以提供上下文来解释某些模式</p></li><li><p>将时间序列和文本数据相结合，以预测事件区域中的移动需求。</p></li><li><p>深度学习已被证明能够胜过传统的出租车需求预测方法，并获得最新的最新成果[9，10]</p></li><li><p>以前的方法都没有探索有关事件的Web数据，特别是非结构化文本形式的Web数据，以便开发更准确的需求预测模型。</p></li><li><p>提出的神经网络体系结构结合了词嵌入，卷积层和注意机制，以构建事件文本描述和历史出租车需求数据的联合模型，用于未来需求预测。而且，这两个提议的体系结构非常通用，可以应用于来自不同域的类似数据融合问题。</p></li><li><p>使用来自纽约的11亿次出租车出行的大规模公共数据集和来自Web的事件数据</p></li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="城市交通和特殊事件"><a href="#城市交通和特殊事件" class="headerlink" title="城市交通和特殊事件"></a>城市交通和特殊事件</h3><ul><li><p>可伸缩的需求预测解决方案（如本文中提出的需求解决方案）基于从Web上自动提取的事件数据，然后为自己展现出预测事件影响并为事件和动态区域提供可靠预测的潜力。</p></li><li><p>4步模型：基于调查响应数据，他们针对每种事件预测出行的次数，类型，行进时间，行进起点/目的地（OD），模式和由于这些事件而产生的行车里程/过境登机牌。<br>缺点：没有考虑事件特征。事件特征会影响对移动性的影响。</p></li><li><p>作者凭经验表明，事件类别在可用时确实是预测公共交通需求的有价值的输入。</p></li><li><p>推广这些方法的<strong>关键困难</strong>在于，大多数相关事件信息通常都以非结构化文本的形式提供</p></li><li><p><strong>解决方法</strong>：开发跨域数据融合技术，以便将这些信息纳入需求预测模型中。</p></li></ul><h3 id="时间序列和文本的数据融合"><a href="#时间序列和文本的数据融合" class="headerlink" title="时间序列和文本的数据融合"></a>时间序列和文本的数据融合</h3><ul><li><p>一个非常重要但具有挑战性的问题：将时间序列数据与文本信息相结合以更好地理解现实世界中的现象</p></li><li><p>文本信息可能包含与时间序列观察相关的线索，并且至少在某种程度上解释了它们的行为。 </p></li><li><p>跨域数据融合问题的普遍性</p></li><li><p>金融，Ruiz等人[22]分析了Twitter中微博活动之间的相关性，例如与公司相关的推文和转发的数量以及股市事件。 他们的结果表明，这些指标对于改善股票市场的交易策略很有用。</p></li><li><p>一种方法来分析新闻报道中某些词的存在，这些词是从网络上自动提取的。 为了选择相关的单词，作者依赖于一组用于识别相关文档的手动预定义单词，然后允许他们根据其概率比（PR）选择最相关的单词。</p></li><li><p>一个结合ARIMA和支持向量回归（SVR）的模型来预测六家证券公司的季度权益收益率（ROE）这个想法是ARIMA模型能够分析序列的线性自回归分量，但仅基于文本特征向量的非线性SVR模型可以对其进行补充。 在这种情况下，作者依靠TF-IDF来表示文本数据。</p></li><li><p>研究了推文中的文本情感与民意测验的时间序列数据之间的关系。 通过使用10亿条推文的数据集，作者使用一组预先定义的关键字来标识与某些主题相关的推文，例如奥巴马与麦凯恩当选。 然后根据单词情感词典根据它们包含的单词的情感来分析这些单词。 获得的结果表明，与某些情感相关的单词频率可以与民意测验密切相关（最多80％）</p></li><li><p>Twitter被广泛用于政治审议，而只有少数党的提及准确地反映了选举结果。</p></li><li><p>通过解释文本数据中的信息以解释观察的时间序列，所有这些工作都与本文提出的工作有关。 </p></li><li><p>但是它们通过依赖于与某些主题或情感相关的单个单词的预定词典而明显不同。 </p></li><li><p>本文提出的方法使用卷积神经网络层在联合建模方法中自动学习与观察到的时间序列数据相关的单词模式或序列，从而避免使用临时启发式方法，例如情感词频率和预先定义的词典 。</p></li><li><p>研究了使用事件数据来帮助预测公共交通需求的问题</p></li><li><p>包括使用事件描述的词袋表示来学习主题模型（即潜在狄利克雷分配（LDA）），并将主题分配用作浅层神经网络模型的特征。</p></li></ul><p>本文的优势：</p><ul><li>不依赖词袋表示，因此不会忽略词的顺序。能够捕获文本中的多单词模式。</li></ul><p>数据融合的方法（三大类）：</p><ol><li>平等对待不同数据源并将从中提取的特征汇总到一个单一特征向量中的方法</li><li>在不同阶段使用不同数据源的方法</li><li>将不同的数据集同时馈入模型的不同部分的方法</li></ol><p>本文提出的深度学习模型属于第3类。</p><h3 id="交通领域中的深度学习"><a href="#交通领域中的深度学习" class="headerlink" title="交通领域中的深度学习"></a>交通领域中的深度学习</h3><ul><li><p>使用堆叠式自动编码器（SAE）模型来学习通用的流量特征，然后将其用于流量预测。</p></li><li><p>根据来自全加州各州高速公路系统中部署的15.000个单独流量检测器的数据，他们的经验结果表明，SAE的性能优于基于SVR和径向基函数（RBF）的更为传统的方法。</p></li><li><p>研究了长期短期记忆（LSTM）网络在行进速度预测中的使用。 他们从北京获得的经验结果表明，LSTM优于其他方法，例如ARIMA和SVR，作者证明LSTM具有捕获时间序列中长期依赖关系的能力。</p></li><li><p>一种基于卷积层和残差连接的深度学习架构，用于联合预测整个城市范围内的人群流量，即区域之间的流入和流出。利用北京的出租车数据和纽约市的自行车共享数据，作者凭经验表明，他们提出的体系结构优于ARIMA和矢量自回归模型等标准方法。</p></li><li><p>LSTM和混合密度网络（MDN）的组合来预测纽约市出租车的需求。在他们的方法中，先将城市划分为较小的区域，然后使用基于LSTM的模型来联合预测所有区域中下一时间段（20-60分钟）的出租车需求。</p></li></ul><p><strong>缺点</strong>：均未考虑事件的影响以改善其预测</p><p><strong>本文的目标</strong>：旨在通过关注事件区域并显示将事件的文本数据与流动性需求的时间序列观察相结合的数据融合技术来显着改善预测，从而弥合这一差距。</p><h2 id="提议方法"><a href="#提议方法" class="headerlink" title="提议方法"></a>提议方法</h2><h3 id="时间序列趋势（Time-series-detrending）"><a href="#时间序列趋势（Time-series-detrending）" class="headerlink" title="时间序列趋势（Time-series detrending）"></a>时间序列趋势（Time-series detrending）</h3><ul><li><p>准备时间序列数据以供分析的最重要步骤之一就是降低趋势</p></li><li><p>识别这些每日或每周重复模式的简单但非常有效的方法是构建历史平均值模型，该模型包括根据历史数据（来自）计算每对（一天中的时间，一周中的某天）对的个人平均值。【仅限火车】</p></li><li><p>历史平均值代表固定的重复趋势，可以很容易地将其从数据中删除。</p></li><li><p>预测模型的目标是学习预测趋势消除所导致的残差。</p></li><li><p>根据我们在处理城市流动性数据的各种时间序列预测问题方面的经验，通过消除从模型中捕获这些众所周知的经常性趋势的负担，这种简单的去趋势程序可以显着提高预测性能。</p></li></ul><h3 id="文本数据预处理（Text-data-preprocessing-）"><a href="#文本数据预处理（Text-data-preprocessing-）" class="headerlink" title="文本数据预处理（Text data preprocessing ）"></a>文本数据预处理（Text data preprocessing ）</h3><p>遵循一个简单的常规文本预处理管道，该管道包括：</p><ul><li><p>HTML标记删除-在处理从Web提取的数据时，这尤其重要；</p></li><li><p>小写转换-减少单词在文本中出现方式的可变性；</p></li><li><p>标记化-将字符序列分成称为标记的部分；</p></li><li><p>合法化-删除单词的惯用词尾并返回单词的基数或字典形式，这称为引理/论点；</p></li><li><p>删除停用词和非常常用的词-删除诸如“ a”，“ the”和“ and”之类的词，这些词通常不会带来任何其他有用的信息；</p></li><li><p>删除仅出现一次的单词-就像非常频繁的单词一样，这些单词通常信息性不强； 【减少词汇量，降低神经网络的复杂性，还不会降低预测性能】</p></li><li><p>使用的词嵌入技术有望解决文本预处理管道所涵盖的某些问题</p></li><li><p>但在预处理阶段进行这些转换仍然是一种更安全，更可取的方法。显着减少了嵌入层使用的词汇量-具有计算优势。</p></li></ul><h3 id="神经网络架构"><a href="#神经网络架构" class="headerlink" title="神经网络架构"></a>神经网络架构</h3><p>两种提议的体系结构</p><p> 共同点：将文本数据合并到模型中的方式</p><ul><li>由单词嵌入和卷积层组成。</li><li>网络的文本输入是一维向量，每个元素由文本中相应单词的整数标识符组成，因此一个单词的多次出现都被赋予相同的标识符。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><ul><li><p>输入到网络的文本对应于原始文本（在前面所述的文本预处理之后），其形式为单词标识符（整数）的一维向量。</p></li><li><p>向量的长度对应于预定的最大长度S，该长度通常设置为序列集中最长的输入单词序列。</p></li><li><p>表示文本序列的整数矢量进入单词嵌入层。</p></li><li><p><strong>词嵌入</strong>的想法是通过为字典中的每个词分配一个数字向量，从而将语义含义映射到几何空间中，这样嵌入空间中任意两个向量之间的距离就可以捕获它们对应词之间的语义关系。另一方面，在语义上非常不同的单词之间的嵌入空间中的L2距离应该很大。好的单词嵌入还允许在单词嵌入向量之间进行语义上有意义的操作。</p></li><li><p>单词嵌入通常是通过将无监督学习技术应用于非常大的语料库中单词之间的共现统计数据集而获得的。</p></li></ul><ul><li><p>本文使用了流行的<strong>GloVe嵌入</strong></p><ul><li>该嵌入是通过将矩阵分解技术应用于2014年英语版维基百科的转储而获得的。</li><li>基于整个语料库中单词共现频率的矩阵，作者学习单词矢量，使其点积等于单词共现概率的对数，以加权最小二乘目标表示 。</li><li>然后，使用GloVe产生的单词embeddings来初始化我们提出的深度学习架构中的embeddings层。 </li><li>我们为每个单词使用300维嵌入向量。</li></ul></li><li><p>文本中单词嵌入的数字矢量在大小为$S×1×300$的3D张量中放在一起</p><ul><li>其中S表示最大序列长度。</li></ul></li><li><p>然后将这个张量传递给一系列一维卷积过滤器和最大池化层，这些层能够学习以不同的抽象级别<strong>检测文本中的某些模式</strong>，类似于卷积神经网络（CNN）学习检测图像中的特征。</p><ul><li>张量的最后一个维度（300）如何对应于CNN文献中的“深度”或“通道数”。</li></ul></li><li><p>本文使用<strong>三个卷积层</strong>的序列，分别具有3×3、3×3和5×5的大小，以及50、30和30个过滤器。每个卷积层都使用ReLU激活，然后是一个最大池层，其大小与前面的卷积相同。在卷积层之间，我们以50％的概率保持连接而应用Dropout [31]。最后一层的输出经过一个（可选）注意机制，然后与网络从学习到的时序数据表示形式相结合。</p></li></ul><p>图3的顶部说明了用于对文本数据进行建模的神经网络体系结构。<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>由于所提出的体系结构是基于卷积层的事实（并且由于卷积的性质），因此它能够<strong>捕获任意大小的输入文本序列中的可变长度模式</strong>。</p><ul><li>开发了一种简单的<strong>软注意力机制</strong>，更加强调最突出的文本信息。<ul><li>参考论文：K. Cho, A. Courville, Y. Bengio, Describing multimedia content using attention-based encoderdecoder networks, IEEE Transactions on Multimedia 17 (11) (2015) 1875–1886. </li></ul></li></ul><ul><li>该注意机制将最后一个卷积层的文本表示形式（称为$z$）作为输入，并基于时间序列数据的潜在表示所提供的上下文$c$，可以<strong>确定文本的哪些部分与$t + 1$预测更相关</strong>。具体而言，关注层$h$的输出由元素乘积$h =\alpha \bigodot z$给出，其中权重$\alpha_i$由softmax函数给出：<br>$$<br>\alpha_i = \frac{exp(e_i)}{\sum_{k=1}^K exp(e_k)}\<br>$$</li><li>其中$e_i = a_i(z_i,c)$是一个学习的相关函数，在给定上下文向量c的情况下对提取的文本特征$z_i$进行评分，并定义为具有双曲正切（tanh）激活的完全连接层。</li></ul><p>为了对时间序列数据建模，本文考虑了两种不同的方法：</p><ol><li>一种基于循环神经网络（RNN），即<strong>LSTM</strong></li><li>另一种基于<strong>完全连接（FC）密集层</strong>。</li></ol><p><strong>利用LSTM的数据融合体系结构</strong>：  </p><ul><li><strong>DL-LSTM</strong>  </li><li><strong>使用LSTM编码时间序列数据</strong>  </li></ul><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p><strong>步骤</strong>：</p><ul><li><p>时间序列数据与二进制变量向量一起进入LSTM层，该向量指示时间序列中时间t处的对应值是否具有事件，因此LSTM的隐藏状态单元还可以将该信息考虑在内以确定下一个状态。</p></li><li><p>为了防止过度拟合，我们对LSTM的权重应用了正则化。 </p></li><li><p>然后对应于该LSTM单元的最后一个隐藏状态矢量被假定为已编码的需要进行预测对于$t + 1$相关的时间序列信息。</p></li><li><p>令$z_ts$和$z_text$分别表示所提出的神经网络体系结构对时间序列和文本数据所学习的潜在表示。</p><ul><li>$z_ts$是一个向量，它对应于对时间序列数据进行编码的LSTM（或FC）层的输出</li><li>向量$z_text$则对应于对文本数据进行建模的注意力机制和卷积层的输出。</li></ul></li><li><p>在网络的最后一层，通过完全连接的（密集）层将这两个学习的表示进行组合，该层将计算$t+1$的最终预测，作为权重可调的$w_ts$和$w_text$的两个向量与相应的潜在表示之间的点积（ $z_ts$和$z_text$）。</p></li><li><p>在其他相关信息可用的情况下，例如有关天气的信息或有关事件的其他详细信息，可以很容易地将这些信息作为对模型的最后一个密集层的输入</p></li></ul><p>对于LSTM的分析：  </p><p><strong>优势</strong>：</p><ul><li>研究表明LSTM成功地应用于时间序列预测问题，包括出租车需求预测。</li><li>归因于他们捕获数据中长期依赖关系的能力。  </li></ul><p><strong>劣势</strong>：</p><ul><li>RNN很难训练[8]。</li><li>如果将有效的去趋势技术应用于移动需求的时间序列数据，从而消除了清晰的每日和每周周期性重复模式，那么就可以在很大程度上消除捕获长期依存关系的需求。这可能会使LSTM对于某些时间序列的建模没有吸引力，因为它可能导致过度拟合。</li></ul><p>开发了一种深度学习架构，该架构<strong>依赖于FC层对时间序列数据进行建模</strong>。</p><p>利用<strong>全连接（FC）层</strong>对时间序列数据建模的数据融合体系结构:</p><ul><li><p><strong>DL-FC</strong><br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p></li><li><p>在DL-FC的情况下，所有时间序列信息均以“lagged observations（滞后观察）”或lags. 的形式作为“输入”向量提供给网络。即网络在大小为$L+1$的向量中的时间${t，t-1，…，t-L}$处被馈送观测值，其中$L+1$对应于滞后数。</p></li><li><p>该滞后向量被馈送到具有100-200个隐藏单位和双曲线正切（tanh）激活的FC层，该层还可以接收带有其他相关信息的附加输入，例如天气数据或事件详细信息。</p></li><li><p>然后，此FC层的输出将传递到具有50个单位和tanh激活的第二FC层。</p></li><li><p>在每个FC层之前应用BatchNormalization [34]，在FC层之间应用Dropout，并在必要时使用正则化。</p></li><li><p>最后一个FC层的输出对应于一个潜在向量表示，该向量表示对来自时间序列和其他相关输入（例如天气）的所有必要信息进行编码。</p></li><li><p>然后将该潜在矢量表示与文本数据的潜在表示相结合，以便使用密集层生成t + 1的预测。 通过将去除的重复趋势（基于历史平均值）加回到神经网络的输出中，可以得到最终的预测。 </p></li></ul><ul><li>提议的两种深度学习架构的所有层（用于将时间序列与文本数据融合）的所有层均使用<strong>Adam优化器</strong>[35]和大小为64的mini-batches 通过反向传播进行了训练。</li><li>使用单独的验证集来跟踪 训练期间表现最佳的模型。该验证集还用于指导上述深度学习架构所反映的大多数设计和超参数选择。</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ul><li>Keras提出了将时间序列数据与文本信息相结合的数据融合方法</li><li>考虑有事件的地区的出租车需求预测问题</li></ul><h3 id="数据集和样例研究"><a href="#数据集和样例研究" class="headerlink" title="数据集和样例研究"></a>数据集和样例研究</h3><p><strong>数据集</strong>：</p><ul><li>包括从纽约（2009年1月至2016年6月）的11亿次出租车出行</li><li>查看了纽约市顶级场馆清单[37]，并选择了可在线获取更完整事件记录的两个场馆：巴克莱中心和5号航站楼。<br>位于布鲁克林的巴克莱中心是现代化的 拥有18,000个座位的多功能竞技场，定期举办重要的音乐表演，并成为NBA布鲁克林篮网的新家。<br>5号航站楼是一个三层场地，经常在曼哈顿的中心地带举办许多观众的音乐会。</li></ul><p>考虑到这两个场所的地理坐标，我们选择了在±0.003十进制度度（约500米）的边界框内进行的所有出租车接送作为我们的研究区域。</p><ul><li>单个拾音器 pickups 按每日计数的时间序列分组。</li><li>目标：根据前几天的接载，天气数据和从Web提取的事件信息，来预测该区域第二天的出租车接载的数量。</li></ul><p><strong>气象数据</strong></p><ul><li>从美国国家海洋与大气管理局（NOAA）获得的，并且与位于纽约中央公园的气象站的测量结果相对应。它包含有关最低和最高温度，降水，风，是否有雪，雾，雪深等信息。</li></ul><p><strong>事件数据</strong></p><ul><li><p>使用屏幕抓取技术或API从网络上自动提取的。对于巴克莱中心，事件信息是从其官方网站上抓取的，因为它保留了非常准确和详细的日历，使我们可以及时返回并检索与出租车需求数据匹配的时期的数据。自2012年底启用至2016年6月，我们共收集了751个事件。对于5号航站楼，我们使用Facebook API提取了相同时间段的315个事件。 </p></li><li><p>在这两种情况下，事件数据都包括事件标题，日期，时间和描述。</p></li><li><p>实际上，人们可以轻松地使用许多可用的在线事件目录和聚合器之一。本文选择依赖这些特定目录是为了方便检索过去的事件，通常在常规的在线事件目录中不可用。</p></li><li><p>数据的多样性和异质性。注意描述的长度和质量如何变化。<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p></li><li><p>对于巴克莱中心和5号航站楼，每个事件描述的平均单词数分别为113（标准±114）和20（标准±16）。 实际上，文本中的单词数可以大于500。但是，由于使用卷积层，它可以捕获任意大小的输入文本序列中的模式，因此对于所提出的神经网络体系结构而言，这不是问题。</p></li><li><p>由于某些事件发生在深夜，因此它们可能会导致第二天黎明时出租车需求急剧上升，从而导致第二天的总需求急剧上升。所以导出了其他功能以解决这一问题。</p></li></ul><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><ul><li>对于巴克莱中心，培训集包括21个月的观察结果（2013年1月至2014年9月），验证集包含2014年的剩余数据（3个月），而2015年1月至2016年6月的数据（18个月）为 用于测试不同的方法。</li><li>关于第5终端，由于事件的频率较小，因此我们增加了验证集的大小，以便包含代表性的事件数。 然后，训练集包含2年的数据（2013年和2014年），2015年用于验证，而2016年的其余数据用于测试。 </li></ul><h4 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h4><p>评估不同信息来源的贡献  -增量分析</p><p>仅从负责对时间序列数据建模的网络部分开始，并不断向网络中添加组件，直到获得完整模型。</p><ul><li>从仅将滞后观察或滞后（称为“ L”）作为输入的模型开始</li><li>然后移至还包括以下模型：天气信息（表示为“ L + W”），有关事件存在的信息（“ L + W + E”）</li><li>以及最后，完整的模型还将考虑事件的文本描述（“ L + W + E + T”）。</li></ul><p>在其最简单的版本（“ L”）中，建议的模型DL-LSTM和DL-FC分别等效于使用普通LSTM和标准前馈神经网络方法进行时间序列预测。</p><p>现有的两种最流行的时间序列预测方法进行比较：</p><ul><li><p>支持向量回归（SVR）</p></li><li><p>高斯过程（GPs）</p></li><li><p>SVR方法使用线性核，实验结果表明，它为该特定问题提供了最佳结果。</p></li><li><p>GP，使用了一个复合协方差函数，该函数由平方指数和白噪声协方差函数的和组成。</p></li><li><p>SVR和GP的超参数均基于其在验证集上的性能进行了调整，使用了该组{0.001,0.01,0.1,1.0,10,100}中所有可能组合的详尽搜索过程。</p></li><li><p>对于GP和SVR这两种基线方法，我们还对上述不同信息源对预测性能的贡献进行了相同的增量分析。</p></li></ul><h4 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h4><ul><li>MAE</li><li>RMSE</li><li>MAPE</li><li>$R^2$</li></ul><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><ul><li>SVR和DL-FC是该研究领域中性能最好的方法</li><li>LSTM的优势并没有延续到某些可以通过时间窗口方法解决的更简单的时间序列预测任务。</li></ul><p>分析不同信息来源的贡献：</p><ul><li>包括有关天气的信息可以改善预测。</li><li>对不同方法的预测性能影响最大的信息源是有关事件的信息。</li><li>从DL-LSTM的结果中观察文本信息的贡献</li></ul><p>进行更细粒度的分析</p><p>同一个需求预测问题的实例，但它们领域的固有特征使得某些类型的深度学习架构比其他类型更适合。</p><p>不同天的类型（事件日和非事件日）</p><ul><li>使用有关事件的文本信息可以减少两种类型的预测误差。</li><li>事件日的误差会更大，因为事件日对应于非经常性场景，而这种非固有场景本来就很难预测</li></ul><p>另一数据集</p><ul><li>LSTM是仅对时间序列数据建模的最佳方法，可以通过比较仅使用滞后观测值（或滞后）作为输入（“ L”）的不同模型的结果来验证。</li><li>包括天气信息（“ W”）会提高所有方法的预测性能。</li><li>包含有关事件的存在的信息（“ E”）可以显着减少所有方法的预测误差。</li></ul><p>从数据融合的角度来看，这些结果清楚地凸显了时间序列数据和文本信息的关键数据融合的重要性，特别是对于本文所考虑的事件区域中的出租车需求预测问题。文本数据包含上下文信息，该上下文信息与时间序列数据的某些方面相关并解释了这些方面。</p><p>在预测充满活力的城市地区的出行需求时，应对特殊事件影响的重要性。<br>通过利用从Web上自动提取的事件信息，以及开发两种新颖的深度学习体系结构以将该信息与历史时间序列数据相结合，我们能够在两个研究领域中显着减少事件区域中的预测误差 。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li><p>两种深度学习数据融合架构，该架构将时态数据与非结构化文本形式的信息结合起来，以改善时间序列预测。</p></li><li><p>利用单词嵌入和卷积层来捕获文本中与时间序列观察结果相关的模式，这些模式是使用LSTM或完全连接的层的堆栈进行建模的。</p></li><li><p>本文开发的神经网络体系结构能够构造时间序列和文本数据的潜在表示，然后可以将其组合以产生更准确的预测。</p></li><li><p>考虑了事件区域中出租车需求预测的问题</p></li><li><p>通过利用来自Web的事件信息，提出的深度学习架构能够显着提高其预测的质量，从而显着优于现有技术中不占事件信息的其他流行的时间序列预测方法 。</p></li><li><p>探索如何利用提出的数据融合技术来开发全市范围的时空深度学习模型，以解释有关整个城市发生的所有事件的信息，包括标题和描述中的文字信息，以帮助模型区分有影响的事件和可忽略的事件。</p></li></ul><p>即</p><ol><li>文本信息和时间序列信息融合</li><li>词嵌入和卷积层来捕获文本中与时间序列观察结果相关的模式</li><li>强调了在对移动需求进行建模时需要考虑事件的影响。</li></ol><p><strong>模型总结</strong>：</p><p>文本信息：</p><ul><li>使用词嵌入，从而将文本信息转换成向量形式</li><li>然后利用卷积可以捕获文本信息</li><li>最后使用软注意力机制，可以确定文本的哪些部分与 𝑡+1 预测更相关。</li></ul><p>LSTM：</p><ul><li><p>时间序列数据与二进制变量向量（事件数据）一起进入LSTM层，该向量指示时间序列中时间t处的对应值是否具有事件，因此LSTM的隐藏状态单元还可以将该信息考虑在内以确定下一个状态。然后对应于该LSTM单元的最后一个隐藏状态矢量被假定为已编码的需要进行预测对于 𝑡+1 相关的时间序列信息。</p></li><li><p>在其他相关信息可用的情况下，例如有关天气的信息或有关事件的其他详细信息，可以很容易地将这些信息作为对模型的最后一个密集层的输入</p></li></ul><p>FN：</p><ul><li><p>所有时间序列信息均以“lagged observations（滞后观察）”或lags. 的形式作为“输入”向量提供给网络。即网络在大小为 𝐿+1 的向量中的时间 𝑡，𝑡−1，…，𝑡−𝐿 处被馈送观测值，其中 𝐿+1 对应于滞后数。</p></li><li><p>该滞后向量被馈送到具有100-200个隐藏单位和双曲线正切（tanh）激活的FC层，该层还可以接收带有其他相关信息的附加输入，例如天气数据或事件详细信息。</p></li><li><p>然后，此FC层的输出将传递到具有50个单位和tanh激活的第二FC层。</p></li><li><p>最后一个FC层的输出对应于一个潜在向量表示，该向量表示对来自时间序列和其他相关输入（例如天气）的所有必要信息进行编码。</p></li><li><p>然后将该潜在矢量表示与文本数据的潜在表示相结合，以便使用密集层生成t + 1的预测。 通过将去除的重复趋势（基于历史平均值）加回到神经网络的输出中，可以得到最终的预测。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> taxi demand </tag>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记-出租车需求预测4</title>
      <link href="/posts/25088/"/>
      <url>/posts/25088/</url>
      
        <content type="html"><![CDATA[<h1 id="Deep-Multi-View-Spatial-Temporal-Network-for-Taxi-Demand-Prediction"><a href="#Deep-Multi-View-Spatial-Temporal-Network-for-Taxi-Demand-Prediction" class="headerlink" title="Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction"></a>Deep Multi-View Spatial-Temporal Network for Taxi Demand Prediction</h1><p>论文题目：深度多视图时空网络用于出租车需求预测</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>传统的需求预测方法大多依赖于时间序列预测技术，这些技术无法对复杂的非线性时空关系建模。现有的交通预测方法只考虑了空间关系（例如，使用CNN）或时间关系（例如，使用LSTM）。我们提出了一个深度多视图时空网络（DMVST-Net）框架来对时空关系进行建模。</p><p>提出的模型包含三个视图：</p><ul><li>时间视图（通过LSTM建模具有近时间点的未来需求值之间的相关性）</li><li>空间视图（通过本地CNN建模局部空间相关性）</li><li>语义视图（在共享相似时间模式的区域之间建模相关性）</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><ul><li><p>出租车需求预测问题：如何通过使用历史出租车请求数据来预测未来时间戳中某个地区的出租车请求数量。</p></li><li><p>本文利用联合模型捕获CNN和LSTM的功能，该模型捕获了时空的复杂非线性关系。 但是，我们不能简单地将CNN和LSTM应用于需求预测问题。 </p></li><li><p>将相关性较弱的区域包括在内以预测目标区域实际上会损害性能。 为了解决这个问题，我们提出了一种新颖的<strong>局部CNN</strong>方法，该方法仅考虑空间附近的区域。</p></li><li><p>这种本地的CNN方法是受《第一地理学》的启发：“近处的事物比远处的事物更相关”，并且根据真实数据的观察结果也证明了<strong>需求模式与空间紧密区域的相关性更高</strong>。</p></li><li><p>尽管localCNN方法过滤的边远地区相关性较弱，但这未能考虑到两个位置在空间上可能相距遥远但需求模式相似（即在语义空间上）的情况。</p></li><li><p>建议使用区域图来捕获这种潜在的语义，其中边缘代表需求的相似性 之后，通过图嵌入方法将区域编码为向量，并将这些向量用作模型中的上下文特征。最后，使用完全连接的神经网络组件进行预测。</p></li><li><p>通过滴滴出行的大规模真实世界滑行需求数据进行了验证。 该数据集包含两个月内通过中国广州市的滴滴服务的出租车需求请求，平均每天约300,000个请求。</p></li></ul><p>本文的贡献：</p><ul><li>提出了一个统一的多视图模型，该模型共同考虑了空间，时间和语义关系。</li><li>提出了一个local CNN模型，该模型捕获相对于其邻居的区域的局部特征。</li><li>基于需求模式的相似性构建了一个区域图，以便对相关但空间上遥远的区域进行建模。通过图嵌入学习区域的潜在语义。</li><li>对滴滴出行的大型出租车请求数据集进行了广泛的实验。 结果表明，我们的方法始终优于竞争基准。</li></ul><h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><ul><li>不重叠的位置集 $L = {l_1,l_2,…,l_i,…,l_N}$作为城市的矩形分区【也可以使用更复杂的分区方式，例如通过路网划分空间或六角形分区。】</li><li>时间间隔集 $ I = { I_0,I_1,…,I_t,…,I_T}$</li></ul><p><strong>Taxi request</strong>:<br>出租车请求$o$定义为元组$(o.t，o.l，o.u)$<br>其中</p><ul><li>$o.t$是时间戳</li><li>$o.l$代表位置</li><li>$o.u$是用户标识号。请求者标识用于过滤重复的和垃圾邮件的请求。</li></ul><p><strong>Demand</strong>:<br>定义为<strong>每个时间点在一个位置的出租车请求的数量</strong>,即$y_t^i = | {o:o_t \in I_t \bigwedge o_l \in l_i} |$</p><ul><li>其中$|·|$表示集合的基数</li><li>使用$I_t$代表它的时间间隔t的索引</li><li>$l_i$代表位置$i$处的的位置索引</li></ul><p><strong>Demand prediction problem</strong>: </p><ul><li>目的：<strong>给定直到时间间隔t的数据来预测在时间间隔$t +1$处的需求</strong>。</li><li>除了历史需求数据之外，我们还可以合并上下文特征，例如时间特征，空间特征，气象特征。 </li><li>将位置$i$和时间点$t$的那些上下文特征表示为向量$e_t^i \in \mathbb R^r$<ul><li>其中$r$是特征的数量。</li></ul></li></ul><p>最终目标是预测：</p><p>$$<br>y_{t+1}^i = F( \displaystyle y_{t-h,…,t}^L, \displaystyle \varepsilon_{t-h,…,t}^L)<br>$$</p><ul><li>$i \in L$</li><li>$y_{t-h,…,t}^L$ 历史需求</li><li>$\varepsilon_{t-h,…,t}^L$ 表示在从$t_h$到$t$的时间间隔内，所有位置$L$的上下文特征，其中$t_h$表示开始时间间隔。</li><li>在所有区域和直到$t_h$的先前时间间隔上定义了预测函数$F(·)$，以捕获它们之间的复杂时空相互作用。</li></ul><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>Deep Multi-View Spatial-Temporal Network(DMVST-Net) </p><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><ul><li>空间组件使用本地CNN捕获附近区域之间的空间依赖性。 本地CNN包含几个卷积层。 最后使用一个完全连接的层来获得低尺寸的表示。</li><li>时态视图采用LSTM模型，该模型从空间视图中获取表示，并在相应的时间将它们与上下文特征连接起来。</li><li>语义视图首先构建区域的加权图（权重代表功能相似性）。 节点被编码为向量。 最后使用完全连接的层进行联合训练。 最后，将完全连接的神经网络用于预测。</li></ul><h3 id="Spatial-View-Local-CNN"><a href="#Spatial-View-Local-CNN" class="headerlink" title="Spatial View : Local CNN"></a>Spatial View : Local CNN</h3><ul><li><p><strong>存在的问题</strong>：将相关性较弱的区域包括在内以预测目标区域实际上会损害性能。 </p></li><li><p><strong>解决方案</strong>：提出了一种local CNN方法，该方法仅考虑空间上附近的区域。 </p><ul><li>受到地理第一法则的启发（Tobler 1970）的：“近处的事物比远处的事物更相关”</li></ul></li><li><p>在每个时间间隔$t$，将一个位置$i$及其周围的邻域视为具有一个需求值通道的一个$S×S$图像</p><ul><li>$i$位于图像的中心</li><li>大小$S$控制着空间的粒度</li><li>在城市边界使用<strong>零填充</strong>。</li></ul></li><li><p>对于每个位置$i$和时间间隔$t$，我们都有一个图像作为张量（具有一个通道）$Y_t^i \in \mathbb R^{S×S×1}$</p></li><li><p>local CNN将$Y_t^i$作为输入$Y_t^{i,0}$，并将其馈入K个卷积层。<br>每层$k$的变换定义如下：<br>$$<br>Y_t^{i,k} = f(Y_t^{i,k-1} * W^k_t + b^k_t)<br>$$</p></li><li><p>$*$表示卷积操作</p></li><li><p>$f(·) $一个激活函数</p></li><li><p>使用整流器函数作为激活，即$f(z)= max(0,z)$</p></li><li><p>$W^k_t$和$b^k_t$是第$k$个卷积层中的两个参数集</p></li><li><p>参数$W^{1,…,K}_t$ 和 $b^{1,…,K}_t$在所有区域$i \in L$上共享，以使计算易于处理。</p></li><li><p>在K个卷积层之后，我们使用<strong>flatten层</strong>将区域i和时间间隔t的输出$Y_t^{i,K} \in \mathbb{R}^{S \times S \times \lambda}$转换为特征向量$s_t^i \in \mathbb R^{S^2\lambda}$</p></li><li><p>最后，我们使用一个<strong>完全连接的层</strong>来减小空间表示的尺寸$s_t^i$，其定义为：<br>$$<br>\hat{s}_t^i = f(W^{fc}_t s_t^i + b^{fc}_t)<br>$$</p></li><li><p>$W^{fc}_t$ 和 $b^{fc}_t$是在时间间隔t的两个学习参数</p></li><li><p>最后，对于每个时间间隔t，我们得到$\hat{s}_t^i \in \mathbb R^d $作为区域$i$的表示。</p></li></ul><h3 id="Temporal-View-LSTM"><a href="#Temporal-View-LSTM" class="headerlink" title="Temporal View : LSTM"></a>Temporal View : LSTM</h3><ul><li><p>时间视图对需求时间序列中的顺序关系进行建模。本文建议使用长短期记忆（LSTM）网络作为时间视图组件。 </p></li><li><p>LSTM（1997）是一种神经网络结构，它通过递归将过渡函数应用于输入的隐藏状态向量，提供了一种很好的方法来建模顺序依赖关系。 </p></li><li><p>有人提出解决经典递归神经网络（RNN）在长序列训练中梯度爆炸或消失的问题（Hochreiter et al。2001）。</p></li><li><p>LSTM通过在时间间隔$t$中维护一个存储单元$c_t$来稳定地学习顺序相关性，这可以看作是先前顺序信息的累积。 </p></li><li><p>在每个时间间隔中，LSTM在此工作中取一个输入$g^i_t$，$h_{t-1}$和$c_{t-1}$，然后在激活输入门$i^i_t$时，所有信息都累积到存储单元中。 </p></li><li><p>LSTM具有遗忘门功能。如果忘记门被激活，则网络可以忘记先前的存储单元$c_{t-1}^i$。 同样，输出门$o_t^i$输出控制存储单元的输出。</p></li></ul><p>本文LSTM结构的公式如下：<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><ul><li>$◦$表示Hadamard积(哈达玛积)</li><li>$tanh$是双曲正切函数。 这两个功能都是基于元素的。</li><li>$W_a,U_a,b_a（a \in {i,f,o,g}）$都是可学习的参数。</li><li>LSTM中时间间隔的数量为$h$</li><li>$t$时间间隔之后LSTM的区域$i$的输出为$h^i_t$</li></ul><p>时间分量从空间视图中获取表示并将它们与上下文特征连接起来。<br>$$<br>g_t^i = \hat{s}_t^i \oplus e_t^i<br>$$</p><ul><li>$\oplus$表示串联运算符</li><li>$g_t^i \in \mathbb R_{r+d}$</li></ul><h3 id="Semantic-View-Structural-Embedding"><a href="#Semantic-View-Structural-Embedding" class="headerlink" title="Semantic View : Structural Embedding"></a>Semantic View : Structural Embedding</h3><ul><li>直觉上，共享相似功能的位置可能具有相似的需求模式，</li><li>类似区域不一定在空间上紧密。</li><li>本文构造了一个表示区域之间功能（语义）相似性的位置图。</li></ul><p>将位置的语义图定义为$G =(V，E，D)$</p><ul><li>位置$L$的集合是节点$V = L$</li><li>$E \in V×V$是边集</li><li>$D$是所有边的相似性集合。 </li></ul><p>使用动态时间规整（DTW）来测量节点（位置）$i$和节点（位置）$j$之间的相似度$w_{ij}$。<br>$$<br>w_{ij} = exp(- \alpha DTW(i,j))<br>$$</p><ul><li>$\alpha$是控制距离衰减率的参数（在本文中，$\alpha = 1$）</li><li>$DTW(i,j)$是两个位置的需求模式之间的动态时间规整距离</li></ul><p>使用每周平均需求时间序列作为需求模式。根据实验中的训练数据计算平均值。<br>因为可以访问每两个区域则该图已完全连接</p><p>为了将每个节点编码为低维向量并保持结构信息，我们在图上应用了<strong>图嵌入</strong>方法。 [嵌入是压缩的表示]   </p><ul><li><strong>图嵌入</strong>（Graph Embedding，也叫Network Embedding）是一种将图数据（通常为高维稠密的矩阵）映射为低微稠密向量的过程，能够很好地解决图数据难以高效输入机器学习算法的问题。</li><li>对于每个节点$i$（位置），嵌入方法输出嵌入的特征向量$m^i$。  </li><li>此外，为了将嵌入式$m^i$与我们的整个网络体系结构共同训练，我们将特征向量$m^i$馈送到完全连接的层</li></ul><p><strong>FC层</strong>定义为：<br>$$<br>\hat{m}^i = f(W_{fe} m^i + b_{fe})<br>$$</p><ul><li>$W_{fe}$ 和$b_{fe}$ 是学习参数</li><li>使用<strong>LINE</strong>用于生成嵌入</li></ul><p>注：大规模信息网络(large-scale information Network)无论在存取性，使用性上比起普通的信息处理方式更加复杂，更加多变，例如航空公司网络，出版物网络，社会和通信网络以及万维网。LINE模型致力于将这种大型的信息网络嵌入到低维的向量空间中，且该模型适用于任何类型(有向、无向亦或是有权重)的信息网络。</p><h3 id="Prediction-Component"><a href="#Prediction-Component" class="headerlink" title="Prediction Component"></a>Prediction Component</h3><p>本文的目标是在给定数据直到$t$的情况下预测$t + 1$处的需求。<br>本文通过将$\hat{m}^i$与LSTM的输出$h_t^i$连接起来将三个视图结合在一起：<br>$$<br>q_t^i = h_t^i \oplus \hat{m}^i<br>$$<br>LSTM $h_t^i$的输出同时包含时间和空间视图的影响。<br>然后，本文将$q_t^i$馈入全连接网络，以获得每个区域的最终预测值$\hat{y}<em>{t+1}^i$<br>定义最终的预测函数为：<br>$$<br>\hat{y}</em>{t+1}^i = \sigma(W_{ff} q_t^i + b_{ff})<br>$$</p><ul><li>$W_{ff}$和$b_{ff}$是学习参数</li><li>$\sigma(x)$ 是S形函数并定义为$\sigma(x) = \frac{1}{1+e^{-x}}$</li><li>模型输出范围在[0,1]（需求之已标准化）</li><li>将该预测反规范化以获得实际需求值。</li></ul><h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><p>本部分提供有关用于联合训练我们提出的模型的损失函数的详细信息。 </p><p>我们使用的损失函数定义为：<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><ul><li>θ是DMVST-Net中所有可学习的参数，</li><li>γ是超参数。</li></ul><p>损失函数由两部分组成：</p><ul><li><p>均方损失</p></li><li><p>均值绝对百分比损失的平方</p></li><li><p>实际上，均方误差与大值的预测更相关。 为了避免训练以大样本为主导，我们还使平均绝对百分比损失最小化。 </p></li><li><p>在实验中，所有比较的回归方法都使用方程中定义的相同损失函数进行公平比较。</p></li></ul><p>算法1中概述了训练管道。<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><ul><li>使用Adam（Kingma and Ba 2014）进行优化。 </li><li>使用Tensor ﬂ ow和Keras（Chollet等，2015）来实现我们提出的模型。</li></ul><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><strong>数据集描述</strong>：</p><ul><li>使用didi出行数据 </li><li>从02/01/2017 到 03/26/2017 ，广州</li></ul><p><strong>评价指标</strong>：</p><ul><li>MAPE</li><li>RMSE</li></ul><p><strong>基线</strong>：</p><ul><li>HA</li><li>ARIMA</li><li>LR</li><li>MLP</li><li>XGBoost</li><li>ST-ResNet</li></ul><p>不同视图组件的效果:</p><ul><li>Temporal view</li><li>Temporal view + Semantic view</li><li>Temporal view+Spatial (Neighbors) view</li><li>Temporal view + Spatial (LCNN) view </li><li>DMVST-Net</li></ul><p>两种比较方式：</p><ul><li>与基线（最新方法）比较</li><li>与自己提出的方法的变体进行比较</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>将空间，时间和语义视图进行了整合，并以本地CNN为模型 ，LSTM和语义图嵌入。 </li><li>在大型出租车需求数据集上评估了我们的模型。 实验结果表明，我们提出的方法明显优于几种竞争方法。 </li><li>由于在本文中对语义信息进行了隐式建模，我们计划在未来的工作中纳入更多的显式信息（例如POI信息）。</li></ul>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> taxi demand </tag>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记-出租车需求预测3</title>
      <link href="/posts/36232/"/>
      <url>/posts/36232/</url>
      
        <content type="html"><![CDATA[<h1 id="Origin-Destination-Matrix-Prediction-via-Graph-Convolution-a-New-Perspective-of-Passenger-Demand-Modeling"><a href="#Origin-Destination-Matrix-Prediction-via-Graph-Convolution-a-New-Perspective-of-Passenger-Demand-Modeling" class="headerlink" title="Origin-Destination Matrix Prediction via Graph Convolution a New Perspective of Passenger Demand Modeling"></a>Origin-Destination Matrix Prediction via Graph Convolution a New Perspective of Passenger Demand Modeling</h1><p>论文题目：基于图卷积的起点-终点矩阵预测：旅客需求建模的新视角</p><p>参考笔记：<a href="https://cloud.tencent.com/developer/article/1619218" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1619218</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>为了获得乘客的出行模式，打车平台需要提前预测一个地区到另一个地区的乘客需求数量，即OD矩阵预测(ODMP)问题。OD矩阵预测比普通需求预测更具挑战性。</li><li>除了要预测一个地区的需求产生量，还需要预测需求的目的地。此外，数据稀疏性是一个严重的问题。</li><li>因此本文提出了一种基于网格嵌入的单馈多任务学习模型(GEML)。该模型主要包含两个部分，分别提取时间信息和空间信息。</li><li>网格嵌入部分是为了对乘客的空间移动模式和不同区域的相邻关系进行建模，其预加权聚合器的目的是感知数据的稀疏性和范围；</li><li>多任务学习部分则侧重于时间属性建模和捕获ODMP问题目标。两个数据集UCAR和Didi的结果表明GEML方法优于基准模型。</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>除了预测某一区域内可能的乘客需求数量外，了解每次出行的来源地和目的地的乘客需求也很重要。因为不同时段两个区域之间的需求量不仅承载着乘客需求的强度，而且有利于挖掘有用的出行模式。</p><p>本文从一个新的角度研究了乘客需求模型，即OD矩阵预测（ODMP）</p><p><strong>O</strong>rigin-<strong>D</strong>estination <strong>M</strong>atrix <strong>P</strong>rediction（<strong>ODMP</strong>）</p><p>OD矩阵包含两个方面的信息: </p><ol><li>不同的OD组合</li><li>每个OD对的旅客需求数量。</li></ol><p>ODMP的<strong>目标</strong>：预测在给定时间段内从一个地理区域到另一个地理区域的叫车订单数量。</p><p>为了同时兼顾出行产生量和目的地，时空特性以及数据稀疏性，本文提出了一种基于网格嵌入的单馈多任务学习模型(GEML)，以基于图对出行模式进行建模。具体来说，我们用图表示与地理区域相关的乘客订单记录，其中节点表示地理区域(以网格形式定义)，节点之间的边表示乘客需求，边权重表示订单数量。利用改进后的网格，可以构造出给定时间间隔内的OD矩阵。如图1所示，将区域划分为16个网格，订单记录汇总在相应的OD矩阵中。<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>本文模型的灵感来自于最近大火的GCNs，然而如果我们直接将已有的GCNs应用到OD矩阵所生成的图上，由于数据稀疏，学习到的具有很少订单的网格嵌入往往是不可靠和无效的，此外，如果没有任何历史订单记录的孤立节点(例如，新建社区)，学习到的网格嵌入也是不可行的(无论作为O点还是D点)。为了缓解数据的稀疏性问题，我们提出基于地理学第一定律探索网格的地理相关性，即所有的东西都是相关的，但附近的东西比遥远的东西更相关。例如，在两个地理位置相近的网格中，乘客需求的数量往往接近彼此。特别地，我们考虑了网格嵌入部分的两种邻域，即地理邻域（地理上相邻的）和语义领域（通过OD流连接起来的）。前者用于度量一个网格与其邻域之间的内在紧密程度，后者用于对网络OD之间的交通流强度建模。</p><p>基于网格嵌入学习得到的网格的表示，结合乘客需求的重要时间信息，设计了一个面向ODMP的多任务神经网络。受既有工作的启发，我们对一个网格的流入流和流出流分别建模，预测每个网格在不同时间段的流入和流出需求数量。引入这两个子任务的基本原理是，我们能够在每个网格上单独捕获更多的动态出行模式。通过补充两个单独的子任务，总体需求预测任务可以捕获更强的内在时间模式，因为每个网格中的总体需求具有更大的规模或粒度。例如，在早高峰时段，当网格划分的粒度很小时，网约车需求的目的地可能存在很大不同，导致数据稀疏性问题，这意味着乘客需求的目的地可能分布得非常广泛，但这些网格的总流入流和流出流是非常大的。</p><p>本文的主要贡献：</p><ul><li>提出了一种新颖的问题ODMP，即OD矩阵预测问题。预测给定时间段内给定起点和终点的乘客需求，这可以极大地帮助乘车平台准备汽车和调度订单。</li><li>通过将感兴趣的区域划分为地图上的网格来制定ODMP问题。然后设计网格嵌入网络，以通过新颖定义的网格邻域（地理和语义邻居）之间的图卷积为每个网格执行嵌入，该模型通过模仿GCN中的消息传递模式来建模不同网格之间的流量传输关系。</li><li>设计了一个多任务学习网络，该网络依靠长期短期记忆循环网络（LSTM）来捕获旅客需求的时间趋势。两个子任务预测网格中的单个流入流和流出流需求，而主任务预测每对网格之间的需求。</li><li>在两个真实大规模叫车数据集上的大量实验表明提出的GEML模型性能优于基准模型。</li></ul><h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ol><li><p>Grid（网格）：整个感兴趣的空间区域（例如特定城市）被划分为n个不重叠的网格，表示为$G = {g_1，g_2，…，g_n}$。一个网格示例其中区域分为16个网格，每个网格的范围由最大和最小坐标定义。</p></li><li><p>Time Slot:将时间划分为t个时隙序列,表示为${Slot_1,Slot_2,…,Slot_t}$。任何两个连续时隙之间的间隔是恒定的。</p></li><li><p>OD Matrix:定义OD矩阵为$M \in \mathbb{N}^{G \times G}$，每个输入$m_{i,j}\in M$表示从网格 $g_i$到网格 $g_j$的乘客需求数量。</p><ul><li>在每个时隙中，对于任何两个网格$g_i, g_j \in G $，从$g_i$到$g_j$的旅行需求总数表示为$m_{i,j}$。</li></ul></li></ol><h3 id="OD矩阵预测"><a href="#OD矩阵预测" class="headerlink" title="OD矩阵预测"></a>OD矩阵预测</h3><p>问题定义：</p><ul><li>对于t个时隙，给定t个观察到的OD矩阵${M1，M2，…，Mt}$的序列和一组辅助特征$X_aux$（根据可用性可选），起源-目标矩阵预测（ODMP）是一个回归任务以预测下一个时隙$Slot_{t + 1}$中的OD矩阵$M_{t + 1}$。</li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>步骤：</p><ul><li>OD矩阵可以从乘车服务提供商的出行记录中提取,(如果可获得)外部数据源将用于产生辅助功能</li><li>GEML的网格嵌入部分通过两种邻域的信息聚合来学习每个网格的嵌入向量：地理邻域和语义邻域</li><li>每个网格的顺序矢量表示将被馈入多任务神经网络，以学习最近时隙$t_a$中网格的表示。</li><li>利用网格的矢量表示来生成预测的OD矩阵。</li></ul><p>GEML模型能同时捕获空间和时间特性。</p><ul><li>从<strong>空间角度</strong>出发，提出了一种基于邻域的网格嵌入方法，通过聚集邻域信息来学习每个网格的向量表示。</li><li>从<strong>时间的角度</strong>，我们设计了一个多任务学习框架来模拟乘客需求随时间的动态趋势。接下来，我们将介绍网格嵌入和多任务学习的技术细节。</li></ul><h3 id="Grid-Embedding"><a href="#Grid-Embedding" class="headerlink" title="Grid Embedding"></a>Grid Embedding</h3><ul><li>由于GCN在低需求网格上的局限性，我们在ODMP的背景下提出了两种邻域函数用于乘客需求建模，即地理邻域和语义邻域。</li><li>它们被用来测量一个网格与其相邻网格之间的内在紧密度，并分别捕获网格网络中始发地与目的地之间的业务流的语义强度。</li><li>前者用于度量一个网格与其邻域之间的内在紧密程度，后者用于对网络OD之间的交通流强度建模</li></ul><h4 id="Geographical-Neighborhood"><a href="#Geographical-Neighborhood" class="headerlink" title="Geographical Neighborhood"></a>Geographical Neighborhood</h4><ul><li>两个区域中心点之间的距离小于一定的阈值可定义为地理邻域。</li></ul><p>$$<br>\Phi_i = {g_i|dis(g_i,g_j) \leq L }<br>$$</p><ul><li>$dis(g_i,g_j)$表示两个网格中心的空间距离</li></ul><h4 id="Semantic-Neighborhood"><a href="#Semantic-Neighborhood" class="headerlink" title="Semantic Neighborhood"></a>Semantic Neighborhood</h4><ul><li>如果两个区域之间至少有一个OD流 (可以是相反方向), 即可定义为两者为语义邻域。</li><li>在任意时间段$t′= 1, 2,…，t$我们可以通过公式获取网格i的语义邻域。</li></ul><p>$$<br>\Omega_{t′}^i = {g_i|m_{i,j} &gt; 0 || m_{j,i} &gt; 0, m_{i,j} \in M_t′,m_{j,i} \in M_t′ }<br>$$</p><ul><li>由公式可知，不同网格在不同时间间隔的语义邻居个数是不确定的。由于ODMP问题对时间敏感，因此考虑不同网格在不同时间间隔的语义关系至关重要。</li></ul><h4 id="Pre-Weighted-Aggregator-for-Grid-Embedding"><a href="#Pre-Weighted-Aggregator-for-Grid-Embedding" class="headerlink" title="Pre-Weighted Aggregator for Grid Embedding"></a>Pre-Weighted Aggregator for Grid Embedding</h4><ul><li>通过聚合地理邻域$\Phi_i$和语义邻域$\Omega_{t′}^i$推断在时隙$t_k$时的每个网格$g_i$的向量表示</li><li>不是为每个网格训练一个不同的嵌入向量，而是训练一个聚合器函数，它学会从网格的邻域中积累和选择特征信息</li></ul><p>朴素聚合器形式：</p><p>$$<br>\mathbf v_i = \sigma \Big (\mathbf W \cdot MEAN({ \mathbf v_i’}  { \mathbf v_j’,g_j \in N_i)}) \Big )<br>$$</p><ul><li><p>vi是网格gi的嵌入向量</p></li><li><p>MEAN(·)表示对应元素均值</p></li><li><p>vj是网格gj的嵌入向量</p></li><li><p>朴素聚合方法：计算vi’和vj’对应元素的均值，并将它们连接到之前的特性vi’</p></li><li><p>尽管有一些基本聚合器的变体(例如pooling aggregator and LSTM aggregator)，现有的图卷积聚合方法在ODMP场景下缺乏充分捕捉不同网格之间关系的能力，无法进行需求建模，原因是这些聚合器在融合每个网格邻居的所有特性时无法区分它们的重要性，直观地说，两个网格之间的地理距离越近，它们的属性就越相似。此外，在语义邻居集中，邻居网格的受欢迎程度应该对聚合过程产生影响，因为它保留了具有代表性的出行模式。</p></li></ul><p>在此基础上，提出了一种<strong>预加权聚合器</strong>，该聚合器可以选择性地将重点放在网格嵌入的重要邻域上。对于网格的地理邻域，我们利用相邻区域之间的距离作为聚合器的权重因子。因此，我们将<strong>地理邻域的预加权聚合器</strong>表示为：</p><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><p>对于<strong>语义邻域</strong>，degree代表OD流量，即两个区域之间的OD流（从i到j或从j到i），$\epsilon$是一个非常小的值接近于零,以防$degree(g_j) = 0$。</p><p>注意，两种表示都是随时间变化的，是一个动态指标。最后，将两种语义表示连接起来得到一个网格最终的语义表示。</p><h3 id="Multi-TaskLearning"><a href="#Multi-TaskLearning" class="headerlink" title="Multi-TaskLearning"></a>Multi-TaskLearning</h3><ul><li>一种具有periodic-skip LSTM的多任务学习方案</li></ul><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><h4 id="Periodic-Skip-LSTM"><a href="#Periodic-Skip-LSTM" class="headerlink" title="Periodic-Skip LSTM"></a>Periodic-Skip LSTM</h4><p>ht = LSTM(xt,ht−1). </p><p>在预测下一小时的乘客需求时，LSTM中的序列建模方案将迫使模型从之前的连续小时中收集信息。这可能对需求预测没有多大帮助，因为不相关的输入会产生很多噪音。为此，为了更好地对周期性进行建模，我们取网格嵌入序列{vi1, vi2，…， vit}作为输入，进一步将Eq.(8)转换为Periodic-Skip LSTM，跳过不相关的顺序模式。</p><p>hi t = LSTMps(vi t,hi t−p) </p><ul><li>其中p是跳过的隐藏状态数</li></ul><h4 id="Main-Task-Predicting-the-OD-Matrix"><a href="#Main-Task-Predicting-the-OD-Matrix" class="headerlink" title="Main Task: Predicting the OD Matrix"></a>Main Task: Predicting the OD Matrix</h4><p>由periodic-skip LSTM框架,我们可以得到在时间t网格gi的向量表示。为了得到OD矩阵中每一项mij的值，我们构造了一个过渡矩阵Wm∈R（d×d）以对OD流进行建模。此时，预测值m可由下式计算。损失函数为均方误差。</p><p>mi,j = (Wmhi t)⊤hj t, (10) andweusemeansquarederrortocomputethelossfunctionfor themaintask:<br>LODMP =<br>1 |Mt+1|×N<br>N Õ n=1||Mt+1− ˆ Mt+1||, (11) wheremi,j ∈ Mt+1 is the real value in the OD matrix at time t +1,n ≤ N istheindexofthetrainingsample. </p><h4 id="Two-Subtasks-Predicting-the-In-and-Out-Degrees"><a href="#Two-Subtasks-Predicting-the-In-and-Out-Degrees" class="headerlink" title="Two Subtasks:Predicting the In-and Out-Degrees"></a>Two Subtasks:Predicting the In-and Out-Degrees</h4><p>在预测上述总体OD矩阵的主要任务的同时，我们还分别对流入流（p）和流出流（q）进行了建模。</p><ul><li>Win和Wout是用于将网格嵌入投影到标量的两个投影权重</li><li>损失函数为均方误差，G是网格的个数。</li></ul><h4 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h4><p>针对上述三个任务，我们将主要任务的损失和两个子任务的损失结合起来，制定出总体损失函数。</p><h2 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h2><p>四个问题：</p><ol><li>GEML在ODMP任务上的效果如何？</li><li>模型的每个拟议组成部分如何对GEML的性能做出贡献？</li><li>每个主要超参数如何影响GEML的预测性能？</li><li>GEML学习了哪些实际的移动性模式？</li></ol><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><ul><li>UCAR:北京，2016 8.1-8.31</li><li>Didi:成都 2016 11.1-11.30 </li></ul><h3 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h3><ul><li>HA</li><li>LSTM</li><li>LSTNet</li><li>GCRN</li></ul><p>变体：</p><ul><li>GEML-S1</li><li>GEML-S2</li><li>GEML-S3</li><li>GEML-S4</li><li>GEML-AF</li></ul><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><p>指标</p><ul><li>RMSE</li><li>SMAPE</li></ul><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li>定义了乘客需求的ODMP问题</li><li>不仅需要预测区域中的需求数量，还需要预测需求的目的地。</li><li>通过区域邻居的信息对区域的移动性模型进行建模，在此基础上设计了网格嵌入框架。</li><li>在其聚合过程中添加了预加权函数，以便它可以感知数据的范围和稀疏性。</li><li>利用具有非周期性跳过成分的多任务LSTM对时间趋势进行建模。</li><li>子任务可以通过扩大每个网格内总体需求的规模或粒度来协助主要任务捕获更强大的固有时间模式。</li><li>整个过程是一个端到端模型，称为基于网格嵌入的多任务学习（GEML）。</li><li>在两个现实世界和大规模的乘车数据集上评估了模型</li></ol><p><strong>缺点</strong>：有一个致命弱点，即没有考虑行程时间，由O点到D点需要一定的行程时间，因此实时的OD矩阵是不可能得到的，因此利用真实的历史OD矩阵作为输入在实时预测中是不可行的。</p>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> taxi demand </tag>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文笔记-出租车需求预测2</title>
      <link href="/posts/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
      <url>/posts/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Predicting-Taxi-Demand-at-High-Spatial-Resolution-Approaching-the-Limit-of-Predictability"><a href="#Predicting-Taxi-Demand-at-High-Spatial-Resolution-Approaching-the-Limit-of-Predictability" class="headerlink" title="Predicting Taxi Demand at High Spatial Resolution: Approaching the Limit of Predictability"></a>Predicting Taxi Demand at High Spatial Resolution: Approaching the Limit of Predictability</h1><p>论文题目：以高空间分辨率预测出租车需求：接近可预测性的极限</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>采用整体方法来预测高空间分辨率下的出租车需求。</li><li>使用两个真实的数据集（黄色出租车和纽约市的Uber行程）展示了我们的技术，并对曼哈顿的9,940个构建块进行了评估。</li></ul><p>我们的方法包括两个关键步骤</p><ul><li>首先，我们使用<strong>熵</strong>和<strong>人员流动的时间相关性</strong>来衡量构建模块级别的需求不确定性。</li><li>其次，为了确定哪种预测算法可以达到理论上的最大可预测性，我们实现并比较了三个预测器：<ul><li><strong>Markov预测器</strong>（基于可概率性的预测算法）</li><li><strong>Lempel-Ziv-Welch预测器</strong>（基于序列的预测算法）</li><li><strong>神经网络预测器*</strong>（使用机器学习的预测算法）</li></ul></li></ul><p>结果表明：</p><ul><li>可预测性因构建基块而异，平均而言，理论上最大可预测性可高达83％。</li><li>预测器的性能也有所不同：神经网络预测器为可预测性较低的块提供更高的准确性，而马尔可夫预测器为可预测性较高的块提供更高的准确性。</li><li>在具有最高最大可预测性的区块中，马尔可夫预测器能够以89％的精度预测出租车需求，比神经网络预测器好11％，而仅需要0.03％的计算时间。 </li><li>这些发现表明，<strong>最大可预测性</strong>可以作为选择预测算法的良好指标。</li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>出租车服务是存在不平衡的问题的。</p><p>本文<strong>出租车需求预测问题</strong>的<strong>定义</strong>：</p><ul><li>给定区域i中的历史出租车需求数据，希望预测下一个时间间隔内区域i内出现的出租车数量。</li></ul><p><strong>目标</strong>：</p><ul><li>预测满足的出租车需求（met taxi demands）</li></ul><ul><li>使用接客数量来表示某个地区的出租车需求，并将其视为顺序数据</li><li>可以从满足的出租车需求中推断出未满足的需求</li></ul><p>许多预测出租车需求的方法，包括不确定性分析，概率模型，时间序列，SVM和神经网络。</p><p>两个关键问题：</p><ol><li>给定预测算法$\alpha$，同时考虑出租车需求序列的随机性和时间相关性，预测算法$\alpha$可以达到的潜在精度的上限是多少？</li><li>给定潜在精度的上限，考虑到计算时间和精度之间的权衡，哪个预测器具有更好的性能？</li></ol><p>本文的<strong>解决方案</strong>：</p><ul><li><p>本文通过分析某个地区出租车需求的<strong>最大可预测性</strong>（$\Pi^{max}$）来选择最佳预测器。</p></li><li><p>最大的可预测性由出租车需求序列的熵定义，同时考虑了<strong>随机性</strong>和<strong>时间相关性</strong>。</p></li><li><p>最大可预测性$\Pi^{max}$由Song等人首先提出，用于分析用户移动性[10]。</p></li><li><p>将最大可预测性$\Pi^{max}$定义为预测算法可以预测出租车需求的<strong>最高潜在精度</strong>。</p></li><li><p>最大的可预测性通过测量人员流动的规律性来捕获出租车需求序列的时间相关程度。</p></li><li><p>对于大多数地区，出租车需求受一定程度的随机性（例如，突发事件）和一定程度的规律性（例如，每周模式）支配，可用于预测。</p></li><li><p>例如，一个$\Pi^{max} = 0.3$的构建块表示在至少70％的时间中，此块的出租车需求似乎是随机的，并且只有30％的时间我们才能希望预测出租车的需求。</p></li><li><p>无论预测算法多么出色，我们都无法以$\Pi^{max} = 0.3$的精度预测高于30％的构建块的未来出租车需求。 </p></li><li><p>$\Pi_{max}$表示构建块中<strong>出租车需求可预测性的基本限制</strong>。</p></li><li><p>出租车需求的时间相关性并不总是成立。不同的区域具有不同的功能，因此具有不同的可预测性。</p></li><li><p>假设要选择最佳预测器，必须分析每个地区出租车需求的最大可预测性（$\Pi^{max}$）</p></li></ul><p>本文的<strong>贡献</strong>：</p><ol><li><p>本文测量纽约市每个组成部分的出租车需求的理论最大可预测性。这代表了预测算法$\alpha$可以达到的潜在精度的上限。结果显示，出租车需求的最大可预测性平均可以达到83％。最大可预测性捕获了出租车需求序列的时间相关程度。所以根据结果可知，<strong>纽约市的出租车需求具有很强的时间格局</strong>。</p></li><li><p>本文实现并比较了<strong>三个预测变量</strong>的预测准确性：</p><ul><li>Markov预测变量（基于概率的方法）</li><li>Lempel-Ziv-Welch（LZW）预测变量（基于序列的方法）</li><li>神经网络（NN）预测变量（一种基于机器学习的方法）  </li></ul><p>结果表明：</p><ul><li>NN预测器为具有<strong>低可预测性</strong>的构建块提供了更高的准确性</li><li>而Markov预测器为具有<strong>高可预测性</strong>的构建块提供了更高的准确性</li><li>最大可预测性是实现实际预测准确性的一个可行目标，并且具有多个功能的计算密集型NN预测器并不总是比简单的Markov预测器好。</li></ul></li><li><p>在考虑准确性和计算成本的同时，了解每个构件的可预测性可以帮助确定使用哪个预测器。</p></li></ol><h2 id="出租车需求的可预测性"><a href="#出租车需求的可预测性" class="headerlink" title="出租车需求的可预测性"></a>出租车需求的可预测性</h2><p>目标：以提议的最大可预测性回答之前的问题</p><p>问题1：考虑到出租车需求的随机性和时间相关性，在构建块i中，给定预测算法$\alpha$和从时间1,2，… n开始的出租车需求$D_n^{(i)}$的序列，我们想要找到预测算法$\alpha$可以达到的最大可预测性（最高潜在准确度）$\Pi^{max}$。</p><h3 id="出租车需求"><a href="#出租车需求" class="headerlink" title="出租车需求"></a>出租车需求</h3><p>使用出租车乘客数$d_t^{(i)}$来代表在构建块i中t时候的出租车的需求<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"><br>由于出租车需求可能会有很大的变化，因此很难预测$d_t^{(i)}$的确切值。<br>为简单起见，我们使用因子$q$对出租车需求的值取整。<br>将每q个出租车需求分组为一个出租车需求。<br>尝试预测每个q级别的出租车需求，<br>将q = 10设置为使预测更加宽松，同时保持较低的误差。 至于q的影响力见下方论文。</p><ul><li>K. Zhao, D. Khryashchev, J. Freire, C. Silva, and H. Vo, “Predicting taxi demand at high spatial resolution: Approaching the limit of predictability,”<a href="https://serv.cusp.nyu.edu/projects/demandprediction/" target="_blank" rel="noopener">https://serv.cusp.nyu.edu/projects/demandprediction/</a> 2016taxidemand.pdf, Tech. Rep., July 2016. </li></ul><h3 id="熵（Entropy）"><a href="#熵（Entropy）" class="headerlink" title="熵（Entropy）"></a>熵（Entropy）</h3><ul><li>熵是表征可预测性程度的有效方法。 通常低熵意味着较高的可预测性。</li><li>本文使用三种熵测度：<ul><li>随机熵$S_{random}^{(i)}$</li><li>香农熵$S_{Shannon}^{(i)}$</li><li>实熵$S_{real}^{(i)}$</li></ul></li></ul><hr><ol><li><strong>Random Entropy</strong></li></ol><p>$<br>S_{random}^{(i)} = \log_2 N^{(i)}<br>$</p><ul><li>$N^{(i)}$是$D_n^{(i)}$中唯一出租车需求的数量</li><li>较低的熵意味着较高的可预测性。如果构建块i中的出租车需求较低，则随机熵$S_{random}^{(i)}$将很小，并且很容易预测出租车需求。</li></ul><hr><ol start="2"><li><strong>Shannon Entropy</strong></li></ol><p>$<br>S_{Shannon}^{(i)} = - \sum_{t=1}^{N^{(i)}} p(d_t^{(i)})\log_2 p(d_t^{(i)})<br>$</p><ul><li>$p(d_t^{(i)})$是在第i个构建块上有$d_t^{(i)}$出租车需求的概率–它表征了出租车需求的不确定性。</li></ul><p>注：<strong>随机熵和香农熵都与时间无关，并且不考虑时间模式。</strong></p><hr><ol start="3"><li><strong>Real Entropy</strong></li></ol><p>$<br>S_{real}^{(i)} = - \sum_{S_n^{(i)} \subset D_n^{(i)}} P(S_n^{(i)})\log_2 [P(S_n^{(i)})]<br>$</p><ul><li>$P(S_n^{(i)})$表示在出租车需求序列$D_n^{(i)}$中找到特定时间顺序的子序列$S_n^{(i)}$的概率。 </li></ul><p>注：与香农熵和随机熵不同，真实熵不仅考虑构建块中不同出租车需求的<strong>频率</strong>，而且考虑出租车需求的<strong>时间模式的顺序</strong>。</p><hr><ul><li>查找给定集合的所有子集的问题具有指数复杂度（$O（2_n）$）</li><li>使用LempelZiv估计器来计算实际熵。 Lempel-Ziv估计器可以快速收敛到实际熵[16]。 对于时间n之后的出租车需求序列，可以通过以下方式估算熵</li></ul><p>$<br>S_{real}^{(i)} = \Big ( \frac{1}{n} \sum_t s_t^{‘(i)}\Big )^{-1} \ln n<br>$</p><ul><li>$s_t^{‘(i)}$表示从时间t开始的最短子序列的长度，从1到t-1都没有出现。</li></ul><h3 id="最大可预测性（Maximum-Predictability-Pi-max-）"><a href="#最大可预测性（Maximum-Predictability-Pi-max-）" class="headerlink" title="最大可预测性（Maximum Predictability $\Pi^{max}$）"></a>最大可预测性（Maximum Predictability $\Pi^{max}$）</h3><ul><li>本文将可预测性$\Pi$<strong>定义</strong>为算法$\alpha$可以正确预测构建块中未来的出租车需求的<strong>成功率</strong>。</li><li>对于具有唯一出租车需求$N^{(i)}$的构件，可预测性度量取决于Fano不等式$\Pi<br>\leq \Pi^{max}$</li></ul><p>给定熵$S$和构建块$i$中不同的出租车需求$N^{(i)}$，可以通过以下公式计算最大可预测性$\Pi^{max}$：</p><p>$$<br>S = - \Pi^{max} \log_2(\Pi^{max}) - (1-\Pi^{max})\log_2(1-\Pi^{max})+(1-\Pi^{max})\log_2(N^{(i)} - 1)<br>$$</p><ul><li><p>最大可预测性$\Pi^{max}$为0到1之间的值。值越大，算法越准确。</p></li><li><p>通过不同的熵测度$S_{random}^{(i)}$，$S_{Shannon}^{(i)}$和$S_{real}^{(i)}$，我们具有不同的最大可预测性值$\Pi^{random}$，$\Pi^{Shannon}$和$\Pi^{real}$。</p></li><li><p>本文证明，$\Pi^{random}$≤$\Pi^{Shannon}$≤$\Pi^{real}$，因此$\Pi^{real}$是最大可预测性$\Pi^{max}$。</p></li></ul><hr><p>所以问题的回答是：给定一个预测算法α，$\Pi^{max}$是预测算法α可以达到的最大可预测性。</p><p>证明略</p><h3 id="可扩展性（Scalability-）"><a href="#可扩展性（Scalability-）" class="headerlink" title="可扩展性（Scalability ）"></a>可扩展性（Scalability ）</h3><p>$</p><ul><li><p>\Pi^{max} \log_2(\Pi^{max}) - (1-\Pi^{max})\log_2(1-\Pi^{max})+(1-\Pi^{max})\log_2(N^{(i)} - 1) - S = 0<br>$</p></li><li><p>$S$和$N^{(i)}$是已知的</p></li><li><p>定义函数$f(\Pi^{max}) = - \Pi^{max} \log_2(\Pi^{max}) - (1-\Pi^{max})\log_2(1-\Pi^{max})+(1-\Pi^{max})\log_2(N^{(i)} - 1) - S$</p></li><li><p>由于$\Pi^{max}$是一个介于0到1之间的值，所以当$f(\Pi^{max})= 0$时找到交点可以求解$\Pi^{max} = \gamma$的方程。</p></li><li><p>求解所有方程$f(\Pi^{max})= 0$的计算时间为$O(m)$，其中$m$是积木总数。所以可以通过在多个处理器上为每个构件分配$f(\Pi^{max})= 0$的计算来进行缩放。</p></li></ul><h2 id="预测者"><a href="#预测者" class="headerlink" title="预测者"></a>预测者</h2><p>这一部分作者提出了三种预测器：</p><ul><li>Markov预测器（基于概率的方法）</li><li>LZW预测器（基于序列的方法）</li><li>神经网络（NN）预测器（基于机器学习的方法）</li></ul><p>用这三种预测器来预测出租车需求并比较它们的性能</p><p>问题2：给定构造块i的最大可预测性$\Pi^{max}$，考虑到计算时间和精度之间的折衷，我们希望找到一种性能更好的预测器。</p><h3 id="Markov预测器"><a href="#Markov预测器" class="headerlink" title="Markov预测器"></a>Markov预测器</h3><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><h3 id="LZW预测器"><a href="#LZW预测器" class="headerlink" title="LZW预测器"></a>LZW预测器</h3><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><h3 id="神经网络预测器"><a href="#神经网络预测器" class="headerlink" title="神经网络预测器"></a>神经网络预测器</h3><p><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></p><h2 id="数据集和预处理"><a href="#数据集和预处理" class="headerlink" title="数据集和预处理"></a>数据集和预处理</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>出租车数据集：</p><ul><li>the NYC yellow taxi data set</li><li>the NYC Uber taxi data set</li></ul><p>Pluto(Primary Land Use Tax Lot Output)数据集：</p><ul><li>NYC Pluto data set</li></ul><p>数据预处理和可扩展性</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>出租车需求的最大可预测性平均可以达到83％的准确性。</p><h3 id="Limits-of-Predictability-可预测性的限制"><a href="#Limits-of-Predictability-可预测性的限制" class="headerlink" title="Limits of Predictability 可预测性的限制"></a>Limits of Predictability 可预测性的限制</h3><p>由于Πmax捕获了出租车需求的时间相关性，因此，如果在我们的预测算法中考虑时间相关性，我们可以达到更高的潜在预测准确性。 如果我们每天而不是按小时对出租车需求进行分组，也会发现类似的结果</p><h3 id="Predictability-of-Different-Functional-Buildings-不同功能性建筑物的可预测性"><a href="#Predictability-of-Different-Functional-Buildings-不同功能性建筑物的可预测性" class="headerlink" title="Predictability of Different Functional Buildings 不同功能性建筑物的可预测性"></a>Predictability of Different Functional Buildings 不同功能性建筑物的可预测性</h3><ul><li>不同的构造块表现出不同的最大可预测性。</li><li>住宅和工作场所都具有较高的可预测性，因此具有很强的时间格局。 对于其他地方，例如旅馆，交通中心或办公室物业，其可预测性不如住宅或工作场所高。 这些地区的出租车需求主要取决于白天发生的事件。<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></li></ul><h3 id="Uber-Versus-Yellow-Taxi"><a href="#Uber-Versus-Yellow-Taxi" class="headerlink" title="Uber Versus Yellow Taxi"></a>Uber Versus Yellow Taxi</h3><ul><li>研究了Uber和黄色出租车之间的可预测性差异。</li><li>由于Uber出租车的稀疏性，我们在邻域级别分析了这两个数据集的最大可预测性。 我们将每个社区的每小时出租车需求分组，并检查每小时出租车需求序列的最大可预测性。</li><li>Uber出租车服务的最大可预测性高于黄色出租车。 这可能是因为黄色的出租车通常采用随机巡航策略，而Uber的出租车在接到请求后便前往乘客所在的地方。 Uber出租车可以更好地捕获某个地区出租车需求的时间相关性。</li></ul><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><ul><li>使用三周的出租车接送数据作为训练数据，并以一周的出租车接送数据作为测试数据来比较这三个预测指标。</li><li>每小时对每个构建块的出租车需求进行分组</li><li>为了减少偏差，在上周的星期一，星期三和星期日的午夜（00:00），早晨（06:00），中午（12:00）和晚上（18:00）选择12个时间间隔作为 预测目标。</li><li>根据最大可预测性Πmax对构建基块进行排名，并将它们分为十个1,000个构建基块组。 我们选取每组的一组中位数构建基块进行调查（每组10个）</li><li>总共有60,480个训练样本和1,200个测试样本。</li></ul><p><strong>Error metrics</strong></p><ul><li>采用误差测量值对称平均绝对百分比误差（sMAPE）来评估预测算法。<br><img src= "/img/loading.gif" data-lazy-src="attachment:image.png" alt="image.png"></li></ul><h3 id="评估-1"><a href="#评估-1" class="headerlink" title="评估"></a>评估</h3><p>NN预测器为可预测性较低的构建块提供了更高的准确性。在最大可预测性Πmax= 72％最低的组中，NN预测器的预测精度为70％-在所有预测器中最高。这是因为NN预测器能够捕获多个特征，例如天气信息[7]，而其他两个预测器则无法捕获。马尔可夫预测器为具有高可预测性的构建块提供了更好的准确性，并迅速收敛到可预测性上限Πmax。这与以前的研究一致，即最大可预测性与马尔可夫预测器预测准确性之间存在高度正相关[12]。在具有最高最大可预测性的构造块中，马尔可夫预测器能够以89％的准确性预测出租车需求，比预测的要好11％。 NN预测器。除了。马尔可夫预测器的计算时间仅为0.5秒，仅为NN预测器的0.03％（见图6（b））。对于具有高可预测性的区域，马尔可夫预测器能够以更少的计算时间提供更好的预测精度。从实验中我们发现，最大可预测性Πmax可以帮助我们确定使用哪个预测器。在可预测性较低的区域中，我们可以使用NN预测器通过捕获多个特征来达到高精度，而在可预测性较高的区域中，马尔可夫预测器能够在保持较低计算时间的同时提供更好的预测精度。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><ol><li>Predicting Taxi Demand </li><li>Inferring Unmet Demand  </li></ol><p>the unmet taxi demand：需要出租车但找不到出租车的人数<br>从出租车数据集推断未满足的出租车需求</p><ol start="3"><li>Temporal Pattern of Human Mobility</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>分析了纽约市超过1400万的黄色和Uber出租车接送样品。</li><li>出租车需求具有很高的可预测性（平均高达83％），这表明人类流动性与时间之间具有很强的相关性。</li><li>研究哪种预测算法可以达到最大的可预测性。</li><li>证明了计算密集型NN预测器并不总是比Markov预测器具有更好的预测精度。在可预测性较低的区域，NN预测器可以通过捕获其他特征（天气等）来达到高精度。</li><li>在具有高可预测性的区域中，马尔可夫预测器能够在保持较低计算时间的同时达到较高的预测精度。</li><li>可能由于不同的巡航策略，可以在Uber出租车数据中更好地捕获出租车需求的时间相关性。</li><li>本文的结果不仅限于出租车需求问题。可以将相同的方法用于其他时空数据集中的预测</li></ul><p>未来的工作：</p><ul><li>将针对时空预测问题提出一种基于可预测性的新预测器，例如，马尔可夫模型的输出可被视为NN预测器的功能。</li></ul><p>日志总结：</p><ol><li>从中了解了“最大可预测性”，本文作者是指预测算法可以预测未来出租车需求的成功率，即预测算法可以达到的最大预测性。</li><li>作者在本文通过比较三个预测器的预测准确性，从中发现某些规律，从而表明在考虑准确性和计算成本的同时，可以根据每个构件的最大可预测性来帮助确定使用哪个预测器。</li><li>作者是在纽约市的数据集上进行的实验，并且可知纽约市的出租车需求具有很强的时间格局。</li><li>了解了the unmet taxi demand,比如需要出租车但找不到出租车的人数。研究角度是可以从出租车数据集推断未满足的出租车需求</li></ol>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> taxi demand </tag>
            
            <tag> paper </tag>
            
            <tag> summary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Simpler The Better:A Unified Approach to Predicting Original Taxi Demands based on Large-Scale Online Platforms</title>
      <link href="/posts/60400/"/>
      <url>/posts/60400/</url>
      
        <content type="html"><![CDATA[<p><strong>论文题目</strong>：越简单越好:基于大型在线平台的原始出租车需求预测的统一方法</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>翻译</strong>：<br>出租车呼叫应用程序因其为有需要的乘客调度闲置出租车的效率而越来越受欢迎。为了准确平衡出租车供需，网络出租车平台需要预测单位原始出租车需求(UOTD)，即单位时间(如每小时)和单位区域(如每个POI)提交的叫车需求数量。预测UOTD对于大型工业在线出租车平台来说不是小事，因为准确性和灵活性都是必不可少的。复杂的非线性模型，如GBRT和深度学习，通常是准确的，但需要劳动密集型的模型重新设计后，场景变化(例如，额外的约束，由于新的法规)。为了准确地预测UOTD，同时保持对场景变化的灵活性，我们提出LinUOTD，这是一个具有超过2亿个特征维度的统一线性回归模型。简单的模型结构消除了重复模型设计的需要，而高维特征有助于准确的UOTD预测。我们进一步设计了一系列优化技术，以提高模型的训练和更新效率。对来自一个工业在线出租车平台的两个大型数据集的评估证实LinUOTD在准确性上优于流行的非线性模型。我们设想在UOTD预测中采用具有高维特征的简单线性模型的经验，作为一项试点研究，可以提供一些见解</p><p><strong>总结</strong>：</p><ul><li><p>Unit Original Taxi Demand 简称 UOTD ，单位原始出租车需求</p></li><li><p>为了准确平衡出租车供需，网络出租车平台需要预测单位原始出租车需求(UOTD)，即单位时间(如每小时)和单位区域(如每个POI)提交的叫车需求数量。</p></li><li><p>为了准确地预测UOTD，同时保持对场景变化的灵活性，本文提出了LinUOTD，一个具有超过2亿个特征维度的统一线性回归模型。</p></li></ul><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><ul><li>出租车需求的代表性指标是单位原始出租车需求(UOTD)，即单位时间(如每小时)、单位区域(如每个POI)提交给网络出租车平台的叫车订单数量。</li><li>pick-ups  简称 PU 拾取器</li><li>UOTD不同于拾取器的数量(PU)。单位时间和单位区域的PU数是UOTD的子集，因为UOTD忽略了最终放弃打车的潜在乘客。相比之下，UOTD则反映了给定时间和空间内完整的原始乘客需求。</li></ul><p>UOTD信息对网络出租车平台有三重好处。</p><ol><li>拓展潜在市场。通过比较历史UOTD和对应的PU数，平台可以发现呼叫出租车动机强但最终打车次数少的时间和地区。</li><li>评估激励机制。UOTD反映了用户在采用新的折扣策略和动态定价后乘坐出租车出行的意愿。</li><li>指导出租车调度。在线出租车平台可以提前预测漫游出租车的流量，为乘客分配漫游出租车提供便利。</li></ol><ul><li>过去的出租车需求预测方面的重点是预测PU的数量。他们通常根据出租车轨迹与出租车轨迹之间的关系来预测出租车轨迹的数量。然而，出租车轨迹并不总是与uotd相关联。这样就不可能将PU预测扩展到UOTD预测，使得PU预测无法扩展到UOTD预测。</li></ul><p>两种范式:</p><ol><li>具有少量特征的复杂(非线性)模型：<ul><li>优点:(在大多数时空预测研究中是首选的)</li><li>缺点:(需要连续不断地重复劳动密集型的模型重新设计过程)</li></ul></li><li>具有大量特征集的简单(线性)模型<ul><li>一个具有大量特征的线性模型，以统一的框架方便新信息的整合。</li></ul></li></ol><p>存在的问题以及解决的方案：</p><ol><li><p>统一的简单线性模型是否能够准确地预测UOTD</p><ul><li>本文通过整合来自异构数据集的高维特征来解决这个问题</li><li>首先调查多个真实数据集，包括原始出租车需求(OTD)，兴趣点(POI)和气象学</li><li>本文框架将在空间、时间、气象和事件域上处理四种基本特征，并基于在线出租车平台的业务逻辑生成大量组合特征。</li></ul></li><li><p>高维特征也给在大规模数据集上训练高维特征模型带来了额外的挑战。</p><ul><li>本文的LinUOTD框架通过一个<strong>基于参数服务器的分布式框架</strong>来解决这个问题，以并行化和加速模型训练，并通过一个基于hash的特征标记方案来实现并行和可扩展的特征工程。</li><li>LinUOTD采用L1和L2正则化方法，设计了<strong>时空正则化</strong>方法，在避免过拟合的同时进行准确的预测。</li></ul></li></ol><p>本文提出的框架概述<br><img src= "/img/loading.gif" data-lazy-src="/image/60400/a001.png" alt="图1. 框架概述"></p><p>本文的贡献：</p><ul><li>首次尝试采用具有<strong>高维（数亿）特征</strong>的简单线性模型来预测UOTD，以满足大型在线出租车平台的准确性和灵活性要求。</li><li>将模型重新设计的开销转换为特征工程，并应用分布式学习框架来支持快速，并行和可扩展的特征更新和测试。</li><li>本文的方法在预测准确性方面优于经典非线性模型。</li></ul><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>数据集描述：</p><ul><li>数据集中包括原始出租车需求记录，地理信息和气象数据。 </li><li>在中国的两个大城市（北京和杭州）收集的</li></ul><h3 id="原始出租车需求记录数据（Original-Taxi-Demand-Record-Data）"><a href="#原始出租车需求记录数据（Original-Taxi-Demand-Record-Data）" class="headerlink" title="原始出租车需求记录数据（Original Taxi Demand Record Data）"></a>原始出租车需求记录数据（Original Taxi Demand Record Data）</h3><p><strong>北京数据集</strong></p><ul><li>北京数据集的原始出租车需求记录是从中国的在线出租车平台按比例采样的。</li><li>原始数据集包含北京三个月连续75天的23,851,235原始出租车需求记录。</li><li>数据集中的每个记录都由用户ID，时间戳，起点和终点的位置，距离估计和折扣信息组成。</li></ul><p><strong>杭州数据集</strong></p><ul><li>杭州数据集的原始出租车需求记录是从同一在线出租车平台上按比例抽样的。</li><li>原始的杭州数据集包含三个月内连续75天的连续12,354,687张出租车的原始需求记录。</li><li>数据集中的每个记录都由用户ID，时间戳，起点和终点的位置，距离估计和折扣信息组成。</li></ul><p>其中</p><ul><li>用户ID，时间戳，起点和终点由用户提交。</li><li>出租车平台基于用户提交的信息来计算距离估计和折扣信息。</li><li>用户ID和目的地被省略，因为它们与UOTD预测无关。</li></ul><p><strong>数据集分析图</strong>分为6个部分:</p><ul><li>所有原始出租车需求记录中起点位置的热图，可以观察到出租车需求集中的几个位置。(集中在市中心，密集的居民区和交通枢纽)</li><li>每天原始出租车需求记录的标准化数量的分布（三个月内）。可以表明出租车需求可能会受到动态时间因素（例如天气）的影响。</li><li>原始滑行需求的标准化数量相对于不同估计距离的分布（黄色曲线）和累积百分比（绿色曲线）。可以表明短途旅行占了出租车总需求的主导。</li><li>折扣的分布，可以看到大多数出租车需求在哪个区间内获得折扣。</li></ul><p>因为不同城市间的气候、经济和城市规划存在差异，所以添加两种数据：</p><ul><li>POI兴趣点空间分布图</li><li>每月天气分布图</li></ul><h3 id="POI-Data"><a href="#POI-Data" class="headerlink" title="POI Data"></a>POI Data</h3><ul><li>来自中国主要在线地图服务提供商的大规模地理信息数据集。</li><li>本文利用兴趣点（POI）的地理信息</li></ul><p><strong>北京</strong>：</p><ul><li>北京POI数据集包含北京的55,447个不同的POI。</li><li>每个记录都包含一个职位，一个姓名，一个行政区和一个三级类别。 </li></ul><p>一条记录如下所示：<br><span class="inline-tag grey">$\text{(116.49460 40.00057, Wangjing, Playground, Changyang District, Entertainment:OurdoorActivity:Playground)}$</span><br>其中 </p><ul><li>（116.49460 40.00057）代表POI的经度和纬度，</li><li>Wangjing Playground表示POI名称，</li><li>Changyang District是一个行政区在北京，</li><li>Entertainment:OurdoorActivity:Playground即娱乐：户外活动：游乐场，这是三个级别的类别。<ul><li>三级类别分别由粗略级（娱乐），中级（户外活动）和子级（游乐场）组成。 </li></ul></li></ul><ul><li>总共将POI分为16个粗级别，83个中级别和155个子级别。 </li><li>POI可以帮助解释出租车旅行的动机，从而解释UOTD的时空分布。</li></ul><p>16种POI的粗略分类：<br><img src= "/img/loading.gif" data-lazy-src="/image/60400/a002.png" alt="POI的粗略分类"></p><ul><li>本文的目标是预测每个POI的UOTD，因此本文通过将每个原始出租车需求记录的坐标与最近的POI相关联来对其进行预处理。</li></ul><p><strong>杭州</strong>：<br>杭州的POI数据集，其中包含42965个杭州不同的POI，和北京有着相同的类别</p><h3 id="Meteorology-Data"><a href="#Meteorology-Data" class="headerlink" title="Meteorology Data"></a>Meteorology Data</h3><ul><li>通过中国政府的在线气象网站在相应的三个月内进一步收集北京的气象数据。</li><li>每个气象记录都包含一个时间戳以及每小时的天气状况，温度，风，湿度和空气质量信息。天气状况分为雨夹雪，薄雾，雪，雨，晴朗，多云，雾和阴天。</li><li>空气质量分为六个等级：良好，中等，轻度污染，中度污染，重度污染和重度污染。</li></ul><p><strong>存在的问题及解决方法</strong>：<br>问题： </p><ul><li>在气象数据集中，有6％的记录不完整或为空。</li></ul><p>解决方法：</p><ul><li>通过将前几个小时和下几个小时的值进行平均，本文可以完成缺少的温度，风力和湿度。</li><li>对于缺少的天气状况和空气质量信息，我们使用与前一小时相同的值。</li></ul><h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><ul><li>采用高维特征来补偿简单线性模型的表达，从而使它们具有复杂非线性模型的预测能力。</li></ul><h3 id="基本特征"><a href="#基本特征" class="headerlink" title="基本特征"></a>基本特征</h3><h4 id="时间特征（Temporal-Features）"><a href="#时间特征（Temporal-Features）" class="headerlink" title="时间特征（Temporal Features）"></a>时间特征（Temporal Features）</h4><ul><li>Month</li><li>Day of month</li><li>Day of week</li><li>Hour</li><li>Holiday</li><li>Historical UOTD </li></ul><p>平日，周末和全天的标准化每小时出租车需求分布图，可以看出需求在工作日和周末之间具有不同的时间模式。（工作日有两个高峰，分别代表早晨高峰和傍晚高峰。 在周末，晚上只有一个高峰。）</p><h4 id="空间特征（Spatial-Features）"><a href="#空间特征（Spatial-Features）" class="headerlink" title="空间特征（Spatial Features）"></a>空间特征（Spatial Features）</h4><ul><li><p>District</p></li><li><p>POI name</p></li><li><p>POI category</p></li><li><p>Distance distribution</p></li><li><p>16种粗略POI类别中的每一种的规范化出租车需求图。[属于“基础设施”类别（例如，火车站和机场）的POI和“住所”看到了更多的出租车需求。]<br>标准化的出租车平均需求量</p></li><li><p>短，中，长出租车行程图。 [长途骑行（&gt; 20公里）的需求相对稳定，而对短途骑行（&lt;8km）的需求在不同的日子里急剧波动,如果来自POI的滑行需求主要由长途行驶所控制，则从该POI开始的滑行需求很可能在一段时间内保持稳定。]</p></li></ul><h4 id="气象特征（Meteorological-Features）"><a href="#气象特征（Meteorological-Features）" class="headerlink" title="气象特征（Meteorological Features）"></a>气象特征（Meteorological Features）</h4><ul><li>Weather condition </li><li>Temperature </li><li>Wind</li><li>Humidity</li><li>Air quality </li></ul><h4 id="活动特征（Event-Features）【影响打车者的动机】"><a href="#活动特征（Event-Features）【影响打车者的动机】" class="headerlink" title="活动特征（Event Features）【影响打车者的动机】"></a>活动特征（Event Features）【影响打车者的动机】</h4><ul><li>Discount pricing strategy</li><li>Even-odd license plate plan</li><li>Version of the App </li></ul><h3 id="组合特征"><a href="#组合特征" class="headerlink" title="组合特征"></a>组合特征</h3><ul><li>线性模型无法描述输入特征之间的相关性</li><li>将多样化的组合特征输入模型有助于从多尺度和多方面表征不同因素之间的相互作用，这是提高模型的预测能力的关键。</li></ul><h4 id="时时组合特征（Temporal-Temporal-Combinational-Features）"><a href="#时时组合特征（Temporal-Temporal-Combinational-Features）" class="headerlink" title="时时组合特征（Temporal-Temporal Combinational Features）"></a>时时组合特征（Temporal-Temporal Combinational Features）</h4><ul><li>将一天中的小时和星期几作为一个组合功能</li></ul><h4 id="时空组合特征（Temporal-Spatial-Combinational-Features）"><a href="#时空组合特征（Temporal-Spatial-Combinational-Features）" class="headerlink" title="时空组合特征（Temporal-Spatial Combinational Features）"></a>时空组合特征（Temporal-Spatial Combinational Features）</h4><ul><li>POI类别和“小时”的组合将捕获出租车需求的这种时空依赖性</li></ul><p>居住类POI和基础设施类POI的平均每小时标准化出租车需求图</p><h4 id="气象空间组合特征（Meteorological-Spatial-Combinational-Features）"><a href="#气象空间组合特征（Meteorological-Spatial-Combinational-Features）" class="headerlink" title="气象空间组合特征（Meteorological-Spatial Combinational Features）"></a>气象空间组合特征（Meteorological-Spatial Combinational Features）</h4><ul><li>结合气象和空间特征的理由是，气象信息对出租车需求的影响因功能不同的POI而异。</li><li>雨天和非雨天娱乐场所和机场的平均每小时标准化出租车需求图【降雨对机场的影响不明显。娱乐场所的需求对降雨敏感】</li></ul><h4 id="其他组合功能（Other-Combinational-Features）"><a href="#其他组合功能（Other-Combinational-Features）" class="headerlink" title="其他组合功能（Other Combinational Features）"></a>其他组合功能（Other Combinational Features）</h4><ul><li>将POI，小时和天气组合为时空气象特征</li></ul><p>本文最终选择了100多种功能，这些功能包括2亿个维度。</p><p>本文的特征表：</p><p><img src= "/img/loading.gif" data-lazy-src="/image/60400/a003.png" alt="特征表"></p><h2 id="模型和优化"><a href="#模型和优化" class="headerlink" title="模型和优化"></a>模型和优化</h2><ul><li>LinUOTD模型，具有高维特征和时空正则化器的线性回归模型</li><li>为了有效地训练LinUOTD模型，本文采用了基于参数服务器的分布式学习框架和基于散列的令牌化方法</li></ul><h3 id="UOTD预测模型"><a href="#UOTD预测模型" class="headerlink" title="UOTD预测模型"></a>UOTD预测模型</h3><ul><li>原始数据集 $D= \lbrace (x_i,y_i)|i=1,2,…,N \rbrace$</li><li>$(x_i,y_i)$ 表示第i个样本的UOTD和相应的特征向量。</li><li>$p_i = w^\prime x_i$ </li></ul><p>$$<br>obj_{linear}(w) = \sum_{i=1}^N (y_i-p_i)^{2} + \lambda_1 ||w||_1 + \lambda_2 ||w||_2<br>$$</p><ul><li><p>$y$ the UOTD in a specific hour at a given POI  </p></li><li><p>$x \in \mathbb{R}^m $ the feature vector corresponding to $y$ </p></li><li><p>一个具有超过两亿维的非常高维的向量。</p></li><li><p>real-world UOTD records close in space or time tend to be similar.</p></li><li><p>UOTD预测结果的这种平滑性要求导致本文设计以下目标函数来反映时空正则化：<br>$$<br>obj_{spatio-temporal}(w) = \sum_{X \subseteq D} \phi(X) var(\lbrace w^\prime x|x \in X\rbrace)<br>$$</p></li><li><p>$var()$ 表示方差</p></li><li><p>$X$ 是 $D$ 的子集</p></li><li><p>$\phi(X)$ 将POI和时间的子集映射为实值，该实值控制X中实例x的预测方差的正则化。</p></li></ul><p>$$<br>\phi(X) = \prod_{x \in X} \frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{(x - \overline{X})^\prime((x - \overline{X})}{2\sigma}}<br>$$</p><ul><li>$\overline{X} = \frac{1}{|X|} \sum_{x \in X} x$ </li><li>$\overline{X}$表示X的均值</li></ul><p>$$<br>obj_{LinUOTD}(w) =\sum_{i=1}^N (y_i-p_i)^2 + \lambda_1 ||w||<em>1 + \lambda_2 ||w||_2 + \gamma \sum</em>{X \subseteq D} \phi(X) var(\lbrace w^\prime x|x \in X\rbrace)<br>$$</p><p>其中  </p><ul><li>$\gamma$ 权衡参数，可以全局控制时空正则化的影响。</li></ul><p>使目标函数$obj_{LinUOTD}(w)$最小化，采用了具有egulatized dual average (RDA)和AdaGrad的随机梯度下降算法，如FTR1算法。<br>在不考虑L1和L2正则化的情况下，我们可以使用基于minibatch的随机梯度下降来最小化：<br>$$<br>w_{t+1} = w_t - \eta_t g_t<br>$$</p><ul><li>$t$表示迭代次数</li><li>$\eta_t$是第t次迭代的学习率</li><li>$g_t$表示第t次迭代的梯度。</li></ul><p>从迭代t的数据中抽取一个小批量$X \subseteq D$</p><ul><li>以矩阵形式$X_t = (x_1,…,x_l)^\prime$表示</li><li>定义$\overline{X_t} = (\overline{X_t},\overline{X_t},…,\overline{X_t})^\prime$</li></ul><p>在$obj_{LinUOTD}(w)$中没有L1，L2损失的目标函数的导数是<br>$g_t = X^\prime_t(p_t - y_t) + \gamma \phi(X) (X^\prime_t X - \overline{X_t}^\prime\overline{X_t})$</p><ul><li>其中$p_t$和$y_t$分别是$X_t$中数据的预测向量和标签向量。</li></ul><p><strong>RDA</strong>:解决正则化<br>$w_{t+1} = w_t - \eta_t g_t$的对偶如下：<br>$$<br>w_{t+1} = argmin_w(\sum_{s=1}^t g_s w + \frac{1}{2} \sum_{s=1}^t (\sigma_s ||w-w_s||_2^2 + \lambda_1 ||w||_1) + \lambda_2 ||w||_2)<br>$$</p><p>其封闭形式的求解器如下:<br><img src= "/img/loading.gif" data-lazy-src="/image/60400/s001.png" alt="image.png"><br>遵循AdaGrad，调整坐标的学习率以加快学习过程:<br>对于$g_t$的第i个坐标，<br>$$<br>\eta_{t,i} = \frac{\alpha}{\beta+\sum_{s=1}^t g_{s,i}^2}<br>$$<br>以上公式推导参考论文</p><blockquote><ul><li>B. McMahan, G Holt, D Sculley, M. Young, D. Ebner, J. Grady, L. Nie, T. Phillips, E. Davydov, D. Golovin, S. Chikkerur, D. Liu, M. Wattenberg, A. Hrafnkelsson, T. Boulos, and J. Kubica. 2013. Ad click prediction: a view from the trenches. In The 19th ACM International Conference on Knowledge Discovery and Data Mining, Chicago, IL, USA. 1222–1230.</li></ul></blockquote><h2 id="实现和优化"><a href="#实现和优化" class="headerlink" title="实现和优化"></a>实现和优化</h2><ul><li>LinUOTD的结构简单，有2亿维的功能</li><li>在一台机器上训练LinUOTD不可行。 为了有效地学习高维特征，本文利用了一个基于参数服务器的分布式学习框架和一个基于散列的令牌化方法。</li></ul><h3 id="系统优化"><a href="#系统优化" class="headerlink" title="系统优化"></a>系统优化</h3><p>用于训练LinUOTD的基于参数服务器的分布式框架的体系结构：<br><img src= "/img/loading.gif" data-lazy-src="/image/60400/a004.png" alt="用于训练LinUOTD的基于参数服务器的分布式框架的体系结构"></p><ul><li>该框架由多个参数服务器和工作节点组成，适用于海量数据集上的并行机器学习。</li><li>所有模型参数均<strong>均匀地分布</strong>在参数服务器之间，而训练数据则在<strong>训练过程开始时</strong>分配到每个工作节点。</li><li>在训练过程中，每个工作节点运行多个并行工作程序（线程），这些工作程序以小批量分析训练样本，通过全局特征哈希函数从参数服务器中获取相应的参数，并计算预测值和梯度。</li><li>算法1+算法2</li></ul><h3 id="特征工程优化"><a href="#特征工程优化" class="headerlink" title="特征工程优化"></a>特征工程优化</h3><ul><li>通过特征标记化，可以为每个功能（可以是值，字符串或它们的组合）分配唯一的ID</li><li>由于分配全局唯一的功能ID需要所有线程共享一个锁，因此令牌化使并行计算变得困难且效率低下。</li></ul><p>本文提出了一种基于哈希的令牌化方法，该方法以无锁的方式为每个功能分配一个ID，同时确保ID的全局唯一性。 </p><ul><li>通过诸如murmurhash和cityhash的哈希方案将每个离散功能哈希到64位空间。 因此，即使对于十亿维特征，哈希冲突的速率仍然很低。<br>基于哈希的令牌化具有两个优点：</li><li>并行性。 由于所有线程之间需要共享锁，因此无法并行化经典令牌化方案。 相反，基于散列的令牌化是无锁的，因此可以进行并行计算。 </li><li>可伸缩性。 要对新数据特征维度进行令牌化，基于哈希的令牌化仅需要对新维度进行哈希处理，而无需重新分配ID。 </li></ul><p>上述基于散列的令牌化在我们的分布式并行学习框架中至关重要，可大大加快数据准备过程。</p><p>算法1总结了每个工人的梯度计算过程。<br><img src= "/img/loading.gif" data-lazy-src="/image/60400/a005.png" alt="算法1"></p><ul><li>相同参数的梯度将首先通过工作人员内部聚合进行聚合</li><li>然后在工作节点之间转移以进行跨工作人员聚合</li><li>最后所有新计算的梯度将被推送到相应的参数服务器，并且每个参数服务器将使用接收到的梯度相应地更新参数。</li></ul><p>算法2根据Sec中的公式详细说明了每个参数服务器上的参数更新过程<br><img src= "/img/loading.gif" data-lazy-src="/image/60400/a006.png" alt="算法2"><br>以上迭代将一直持续到训练过程结束。</p><h2 id="实验研究"><a href="#实验研究" class="headerlink" title="实验研究"></a>实验研究</h2><h3 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h3><ul><li>在两个数据集上：北京和杭州</li><li>按时间顺序排列每个数据集，并使用前3/4进行训练，其余1/4进行测试。</li></ul><h4 id="基线"><a href="#基线" class="headerlink" title="基线"></a>基线</h4><ul><li><strong>HA(Historical Average)</strong>：<br>历史平均值。使用同期内历史UOTD的平均值来预测UOTD</li><li><strong>ARIMA(Auto-RegressiveIntegratedMovingAverage)</strong>：<br>自回归综合移动平均线。使用时间序列模型预测UOTD。</li><li><strong>Markov(Markov Model)</strong>：<br>马尔可夫模型。通过基于15个最近对应周期的UOTD训练3阶马尔可夫预测器来预测UOTD。</li><li><strong>GBRT(Gradient Boosted Regression Tree)</strong>：<br>梯度增强回归树。使用非参数回归预测UOTD。</li><li><strong>NN(Neural Network)</strong>：<br>通过使用15个最近对应时间段的UOTD训练神经网络来预测UOTD</li><li><strong>HP-MSI</strong>：<br>采用最先进的技术来预测UOTD，以预测要从每个自行车站出租或返还每个自行车站的自行车数量</li></ul><h4 id="指标"><a href="#指标" class="headerlink" title="指标"></a>指标</h4><ul><li>Error Rate (ER)</li><li>Symmetric Mean Absolute Percent Error (SMAPE) </li><li>Root Mean Squared Logarithmic Error (RMLSE) </li></ul><p>$$ER = \frac{\sum_{i=1}^N |p_i-y_i|}{\sum_{i=1}^N y_i}$$<br>$$SMAPE = \frac{2}{N} \sum_{i=1}^N \frac{|p_i-y_i|}{p_i+y_i+1}$$<br>$$RMLSE = \sqrt{\frac{1}{N} \sum_{i=1}^N (\log (p_i+1)-\log(y_i+1))^2}$$</p><h3 id="总体结果"><a href="#总体结果" class="headerlink" title="总体结果"></a>总体结果</h3><p><img src= "/img/loading.gif" data-lazy-src="/image/60400/a007.png" alt="不同方法的性能"></p><ol><li><p>朴素的HA在两个数据集上的表现都很差。但是，有时ARIMA和Markov甚至比单纯的HA方法还差。一个可能的原因可能是时间序列方法忽略了UOTD的空间变化。因此，它们倾向于对不同区域产生不稳定的预测精度，因此在大规模数据集上的总体性能不令人满意。 </p></li><li><p>NN和GBRT是两种竞争方法。原因可能是这两种方法都是受监督的非线性模型，并且能够从多个异构数据源中提取时空特征。</p></li><li><p>专为时空预测而设计的方法（HP-MSI和我们的LinUOTD）可实现最佳整体性能。我们的LinUOTD在两个数据集上的几乎所有指标上均优于HP-MSI。唯一的例外是杭州数据集上的SMAPE指标，其中HP-MSI得出的SMAPE略低。</p></li></ol><p>总之，LinUOTD通过采用简单的线性回归模型，通常优于所有基线（基于时间序列的方法和复杂的非线性模型），这表明正确选择的大量特征集可以弥补UOTD预测中模型的简单性。</p><h3 id="特征贡献分析"><a href="#特征贡献分析" class="headerlink" title="特征贡献分析"></a>特征贡献分析</h3><ul><li>最有帮助的功能是时间（星期几，小时），空间（POI名称，POI类别）和气象（天气状况，温度，空气质量）功能的多尺度组合。</li><li>最后两个主要特征是基本的时间特征，这与直觉是UOTD与不同时间尺度的最新历史UOTD高度相关。 </li><li>具体而言，UOTD倾向于在短时间内（例如，与1天前的UOTD比较）平稳变化，并表现出某些周期性模式（例如，与7天前的UOTD比较）。</li></ul><h3 id="原型系统"><a href="#原型系统" class="headerlink" title="原型系统"></a>原型系统</h3><p>技术：</p><ul><li>使用flatty（基于引导程序的模板）来构建前端基本页面</li><li>使用mapbox API（自定义在线地图的较大提供者）来创建页面上与地图相关的元素</li><li>后端，使用flask（Web应用程序的python微框架）处理请求</li></ul><p>功能：</p><ul><li>用户为POI和当前时间提交关键字（顶部）后，将显示相关POI（左侧），并且POI的位置也会在地图上标记出来。</li><li>用户还可以通过选择“查看3D”来查看3D视图。 </li><li>POI下一个小时的预测将由不同高度和颜色的条形表示。</li><li>当用户通过将光标放在某个POI附近来导航POI时，UOTD预测的详细信息将显示在灰色表中，其中包括接下来五个小时的预测以及最近五个小时的历史UOTD记录。</li></ul><p>特征贡献分析表<br><img src= "/img/loading.gif" data-lazy-src="/image/60400/a008.png" alt="特征贡献分析表"></p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="出租车需求预测"><a href="#出租车需求预测" class="headerlink" title="出租车需求预测"></a>出租车需求预测</h3><ul><li>根据预测模型是否需要出租车轨迹，分为基于轨迹的预测和无轨迹的预测。</li></ul><h4 id="基于轨迹的预测："><a href="#基于轨迹的预测：" class="headerlink" title="基于轨迹的预测："></a>基于轨迹的预测：</h4><ul><li>一个基于历史滑行轨迹的聚类和时间序列预测相结合的框架，以预测市区内的出租车需求。</li><li>一个模型来预测给定出租车站的未来服务数量，其中GPS轨迹和事件信号被转换成感兴趣的时间序列，既是学习基础又是流测试框架。 </li><li>利用出租车的历史GPS轨迹推荐快速接载乘客的地方。</li><li>一种改进的基于ARIMA的预测模型，以使用大规模GPS跟踪数据集来预测给定热点处的乘客时空变化。</li><li>在纽约市分析了超过1400万辆出租车的接送样本，并显示出出租车需求的高度可预测性。</li><li>一种预测查询，以根据移动出租车的历史数据来预测对象的聚集数量。</li><li>结合出租车的轨迹和航班到达数据来预测未满足的出租车需求，这意味着出租车需求与机场出租车的潜在供应之间的差距。</li></ul><p><strong>缺点</strong>：轨迹信息并不总是与UOTD信息相关联。</p><h4 id="基于无轨迹的预测："><a href="#基于无轨迹的预测：" class="headerlink" title="基于无轨迹的预测："></a>基于无轨迹的预测：</h4><ul><li>估计自行车共享系统的总体需求，可以很容易地扩展它以预测出租车需求。</li><li>本文以此作为基准</li></ul><h3 id="使用参数服务器进行分布式机器学习"><a href="#使用参数服务器进行分布式机器学习" class="headerlink" title="使用参数服务器进行分布式机器学习"></a>使用参数服务器进行分布式机器学习</h3><ul><li>工业应用程序通常采用分布式机器学习框架来处理大型数据集上的大量功能。 </li><li>传统的分布式机器学习方法侧重于“数据并行性”，其中每个计算节点都需要存储所有参数和模型的重复项，这导致了通信和存储的巨大开销。</li><li>Distbelief模型 基于参数服务器的分布式框架</li></ul><p>本文的工作采用了基于参数服务器的框架，区别在于两种系统优化技术：</p><ul><li>本文设计了基于散列的特征标记方案，以实现并行和可扩展的数据预处理。 </li><li>本文通过请求聚合提高了通信效率。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>提出了<strong>LinUOTD</strong>，一种用于预测大型在线出租车平台的单位原始出租车需求（UOTD）的统一方法。 </li><li>LinUOTD是具有超过<strong>2亿维特征</strong>的<strong>线性回归模型</strong>。 【简单的模型结构便于模型修改，而高维特征则保证了准确的预测性能。】</li><li>设计了<strong>一个时空正则化方案</strong>，<strong>一个分布式学习框架</strong>和<strong>一个基于散列的令牌化方法</strong>，以在大规模数据集上实现有效，并行和可扩展的特征学习。</li><li>对来自工业在线出租车平台的<strong>两个</strong>大规模数据集的广泛评估验证了我们方法的有效性。</li></ul>]]></content>
      
      
      <categories>
          
          <category> paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> paper </tag>
            
            <tag> 论文笔记 </tag>
            
            <tag> traffic flow </tag>
            
            <tag> taxi demands </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/hexo-hello-World/"/>
      <url>/posts/hexo-hello-World/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
